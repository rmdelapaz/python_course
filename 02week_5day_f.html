<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Full Stack Web Developer Course - Docker and Dependencies Management</title>
    <link rel="stylesheet" href="/styles/main.css">
    <link rel="icon" href="/favicon.png">
</head>
<body>
    <header>
        <h1>Python Full Stack Web Developer Course</h1>
        <h2>Week 2: Python Fundamentals (Part 1)</h2>
        <h3>Friday Morning: Docker and Dependencies Management</h3>
    </header>

    <main>
        <section class="introduction">
            <h3>Introduction to Docker and Dependencies Management</h3>
            <p>Welcome to our session on Docker and dependencies management! This morning, we'll explore how Docker containers can revolutionize the way you manage dependencies in Python projects. While we've already covered virtual environments, Docker takes isolation and reproducibility to a whole new level, addressing many limitations of traditional Python environments.</p>
            
            <p>Think of Docker as not just a tool, but a paradigm shift in how we approach development environments. If a virtual environment is like having a separate toolbox for each project, Docker is like having a complete, isolated workshop for each project—with its own tools, materials, and even operating system.</p>
        </section>

        <section class="docker-review">
            <h3>Quick Docker Review</h3>
            
            <p>Let's briefly recap what we've learned about Docker earlier this week:</p>
            
            <ul>
                <li><strong>Containers</strong>: Lightweight, isolated environments that package an application and its dependencies</li>
                <li><strong>Images</strong>: Read-only templates used to create containers</li>
                <li><strong>Dockerfile</strong>: Instructions for building a Docker image</li>
                <li><strong>Docker Compose</strong>: Tool for defining and running multi-container applications</li>
            </ul>
            
            <p>Today, we'll focus specifically on how Docker addresses the complex challenge of dependency management in Python applications.</p>
        </section>

        <section class="dependency-challenges">
            <h3>The Dependency Management Challenge</h3>
            
            <p>Before diving into Docker solutions, let's understand the key challenges in Python dependency management:</p>
            
            <h4>1. The "Works on My Machine" Problem</h4>
            <p>Despite using virtual environments, differences in operating systems and system libraries can still cause inconsistent behavior.</p>
            
            <p><strong>Example:</strong> A library that depends on a C extension might compile differently on macOS vs. Linux, or fail entirely on Windows.</p>
            
            <h4>2. System-Level Dependencies</h4>
            <p>Python packages often rely on system libraries that virtual environments don't manage.</p>
            
            <p><strong>Example:</strong> Packages like Pillow (imaging), NumPy (with optimized BLAS), or psycopg2 (PostgreSQL) require specific system libraries installed.</p>
            
            <h4>3. Version Conflicts and Dependency Hell</h4>
            <p>Complex applications with many dependencies can lead to irreconcilable version conflicts.</p>
            
            <p><strong>Example:</strong> Package A requires library X version 1.x, while Package B requires library X version 2.x.</p>
            
            <h4>4. Environment Parity</h4>
            <p>Development, testing, and production environments need to be identical to avoid surprises.</p>
            
            <p><strong>Example:</strong> Your application works in development but fails in production due to subtle differences in environment configuration.</p>
            
            <h4>5. Onboarding Friction</h4>
            <p>New team members often spend days setting up a development environment with the right versions of everything.</p>
            
            <p><strong>Example:</strong> A new developer joins and spends their first week just trying to get the app running locally.</p>
            
            <p><strong>Real-World Analogy:</strong> Traditional dependency management is like giving someone a list of ingredients for a complex recipe—they might get slightly different brands, qualities, or preparations. Docker is like delivering a pre-measured, ready-to-mix ingredient kit with exactly what's needed.</p>
        </section>

        <section class="docker-solution">
            <h3>How Docker Solves Dependency Challenges</h3>
            
            <h4>1. Complete Environment Isolation</h4>
            <p>Docker containers include not just Python packages, but the entire runtime environment:</p>
            <ul>
                <li>Specific Python version</li>
                <li>System libraries and dependencies</li>
                <li>Environment variables</li>
                <li>File system state</li>
            </ul>
            
            <h4>2. Consistent Environments Everywhere</h4>
            <p>The same container runs identically on any system with Docker installed, eliminating the "works on my machine" problem.</p>
            
            <h4>3. Declarative Configuration</h4>
            <p>A Dockerfile declares exactly what's in your environment, making it self-documenting and version-controllable.</p>
            
            <h4>4. Layered Caching</h4>
            <p>Docker's layer caching makes rebuilding environments fast after small changes to dependencies.</p>
            
            <h4>5. Isolation Without Performance Penalty</h4>
            <p>Containers have near-native performance while maintaining isolation.</p>
            
            <h4>6. Service Composition</h4>
            <p>Docker Compose lets you define and run multi-container applications (e.g., your app plus its database, cache, etc.).</p>
            
            <p><strong>Real-World Analogy:</strong> If traditional development is like each chef preparing ingredients differently, Docker is like a professional kitchen where every station is identical and prepared exactly to specification—the same tools, same ingredients, same procedures, no matter who's cooking.</p>
        </section>

        <section class="dockerfile-deep-dive">
            <h3>Dockerfile Deep Dive for Python Applications</h3>
            
            <p>The Dockerfile is the blueprint for your application's environment. Let's examine the key components for Python applications:</p>
            
            <h4>Base Image Selection</h4>
            <p>Choosing the right base image is crucial for Python applications:</p>
            
            <pre><code># Official Python images
FROM python:3.9                # Full Python with all dependencies
FROM python:3.9-slim           # Smaller image, fewer system packages
FROM python:3.9-alpine         # Minimal image, smallest size

# Alternative: Use specific OS with Python installed
FROM ubuntu:20.04              # Then install Python yourself</code></pre>
            
            <p><strong>Decision Factors:</strong></p>
            <table>
                <tr>
                    <th>Image Type</th>
                    <th>Pros</th>
                    <th>Cons</th>
                    <th>Best For</th>
                </tr>
                <tr>
                    <td><code>python:3.x</code></td>
                    <td>Complete, works with most packages</td>
                    <td>Large size (~900MB)</td>
                    <td>Complex applications with C extensions</td>
                </tr>
                <tr>
                    <td><code>python:3.x-slim</code></td>
                    <td>Smaller (~150MB), most libraries work</td>
                    <td>May need extra packages for some extensions</td>
                    <td>Web applications, good default choice</td>
                </tr>
                <tr>
                    <td><code>python:3.x-alpine</code></td>
                    <td>Very small (~50MB)</td>
                    <td>Uses musl instead of glibc, compilation issues</td>
                    <td>Microservices, simple scripts, constrained environments</td>
                </tr>
            </table>
            
            <h4>Working Directory</h4>
            <p>Set a dedicated working directory for your application:</p>
            
            <pre><code># Create and set working directory
WORKDIR /app</code></pre>
            
            <h4>Dependency Installation</h4>
            <p>Efficiently installing Python dependencies with pip:</p>
            
            <pre><code># Copy just the requirements file first (for better caching)
COPY requirements.txt .

# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt</code></pre>
            
            <p><strong>Advanced Technique:</strong> Use multi-stage builds to separate dependency installation from application code:</p>
            
            <pre><code># Build stage
FROM python:3.9-slim AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential gcc

WORKDIR /build
COPY requirements.txt .

# Install dependencies
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /build/wheels -r requirements.txt

# Final stage
FROM python:3.9-slim

WORKDIR /app

# Copy built wheels and install
COPY --from=builder /build/wheels /wheels
COPY --from=builder /build/requirements.txt .
RUN pip install --no-cache-dir --no-index --find-links=/wheels -r requirements.txt

# Clean up
RUN rm -rf /wheels</code></pre>
            
            <h4>System Dependencies</h4>
            <p>Installing system packages for common Python libraries:</p>
            
            <pre><code># Example for Debian-based images (including python:slim)
RUN apt-get update && apt-get install -y --no-install-recommends \
    # for Pillow
    libjpeg-dev zlib1g-dev libpng-dev \
    # for psycopg2
    libpq-dev \
    # for lxml
    libxml2-dev libxslt-dev \
    # Cleanup to reduce image size
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*</code></pre>
            
            <p><strong>Alpine Example:</strong></p>
            <pre><code># Example for Alpine images
RUN apk add --no-cache \
    jpeg-dev zlib-dev libpng-dev \
    postgresql-dev \
    libxml2-dev libxslt-dev</code></pre>
            
            <h4>Application Code</h4>
            <p>Copying application code and setting up the container:</p>
            
            <pre><code># Copy application code
COPY . .

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Expose port
EXPOSE 8000

# Run the application
CMD ["gunicorn", "app:app", "--bind", "0.0.0.0:8000"]</code></pre>
            
            <h4>User Setup</h4>
            <p>Running as a non-root user for security:</p>
            
            <pre><code># Create a non-root user
RUN addgroup --system app && adduser --system --group app

# Set ownership
RUN chown -R app:app /app

# Switch to non-root user
USER app</code></pre>
            
            <h4>A Complete Example</h4>
            <p>Bringing it all together for a Flask application:</p>
            
            <pre><code># syntax=docker/dockerfile:1

FROM python:3.9-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Create and set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Create non-root user
RUN addgroup --system app && adduser --system --group app

# Copy application code
COPY . .

# Set ownership
RUN chown -R app:app /app

# Switch to non-root user
USER app

# Expose port
EXPOSE 8000

# Run the application
CMD ["gunicorn", "wsgi:app", "--bind", "0.0.0.0:8000"]</code></pre>
        </section>

        <section class="docker-compose-for-dependencies">
            <h3>Managing Dependencies with Docker Compose</h3>
            
            <p>Docker Compose is extremely powerful for managing service dependencies in your application, like databases, caches, and other services.</p>
            
            <h4>Basic Compose File Structure</h4>
            <pre><code>version: '3'

services:
  web:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/mydatabase
    depends_on:
      - db
  
  db:
    image: postgres:13
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=mydatabase

volumes:
  postgres_data:</code></pre>
            
            <h4>Development-Focused Compose Configuration</h4>
            <p>For development, you'll want hot reloading and easier debugging:</p>
            
            <pre><code>version: '3'

services:
  web:
    build:
      context: .
      dockerfile: Dockerfile.dev  # Development-specific Dockerfile
    command: flask run --host=0.0.0.0 --port=8000 --debug  # Development server with auto-reload
    ports:
      - "8000:8000"
    volumes:
      - .:/app  # Mount current directory for live code changes
    environment:
      - FLASK_ENV=development
      - FLASK_APP=app.py
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/mydatabase
    depends_on:
      - db
  
  db:
    image: postgres:13
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=mydatabase
    ports:
      - "5432:5432"  # Expose port for local database tools
  
  # Additional development services
  adminer:  # Database management UI
    image: adminer:latest
    ports:
      - "8080:8080"
    depends_on:
      - db

volumes:
  postgres_data:</code></pre>
            
            <h4>Multiple Environment Configurations</h4>
            <p>You can create different Compose files for different environments:</p>
            
            <p>Base configuration: <code>docker-compose.yml</code></p>
            <pre><code>version: '3'

services:
  web:
    build: .
    depends_on:
      - db
  
  db:
    image: postgres:13
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:</code></pre>
            
            <p>Development overrides: <code>docker-compose.override.yml</code> (applied automatically)</p>
            <pre><code>version: '3'

services:
  web:
    command: flask run --host=0.0.0.0 --port=8000 --debug
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    environment:
      - FLASK_ENV=development
      - FLASK_APP=app.py
  
  db:
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=mydatabase</code></pre>
            
            <p>Production overrides: <code>docker-compose.prod.yml</code></p>
            <pre><code>version: '3'

services:
  web:
    restart: always
    command: gunicorn wsgi:app --bind 0.0.0.0:8000 --workers 4
    expose:
      - "8000"
    environment:
      - FLASK_ENV=production
    volumes:
      - static_data:/app/static
      - media_data:/app/media
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
      - static_data:/static
      - media_data:/media
    depends_on:
      - web
  
  db:
    restart: always
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_DB=${DB_NAME}

volumes:
  static_data:
  media_data:</code></pre>
            
            <p>Running with a specific configuration:</p>
            <pre><code># Development (default)
docker-compose up

# Production
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up</code></pre>
            
            <h4>Dependencies Beyond Services</h4>
            <p>Docker Compose can also help manage:</p>
            
            <ul>
                <li><strong>Initialization Scripts</strong>: Run database migrations or seed data</li>
                <li><strong>Scheduled Tasks</strong>: Add services for Celery workers or cron jobs</li>
                <li><strong>Development Tools</strong>: Include services for documentation, testing, or monitoring</li>
            </ul>
            
            <pre><code># Example with additional components
services:
  # ...other services...
  
  # Worker for background tasks
  worker:
    build: .
    command: celery -A app.tasks worker --loglevel=info
    volumes:
      - .:/app
    depends_on:
      - web
      - redis
      - db
  
  # Redis for task queue and caching
  redis:
    image: redis:alpine
    volumes:
      - redis_data:/data
  
  # Scheduled task processor
  scheduler:
    build: .
    command: celery -A app.tasks beat --loglevel=info
    volumes:
      - .:/app
    depends_on:
      - worker
      - redis

volumes:
  redis_data:</code></pre>
        </section>

        <section class="dependency-management-strategies">
            <h3>Python Dependency Management Strategies in Docker</h3>
            
            <h4>Pinning Versions</h4>
            <p>Always pin dependency versions for reproducible builds:</p>
            
            <pre><code># requirements.txt with pinned versions
flask==2.0.1
sqlalchemy==1.4.23
psycopg2-binary==2.9.1
gunicorn==20.1.0</code></pre>
            
            <p><strong>Warning:</strong> Be careful with using <code>pip freeze</code> directly, as it will include all sub-dependencies, which can cause problems when updating packages.</p>
            
            <h4>Requirements Files Organization</h4>
            <p>Splitting requirements files by environment or purpose:</p>
            
            <pre><code>requirements/
├── base.txt       # Core dependencies
├── development.txt
├── production.txt
└── testing.txt</code></pre>
            
            <p>Example of <code>requirements/base.txt</code>:</p>
            <pre><code># Core dependencies with pinned versions
flask==2.0.1
sqlalchemy==1.4.23
psycopg2-binary==2.9.1</code></pre>
            
            <p>Example of <code>requirements/development.txt</code>:</p>
            <pre><code># Include base requirements
-r base.txt

# Development-specific packages
pytest==6.2.5
black==21.9b0
flake8==3.9.2
ipython==7.27.0</code></pre>
            
            <p>In your Dockerfile, use the appropriate requirements file:</p>
            <pre><code># For development
COPY requirements/development.txt .
RUN pip install -r development.txt

# For production
COPY requirements/production.txt .
RUN pip install -r production.txt</code></pre>
            
            <h4>Using pip-tools</h4>
            <p>pip-tools helps manage dependencies more precisely:</p>
            
            <p>Create a <code>requirements.in</code> file with your direct dependencies:</p>
            <pre><code># requirements.in - Direct dependencies only
flask>=2.0.0
sqlalchemy>=1.4.0
psycopg2-binary</code></pre>
            
            <p>Compile it to a pinned <code>requirements.txt</code>:</p>
            <pre><code>pip-compile requirements.in</code></pre>
            
            <p>This generates a requirements.txt with all dependencies (including sub-dependencies) pinned:</p>
            <pre><code># requirements.txt
#
# This file is autogenerated by pip-compile
#
click==8.0.1
    # via flask
flask==2.0.1
    # via -r requirements.in
itsdangerous==2.0.1
    # via flask
jinja2==3.0.1
    # via flask
markupsafe==2.0.1
    # via jinja2
psycopg2-binary==2.9.1
    # via -r requirements.in
sqlalchemy==1.4.23
    # via -r requirements.in
werkzeug==2.0.1
    # via flask</code></pre>
            
            <p>In your Dockerfile:</p>
            <pre><code># Install pip-tools
RUN pip install pip-tools

# Copy requirements files
COPY requirements.in .
COPY requirements.txt .

# Install dependencies
RUN pip-sync</code></pre>
            
            <h4>Poetry for Modern Dependency Management</h4>
            <p>Poetry offers a more modern approach to dependency management:</p>
            
            <p>Create a <code>pyproject.toml</code> file:</p>
            <pre><code>[tool.poetry]
name = "myapp"
version = "0.1.0"
description = "My awesome app"
authors = ["Your Name <your.email@example.com>"]

[tool.poetry.dependencies]
python = "^3.9"
flask = "^2.0.1"
sqlalchemy = "^1.4.23"
psycopg2-binary = "^2.9.1"
gunicorn = "^20.1.0"

[tool.poetry.dev-dependencies]
pytest = "^6.2.5"
black = "^21.9b0"
flake8 = "^3.9.2"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"</code></pre>
            
            <p>Dockerfile for Poetry:</p>
            <pre><code>FROM python:3.9-slim

# Install dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    build-essential \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Poetry
RUN curl -sSL https://install.python-poetry.org | python3 -

# Add Poetry to PATH
ENV PATH="${PATH}:/root/.local/bin"

# Set working directory
WORKDIR /app

# Copy Poetry configuration
COPY pyproject.toml poetry.lock* ./

# Configure Poetry not to use virtual environments
RUN poetry config virtualenvs.create false

# Install dependencies
RUN poetry install --no-interaction --no-ansi

# Copy application code
COPY . .

# Run the application
CMD ["gunicorn", "wsgi:app", "--bind", "0.0.0.0:8000"]</code></pre>
            
            <h4>Pipenv Approach</h4>
            <p>Pipenv is another option for managing dependencies:</p>
            
            <p>With Pipenv, you use <code>Pipfile</code> and <code>Pipfile.lock</code>:</p>
            <pre><code># Pipfile
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
flask = "*"
sqlalchemy = "*"
psycopg2-binary = "*"
gunicorn = "*"

[dev-packages]
pytest = "*"
black = "*"
flake8 = "*"

[requires]
python_version = "3.9"</code></pre>
            
            <p>Dockerfile for Pipenv:</p>
            <pre><code>FROM python:3.9-slim

# Install dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install pipenv
RUN pip install pipenv

# Set working directory
WORKDIR /app

# Copy Pipfile and Pipfile.lock
COPY Pipfile Pipfile.lock ./

# Install dependencies system-wide (no virtualenv)
RUN pipenv install --system --deploy

# Copy application code
COPY . .

# Run the application
CMD ["gunicorn", "wsgi:app", "--bind", "0.0.0.0:8000"]</code></pre>
        </section>

        <section class="development-vs-production">
            <h3>Development vs. Production Docker Configurations</h3>
            
            <p>Different environments have different requirements for dependency handling:</p>
            
            <h4>Development Environment Priorities</h4>
            <ul>
                <li><strong>Quick iteration</strong>: Fast rebuilds when dependencies change</li>
                <li><strong>Debugging tools</strong>: Include development packages</li>
                <li><strong>Code synchronization</strong>: Live code reloading</li>
                <li><strong>Local persistence</strong>: Data should persist between container rebuilds</li>
            </ul>
            
            <h4>Production Environment Priorities</h4>
            <ul>
                <li><strong>Security</strong>: Minimal attack surface, no dev tools</li>
                <li><strong>Performance</strong>: Optimized dependencies and configurations</li>
                <li><strong>Reliability</strong>: Stable, pinned versions of everything</li>
                <li><strong>Size</strong>: Smaller images for faster deployments</li>
            </ul>
            
            <h4>Development Dockerfile (Dockerfile.dev)</h4>
            <pre><code>FROM python:3.9

# Install development tools and dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    default-libmysqlclient-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_ENV=development \
    FLASK_DEBUG=1

# Install dev dependencies first
COPY requirements/development.txt .
RUN pip install -r development.txt

# During development, we'll mount the code as a volume
# so we don't need to copy it here

# Default command for development
CMD ["flask", "run", "--host=0.0.0.0", "--port=8000"]</code></pre>
            
            <h4>Production Dockerfile (Dockerfile or Dockerfile.prod)</h4>
            <pre><code>FROM python:3.9-slim AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /build

# Copy requirements
COPY requirements/production.txt .

# Install dependencies with wheel support
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /build/wheels -r production.txt

# Final stage
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_ENV=production

# Install runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq5 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy built wheels and install
COPY --from=builder /build/wheels /wheels
COPY --from=builder /build/production.txt .
RUN pip install --no-cache-dir --no-index --find-links=/wheels -r production.txt \
    && rm -rf /wheels

# Create non-root user
RUN addgroup --system app && adduser --system --group app

# Copy application code
COPY . .

# Set ownership
RUN chown -R app:app /app

# Switch to non-root user
USER app

# Run the application
CMD ["gunicorn", "wsgi:app", "--bind", "0.0.0.0:8000", "--workers", "4"]</code></pre>
            
            <h4>Using Different Configurations</h4>
            <p>For development:</p>
            <pre><code>docker build -f Dockerfile.dev -t myapp:dev .
docker run -p 8000:8000 -v $(pwd):/app myapp:dev</code></pre>
            
            <p>Or with Docker Compose:</p>
            <pre><code>docker-compose -f docker-compose.dev.yml up</code></pre>
            
            <p>For production:</p>
            <pre><code>docker build -t myapp:prod .
docker run -p 8000:8000 myapp:prod</code></pre>
            
            <p>Or with Docker Compose:</p>
            <pre><code>docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d</code></pre>
        </section>

        <section class="advanced-techniques">
            <h3>Advanced Dependency Management Techniques</h3>
            
            <h4>1. Layer Caching Optimization</h4>
            <p>Docker builds images in layers. Optimize your Dockerfile to leverage caching for dependencies:</p>
            
            <pre><code># BAD: Changes to code trigger reinstallation of all dependencies
COPY . .
RUN pip install -r requirements.txt

# GOOD: Dependencies only reinstalled when requirements change
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .</code></pre>
            
            <h4>2. Build Arguments for Environment Selection</h4>
            <p>Use build arguments to selectively install dependencies:</p>
            
            <pre><code># Dockerfile
ARG ENVIRONMENT=production

# Base dependencies
COPY requirements/base.txt .
RUN pip install -r base.txt

# Environment-specific dependencies
COPY requirements/${ENVIRONMENT}.txt ./requirements-env.txt
RUN if [ -s requirements-env.txt ]; then pip install -r requirements-env.txt; fi

# Build with:
# docker build --build-arg ENVIRONMENT=development -t myapp:dev .</code></pre>
            
            <h4>3. Scanning for Vulnerabilities</h4>
            <p>Regularly scan dependencies for security vulnerabilities:</p>
            
            <pre><code># Install safety
RUN pip install safety

# Run security check
RUN safety check -r requirements.txt</code></pre>
            
            <p>Or as part of CI/CD:</p>
            <pre><code>stage: security
script:
  - pip install safety
  - safety check -r requirements.txt</code></pre>
            
            <h4>4. Custom Package Indexes</h4>
            <p>For organizations with private packages:</p>
            
            <pre><code># Using private PyPI server
RUN pip install --index-url https://pypi.example.com/simple/ \
    --extra-index-url https://pypi.org/simple \
    -r requirements.txt</code></pre>
            
            <h4>5. Vendoring Dependencies</h4>
            <p>For air-gapped environments or strict controls:</p>
            
            <pre><code># Download dependencies (on a connected machine)
pip download -d ./vendor -r requirements.txt

# In Dockerfile
COPY vendor /vendor
RUN pip install --no-index --find-links=/vendor -r requirements.txt</code></pre>
            
            <h4>6. Dependency Health Checks</h4>
            <p>Verify dependency compatibility in the container:</p>
            
            <pre><code># Add health check in Dockerfile
RUN pip check

# In CI pipeline
docker run --rm myapp:latest pip check</code></pre>
            
            <h4>7. Managing Native Dependencies</h4>
            <p>Handling packages with complex C extensions:</p>
            
            <pre><code># For packages requiring specific compilation flags
ENV CFLAGS="-march=native -O3"
RUN pip install numpy scipy</code></pre>
            
            <h4>8. Shrinking Final Images</h4>
            <p>Reduce image size by removing build dependencies:</p>
            
            <pre><code># Remove pip cache and unnecessary files
RUN pip install --no-cache-dir -r requirements.txt \
    && find /usr/local -type d -name __pycache__ -exec rm -rf {} +</code></pre>
        </section>

        <section class="practical-workflows">
            <h3>Practical Workflows for Development</h3>
            
            <h4>Local Development Workflow</h4>
            <p>A practical workflow for day-to-day development:</p>
            
            <ol>
                <li><strong>Initial Setup</strong>
                    <pre><code>git clone https://github.com/example/myapp.git
cd myapp
docker-compose up -d</code></pre>
                </li>
                <li><strong>Adding a New Dependency</strong>
                    <pre><code># Add to requirements.txt
echo "new-package==1.0.0" >> requirements.txt

# Rebuild the container
docker-compose build
docker-compose up -d</code></pre>
                </li>
                <li><strong>Development Cycle</strong>
                    <pre><code># Edit code locally (changes are synced via volume mount)
# View logs
docker-compose logs -f

# Run tests
docker-compose exec web pytest

# Restart after major changes
docker-compose restart web</code></pre>
                </li>
                <li><strong>Database Migrations</strong>
                    <pre><code># Create migration
docker-compose exec web flask db migrate -m "Add new table"

# Apply migration
docker-compose exec web flask db upgrade</code></pre>
                </li>
                <li><strong>Cleanup</strong>
                    <pre><code># Stop containers
docker-compose down

# Remove volumes (caution: destroys data)
docker-compose down -v</code></pre>
                </li>
            </ol>
            
            <h4>Working with Multiple Projects</h4>
            <p>When juggling multiple projects:</p>
            
            <ol>
                <li><strong>Unique Naming</strong>
                    <pre><code># Use project-specific container names
# docker-compose.yml
services:
  web:
    container_name: myapp_web</code></pre>
                </li>
                <li><strong>Port Allocation</strong>
                    <pre><code># Assign different ports to different projects
# Project A
services:
  web:
    ports:
      - "8000:8000"
  db:
    ports:
      - "5432:5432"

# Project B
services:
  web:
    ports:
      - "8001:8000"
  db:
    ports:
      - "5433:5432"</code></pre>
                </li>
                <li><strong>Resource Constraints</strong>
                    <pre><code># Limit resources to prevent one project from consuming everything
services:
  web:
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 500M</code></pre>
                </li>
            </ol>
            
            <h4>Advanced Development Conveniences</h4>
            <p>Make development more convenient with these techniques:</p>
            
            <ol>
                <li><strong>Development Helper Scripts</strong>
                    <pre><code># In your package.json or Makefile
"scripts": {
  "start": "docker-compose up",
  "test": "docker-compose exec web pytest",
  "shell": "docker-compose exec web bash",
  "db-shell": "docker-compose exec db psql -U postgres",
  "logs": "docker-compose logs -f",
  "migrate": "docker-compose exec web flask db upgrade"
}</code></pre>
                </li>
                <li><strong>Debugger Integration</strong>
                    <pre><code># Allow remote debugging
# docker-compose.yml
services:
  web:
    ports:
      - "8000:8000"
      - "5678:5678"  # Port for debugger
    command: python -m debugpy --listen 0.0.0.0:5678 -m flask run --host=0.0.0.0 --port=8000</code></pre>
                </li>
                <li><strong>Hot Reloading</strong>
                    <pre><code># Use development server with debug mode
# For Flask
ENV FLASK_ENV=development

# For Django
ENV DEBUG=1</code></pre>
                </li>
            </ol>
        </section>

        <section class="complex-environments">
            <h3>Managing Complex Environments</h3>
            
            <h4>Microservices Architecture</h4>
            <p>When your application consists of multiple services:</p>
            
            <pre><code># docker-compose.yml for microservices
version: '3'

services:
  auth-service:
    build: ./auth-service
    environment:
      - DB_HOST=db
      - REDIS_HOST=redis
    depends_on:
      - db
      - redis
  
  user-service:
    build: ./user-service
    environment:
      - DB_HOST=db
      - AUTH_SERVICE_URL=http://auth-service:8000
    depends_on:
      - db
      - auth-service
  
  product-service:
    build: ./product-service
    environment:
      - DB_HOST=db
    depends_on:
      - db
  
  api-gateway:
    build: ./api-gateway
    ports:
      - "8000:8000"
    environment:
      - AUTH_SERVICE_URL=http://auth-service:8000
      - USER_SERVICE_URL=http://user-service:8000
      - PRODUCT_SERVICE_URL=http://product-service:8000
    depends_on:
      - auth-service
      - user-service
      - product-service
  
  db:
    image: postgres:13
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:alpine
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:</code></pre>
            
            <h4>Hybrid Virtual Environment + Docker Approach</h4>
            <p>For some teams, combining virtual environments and Docker works well:</p>
            
            <ol>
                <li><strong>Local Development</strong>: Use virtual environments for fast iteration</li>
                <li><strong>Integration Testing</strong>: Use Docker Compose for complete environment</li>
                <li><strong>CI/CD Pipeline</strong>: Use Docker for consistent builds</li>
                <li><strong>Production</strong>: Deploy Docker containers</li>
            </ol>
            
            <p>Script to synchronize between virtual env and Docker:</p>
            <pre><code>#!/bin/bash
# sync_deps.sh

# Update requirements.txt from virtual environment
if [[ "$1" == "freeze" ]]; then
  echo "Updating requirements.txt from virtual environment..."
  pip freeze > requirements.txt
  echo "Done."

# Install from requirements.txt to virtual environment
elif [[ "$1" == "install" ]]; then
  echo "Installing requirements.txt to virtual environment..."
  pip install -r requirements.txt
  echo "Done."

# Rebuild Docker containers with new requirements
elif [[ "$1" == "docker" ]]; then
  echo "Rebuilding Docker containers..."
  docker-compose build
  echo "Done."

# Sync in both directions
elif [[ "$1" == "sync" ]]; then
  echo "Syncing between virtual environment and Docker..."
  pip freeze > requirements.txt
  docker-compose build
  echo "Done."

else
  echo "Usage: $0 [freeze|install|docker|sync]"
  echo "  freeze:  Update requirements.txt from virtual environment"
  echo "  install: Install from requirements.txt to virtual environment"
  echo "  docker:  Rebuild Docker containers with current requirements.txt"
  echo "  sync:    Update requirements.txt and rebuild Docker containers"
  exit 1
fi</code></pre>
            
            <h4>Handling Large Dependencies</h4>
            <p>Some Python packages (like ML libraries) are extremely large. Strategies for managing them:</p>
            
            <ol>
                <li><strong>Selective Installation</strong>
                    <pre><code># Install only what you need
RUN pip install tensorflow-cpu  # Instead of full tensorflow</code></pre>
                </li>
                <li><strong>Pre-built Images</strong>
                    <pre><code># Use specialized base images
FROM tensorflow/tensorflow:2.6.0
# Now you have TensorFlow pre-installed</code></pre>
                </li>
                <li><strong>Layer Caching</strong>
                    <pre><code># Split requirements by change frequency
COPY requirements-stable.txt .
RUN pip install -r requirements-stable.txt

# More frequently changing deps later
COPY requirements-changing.txt .
RUN pip install -r requirements-changing.txt</code></pre>
                </li>
            </ol>
        </section>

        <section class="real-world-example">
            <h3>Real-World Example: Flask Application with Comprehensive Dependency Management</h3>
            
            <p>Let's bring everything together with a complete example of a Flask application that demonstrates best practices for dependency management with Docker.</p>
            
            <h4>Project Structure</h4>
            <pre><code>flask_app/
├── .dockerignore           # Files to exclude from Docker build
├── .gitignore              # Files to exclude from Git
├── Dockerfile              # Production Dockerfile
├── Dockerfile.dev          # Development Dockerfile
├── README.md               # Project documentation
├── docker-compose.yml      # Base Docker Compose config
├── docker-compose.override.yml  # Development overrides
├── docker-compose.prod.yml # Production overrides
├── app/                    # Application code
│   ├── __init__.py
│   ├── config.py           # Configuration
│   ├── models.py           # Database models
│   ├── routes.py           # API routes
│   ├── templates/          # Jinja2 templates
│   └── static/             # Static assets
├── migrations/             # Database migrations
├── requirements/
│   ├── base.txt            # Base dependencies
│   ├── development.txt     # Development dependencies
│   └── production.txt      # Production dependencies
├── scripts/
│   ├── entrypoint.sh       # Docker entrypoint script
│   └── start-dev.sh        # Development startup script
└── tests/                  # Application tests</code></pre>
            
            <h4>Dependency Files</h4>
            
            <p><code>requirements/base.txt</code></p>
            <pre><code># Core dependencies
Flask==2.0.1
Flask-SQLAlchemy==2.5.1
Flask-Migrate==3.1.0
SQLAlchemy==1.4.23
psycopg2-binary==2.9.1
gunicorn==20.1.0
python-dotenv==0.19.0
werkzeug==2.0.1
click==8.0.1
itsdangerous==2.0.1
jinja2==3.0.1
markupsafe==2.0.1</code></pre>
            
            <p><code>requirements/development.txt</code></p>
            <pre><code># Include base requirements
-r base.txt

# Development packages
pytest==6.2.5
pytest-cov==2.12.1
black==21.8b0
flake8==3.9.2
ipython==7.27.0
debugpy==1.4.1</code></pre>
            
            <p><code>requirements/production.txt</code></p>
            <pre><code># Include base requirements
-r base.txt

# Production packages
sentry-sdk==1.3.1
blinker==1.4</code></pre>
            
            <h4>Docker Configuration</h4>
            
            <p><code>Dockerfile.dev</code></p>
            <pre><code>FROM python:3.9-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_APP=app \
    FLASK_ENV=development

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements/development.txt .
RUN pip install --no-cache-dir -r development.txt

# Make scripts executable
COPY scripts/entrypoint.sh scripts/start-dev.sh ./
RUN chmod +x entrypoint.sh start-dev.sh

# Set entrypoint
ENTRYPOINT ["./entrypoint.sh"]

# Default command
CMD ["./start-dev.sh"]</code></pre>
            
            <p><code>Dockerfile</code> (for production)</p>
            <pre><code># Build stage
FROM python:3.9-slim AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /build

# Copy requirements
COPY requirements/production.txt .
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /build/wheels -r production.txt

# Final stage
FROM python:3.9-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_APP=app \
    FLASK_ENV=production

# Create non-root user
RUN addgroup --system app && adduser --system --group app

# Set working directory
WORKDIR /app

# Install runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq5 \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy wheels and install dependencies
COPY --from=builder /build/wheels /wheels
COPY --from=builder /build/production.txt .
RUN pip install --no-cache-dir --no-index --find-links=/wheels -r production.txt \
    && rm -rf /wheels production.txt

# Copy application code
COPY . .

# Make scripts executable and fix ownership
RUN chmod +x scripts/entrypoint.sh && \
    chown -R app:app /app

# Switch to non-root user
USER app

# Set entrypoint
ENTRYPOINT ["./scripts/entrypoint.sh"]

# Run gunicorn
CMD ["gunicorn", "app:app", "--bind", "0.0.0.0:8000", "--workers", "4"]</code></pre>
            
            <h4>Docker Compose Files</h4>
            
            <p><code>docker-compose.yml</code> (base configuration)</p>
            <pre><code>version: '3'

services:
  web:
    build: .
    depends_on:
      - db
    restart: unless-stopped
    environment:
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/flask_app
  
  db:
    image: postgres:13
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=flask_app
    restart: unless-stopped

volumes:
  postgres_data:</code></pre>
            
            <p><code>docker-compose.override.yml</code> (development overrides, applied automatically)</p>
            <pre><code>version: '3'

services:
  web:
    build:
      context: .
      dockerfile: Dockerfile.dev
    ports:
      - "8000:8000"
      - "5678:5678"  # For remote debugging
    volumes:
      - .:/app
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
  
  db:
    ports:
      - "5432:5432"
  
  # Additional development services
  adminer:
    image: adminer:latest
    ports:
      - "8080:8080"
    depends_on:
      - db
    restart: unless-stopped</code></pre>
            
            <p><code>docker-compose.prod.yml</code> (production overrides)</p>
            <pre><code>version: '3'

services:
  web:
    build:
      context: .
      dockerfile: Dockerfile
    expose:
      - "8000"
    environment:
      - FLASK_ENV=production
      - LOG_LEVEL=INFO
    volumes:
      - static_data:/app/app/static
    deploy:
      replicas: 2
      restart_policy:
        condition: any
  
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
      - static_data:/static
    depends_on:
      - web
  
  db:
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh

volumes:
  postgres_data:
  static_data:</code></pre>
            
            <h4>Helper Scripts</h4>
            
            <p><code>scripts/entrypoint.sh</code></p>
            <pre><code>#!/bin/bash
set -e

# Wait for database to be ready
if [ "$DATABASE_URL" ]; then
  echo "Waiting for database..."
  
  RETRIES=5
  until psql $DATABASE_URL -c "select 1" > /dev/null 2>&1 || [ $RETRIES -eq 0 ]; do
    echo "Waiting for database to be available... $((RETRIES--)) remaining attempts..."
    sleep 1
  done
fi

# Run database migrations
if [ "$FLASK_ENV" = "production" ]; then
  echo "Running migrations..."
  flask db upgrade
fi

# Execute the command passed to docker run
exec "$@"</code></pre>
            
            <p><code>scripts/start-dev.sh</code></p>
            <pre><code>#!/bin/bash
set -e

# Run any development setup (e.g., migrations)
echo "Running development setup..."
flask db upgrade

# Start development server with debugger enabled
echo "Starting development server..."
python -m debugpy --listen 0.0.0.0:5678 --wait-for-client -m flask run --host=0.0.0.0 --port=8000</code></pre>
            
            <h4>Using the Application</h4>
            
            <p>Development workflow:</p>
            <pre><code># Start development environment
docker-compose up -d

# View logs
docker-compose logs -f

# Run tests
docker-compose exec web pytest

# Add a new dependency
# 1. Add to requirements/base.txt or requirements/development.txt
# 2. Rebuild the container
docker-compose build
docker-compose up -d</code></pre>
            
            <p>Production deployment:</p>
            <pre><code># Build and start production environment
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Scale web service
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d --scale web=3</code></pre>
            
            <p>This example demonstrates:</p>
            <ul>
                <li>Separate Dockerfiles for development and production</li>
                <li>Multi-stage builds for production to minimize image size</li>
                <li>Organized requirements files by environment</li>
                <li>Docker Compose configurations for different environments</li>
                <li>Helper scripts for common tasks</li>
                <li>Environment-specific configuration</li>
                <li>Database migration handling</li>
                <li>Development aids like debugger support</li>
            </ul>
            
            <p>By following these patterns, you can create a robust, scalable workflow for managing dependencies in your Python applications.</p>
        </section>

        <section class="best-practices-summary">
            <h3>Best Practices Summary</h3>
            
            <h4>Dependency Management</h4>
            <ul>
                <li><strong>Always pin versions</strong> in requirements files</li>
                <li><strong>Organize requirements by environment</strong> (base, development, production)</li>
                <li><strong>Use multi-stage builds</strong> for production images</li>
                <li><strong>Leverage layer caching</strong> by ordering Dockerfile instructions properly</li>
                <li><strong>Consider security</strong> by regularly scanning dependencies for vulnerabilities</li>
                <li><strong>Minimize image size</strong> by removing build dependencies and caches</li>
                <li><strong>Use non-root users</strong> for running applications</li>
            </ul>
            
            <h4>Development Workflow</h4>
            <ul>
                <li><strong>Use volume mounts</strong> for fast development cycles</li>
                <li><strong>Include debugging tools</strong> in development environments</li>
                <li><strong>Create helper scripts</strong> for common tasks</li>
                <li><strong>Document procedures</strong> for adding dependencies</li>
                <li><strong>Use Docker Compose</strong> for managing multiple services</li>
                <li><strong>Implement health checks</strong> to verify container readiness</li>
            </ul>
            
            <h4>Production Deployment</h4>
            <ul>
                <li><strong>Use production-optimized images</strong> (slim, multi-stage builds)</li>
                <li><strong>Implement proper logging</strong> for observability</li>
                <li><strong>Set up health monitoring</strong> for containers</li>
                <li><strong>Plan for scaling</strong> with stateless application design</li>
                <li><strong>Consider orchestration</strong> for complex deployments (Kubernetes, ECS)</li>
                <li><strong>Implement robust error handling</strong> for application resilience</li>
            </ul>
        </section>

        <section class="exercise">
            <h3>Exercise: Converting a Python Project to Use Docker</h3>
            
            <p>Let's apply what we've learned with a practical exercise. You'll convert an existing Python project to use Docker with best practices for dependency management.</p>
            
            <h4>Starting Point: A Simple Flask Application</h4>
            <p>Imagine you have this basic Flask application structure:</p>
            
            <pre><code>my_flask_app/
├── app.py
├── requirements.txt
└── templates/
    └── index.html</code></pre>
            
            <p>With these files:</p>
            
            <p><code>app.py</code>:</p>
            <pre><code>from flask import Flask, render_template
import os

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0')</code></pre>
            
            <p><code>requirements.txt</code>:</p>
            <pre><code>flask==2.0.1</code></pre>
            
            <p><code>templates/index.html</code>:</p>
            <pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Flask App&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;Hello, Docker!&lt;/h1&gt;
    &lt;p&gt;This Flask application is running in a Docker container.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
            
            <h4>Exercise Steps</h4>
            
            <ol>
                <li><strong>Restructure the Project</strong>
                    <p>Create a more comprehensive project structure:</p>
                    <pre><code>my_flask_app/
├── .dockerignore
├── .gitignore
├── Dockerfile
├── Dockerfile.dev
├── docker-compose.yml
├── requirements/
│   ├── base.txt
│   ├── development.txt
│   └── production.txt
├── app/
│   ├── __init__.py
│   ├── app.py
│   └── templates/
│       └── index.html
└── scripts/
    └── entrypoint.sh</code></pre>
                </li>
                <li><strong>Split Requirements</strong>
                    <p>Create separate requirements files:</p>
                    <p><code>requirements/base.txt</code>:</p>
                    <pre><code>flask==2.0.1
gunicorn==20.1.0</code></pre>
                    
                    <p><code>requirements/development.txt</code>:</p>
                    <pre><code>-r base.txt
pytest==6.2.5
black==21.8b0</code></pre>
                    
                    <p><code>requirements/production.txt</code>:</p>
                    <pre><code>-r base.txt</code></pre>
                </li>
                <li><strong>Create Dockerfiles</strong>
                    <p>Create a development and production Dockerfile:</p>
                    <p><code>Dockerfile.dev</code>:</p>
                    <pre><code>FROM python:3.9-slim

WORKDIR /app

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_APP=app \
    FLASK_ENV=development

COPY requirements/development.txt .
RUN pip install --no-cache-dir -r development.txt

# During development, we'll mount the code as a volume
# so we don't need to copy it here

CMD ["flask", "run", "--host=0.0.0.0", "--port=5000"]</code></pre>
                    
                    <p><code>Dockerfile</code> (for production):</p>
                    <pre><code>FROM python:3.9-slim

WORKDIR /app

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    FLASK_APP=app \
    FLASK_ENV=production

COPY requirements/production.txt .
RUN pip install --no-cache-dir -r production.txt

COPY . .

RUN addgroup --system app && adduser --system --group app
USER app

EXPOSE 5000

CMD ["gunicorn", "app:app", "--bind", "0.0.0.0:5000"]</code></pre>
                </li>
                <li><strong>Set Up Docker Compose</strong>
                    <p>Create the Docker Compose file:</p>
                    <p><code>docker-compose.yml</code>:</p>
                    <pre><code>version: '3'

services:
  web:
    build:
      context: .
      dockerfile: Dockerfile.dev
    ports:
      - "5000:5000"
    volumes:
      - .:/app
    environment:
      - FLASK_ENV=development
      - FLASK_APP=app</code></pre>
                </li>
                <li><strong>Create Entry Point Script</strong>
                    <p><code>scripts/entrypoint.sh</code>:</p>
                    <pre><code>#!/bin/bash
set -e

# Execute command
exec "$@"</code></pre>
                </li>
                <li><strong>Update Application Structure</strong>
                    <p><code>app/__init__.py</code>:</p>
                    <pre><code>from flask import Flask

app = Flask(__name__)

from app import app as application</code></pre>
                    
                    <p><code>app/app.py</code> (updated from the original):</p>
                    <pre><code>from flask import render_template
from app import app

@app.route('/')
def index():
    return render_template('index.html')</code></pre>
                </li>
                <li><strong>Create .dockerignore File</strong>
                    <p><code>.dockerignore</code>:</p>
                    <pre><code>__pycache__
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
.venv/
.git/
.gitignore
.env
.vscode
*.log</code></pre>
                </li>
                <li><strong>Build and Run the Application</strong>
                    <pre><code># Start development environment
docker-compose up

# Access the application at http://localhost:5000</code></pre>
                </li>
            </ol>
            
            <h4>Challenge Extensions</h4>
            <ol>
                <li>Add a database service (e.g., PostgreSQL) to Docker Compose</li>
                <li>Create a production override file (docker-compose.prod.yml)</li>
                <li>Add health checks to the containers</li>
                <li>Implement a multi-stage build for the production Dockerfile</li>
                <li>Add a Docker Compose service for running tests</li>
            </ol>
            
            <p>This exercise will give you hands-on experience with Docker and dependency management in a Python application.</p>
        </section>

        <section class="conclusion">
            <h3>Conclusion</h3>
            

            <p>Docker has fundamentally changed how we manage dependencies in Python applications. By containerizing your applications, you can:</p>
            
            <ul>
                <li><strong>Ensure consistency</strong> across development, testing, and production environments</li>
                <li><strong>Simplify onboarding</strong> for new team members</li>
                <li><strong>Manage complex dependencies</strong> more effectively</li>
                <li><strong>Isolate applications</strong> to avoid conflicts</li>
                <li><strong>Scale deployments</strong> more easily</li>
                <li><strong>Version entire environments</strong>, not just code</li>
                <li><strong>Improve security</strong> through isolation and minimal images</li>
            </ul>
            
            <p>As we've seen throughout this session, Docker offers powerful solutions to the complex challenge of dependency management in Python applications. Whether you're working on a simple Flask application or a complex microservices architecture, Docker provides tools and patterns that make dependency management more reliable and reproducible.</p>
            
            <p>In our upcoming sessions, we'll build on this foundation as we explore more advanced Python concepts and start building real-world applications that leverage Docker for consistent environments across development and deployment.</p>
            
            <p>Remember: "It works on my machine" is no longer an excuse in the Docker era!</p>
        </section>

        <section class="docker-security">
            <h3>Appendix: Docker Security Considerations</h3>
            
            <p>When using Docker for dependency management, security should be a top consideration:</p>
            
            <h4>1. Base Image Selection</h4>
            <p>Choose trusted base images from official repositories:</p>
            <ul>
                <li>Use official Python images (<code>python:3.x</code>) rather than arbitrary base images</li>
                <li>Consider slim variants (<code>python:3.x-slim</code>) to reduce attack surface</li>
                <li>Pin to specific versions using SHA digests for immutability:
                    <pre><code>FROM python:3.9-slim@sha256:1c4b7c9bade4c1c8418e38a8e606642aeefb87c2060ec4ba6d7cc8cb0c3fff57</code></pre>
                </li>
            </ul>
            
            <h4>2. Dependency Scanning</h4>
            <p>Integrate security scanning into your workflow:</p>
            <pre><code># Using safety in your Dockerfile
RUN pip install safety && \
    safety check -r requirements.txt && \
    pip uninstall -y safety</code></pre>
            
            <p>Or as part of CI/CD:</p>
            <pre><code># GitLab CI example
dependency-scan:
  image: python:3.9-slim
  script:
    - pip install safety
    - safety check -r requirements.txt</code></pre>
            
            <h4>3. Non-Root Users</h4>
            <p>Avoid running containers as root:</p>
            <pre><code># Create a non-root user
RUN addgroup --system app && \
    adduser --system --group app

# Set ownership
RUN chown -R app:app /app

# Switch to non-root user
USER app</code></pre>
            
            <h4>4. Minimal Images</h4>
            <p>Keep images as small as possible to reduce attack surface:</p>
            <ul>
                <li>Use multi-stage builds to exclude build tools from final image</li>
                <li>Remove package manager caches and temporary files</li>
                <li>Avoid installing unnecessary packages</li>
            </ul>
            
            <h4>5. Secret Management</h4>
            <p>Never hardcode secrets in Dockerfiles or images:</p>
            <ul>
                <li>Use environment variables for configuration</li>
                <li>Consider Docker secrets or dedicated secrets management tools</li>
                <li>Don't use build arguments for secrets (they're visible in image history)</li>
            </ul>
            
            <h4>6. Image Signing and Verification</h4>
            <p>Consider signing your Docker images to ensure integrity:</p>
            <pre><code># Sign an image with Docker Content Trust
export DOCKER_CONTENT_TRUST=1
docker push mycompany/myapp:1.0.0</code></pre>
            
            <h4>7. Regular Updates</h4>
            <p>Keep base images and dependencies updated to patch security vulnerabilities:</p>
            <ul>
                <li>Implement automated workflows to rebuild images regularly</li>
                <li>Use tools like Dependabot to notify of dependency updates</li>
                <li>Balance stability with security needs</li>
            </ul>
            
            <h4>8. Container Runtime Security</h4>
            <p>Secure your containers at runtime:</p>
            <ul>
                <li>Limit container capabilities and resources</li>
                <li>Use read-only file systems where possible</li>
                <li>Implement network policies to restrict container communication</li>
            </ul>
            
            <pre><code># Docker Compose example with security constraints
services:
  web:
    image: myapp:1.0
    read_only: true
    tmpfs:
      - /tmp
      - /var/run
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE</code></pre>
        </section>

        <section class="ci-cd-integration">
            <h3>Appendix: CI/CD Integration for Dependency Management</h3>
            
            <p>Automating dependency management with CI/CD pipelines ensures consistent handling across your development workflow:</p>
            
            <h4>GitHub Actions Example</h4>
            <pre><code>name: Docker CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pip-tools safety
          
      - name: Compile dependencies
        run: |
          pip-compile requirements.in
          
      - name: Check for security vulnerabilities
        run: |
          safety check -r requirements.txt
      
      - name: Build and test Docker image
        run: |
          docker build -t myapp:test .
          docker run --rm myapp:test pytest
      
      - name: Push to registry
        if: github.event_name != 'pull_request'
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker tag myapp:test mycompany/myapp:${{ github.sha }}
          docker push mycompany/myapp:${{ github.sha }}
</code></pre>
            
            <h4>Dependency Update Automation</h4>
            <p>GitHub example with Dependabot for automatic dependency updates:</p>
            
            <pre><code># .github/dependabot.yml
version: 2
updates:
  # Python dependencies
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "weekly"
    allow:
      # Allow only direct dependencies
      - dependency-type: "direct"
    commit-message:
      prefix: "pip"
    open-pull-requests-limit: 10
    
  # Docker dependencies
  - package-ecosystem: "docker"
    directory: "/"
    schedule:
      interval: "weekly"
    commit-message:
      prefix: "docker"
    open-pull-requests-limit: 5</code></pre>
            
            <h4>Matrix Testing</h4>
            <p>Test against multiple Python versions and dependency sets:</p>
            
            <pre><code>name: Matrix Testing

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10]
        dependency-set: [minimal, latest]
    
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v2
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ "${{ matrix.dependency-set }}" == "minimal" ]; then
            pip install -r requirements/minimal.txt
          else
            pip install -r requirements/latest.txt
          fi
      
      - name: Run tests
        run: |
          pytest</code></pre>
            
            <h4>Dependency Locking in CI</h4>
            <p>Ensure dependencies are locked and up-to-date:</p>
            
            <pre><code>name: Dependency Check

on:
  push:
    paths:
      - 'requirements/**'
      - 'pyproject.toml'
      - 'Pipfile'

jobs:
  check-deps:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      
      - name: Check if dependencies are up-to-date
        run: |
          # For pip-tools
          pip install pip-tools
          pip-compile --check requirements.in
          
          # OR for Poetry
          # pip install poetry
          # poetry lock --check
          
          # OR for Pipenv
          # pip install pipenv
          # pipenv lock --requirements > requirements.txt
          # pipenv requirements --dev-only > dev-requirements.txt</code></pre>
        </section>

        <section class="monitoring">
            <h3>Appendix: Monitoring Dependencies in Production</h3>
            
            <p>Once your application is deployed, ongoing monitoring of dependencies is crucial:</p>
            
            <h4>1. Dependency Tracking</h4>
            <p>Export a manifest of installed packages in production:</p>
            
            <pre><code># Add to your container startup
pip freeze > /app/installed_packages.txt

# Or for more detail
pip list --format=json > /app/package_details.json</code></pre>
            
            <h4>2. Security Monitoring</h4>
            <p>Continuously monitor for vulnerabilities:</p>
            
            <ul>
                <li><strong>Container scanning</strong>: Tools like Trivy, Clair, or Snyk</li>
                <li><strong>Runtime monitoring</strong>: Tools like Falco</li>
                <li><strong>Software composition analysis</strong>: Track open source components</li>
            </ul>
            
            <h4>3. Dependency Visualization</h4>
            <p>Visualize dependency relationships to understand impact:</p>
            
            <pre><code># Install pipdeptree
pip install pipdeptree

# Generate visualization
pipdeptree --graph-output png > dependencies.png

# OR generate in DOT format for further processing
pipdeptree --graph-output dot > dependencies.dot</code></pre>
            
            <h4>4. Automating Dependency Updates</h4>
            <p>Create automated processes for safe dependency updates:</p>
            
            <ol>
                <li>Automatically create PRs for dependency updates</li>
                <li>Run test suite against the updates</li>
                <li>Deploy to staging environment</li>
                <li>Perform integration tests</li>
                <li>Promote to production if successful</li>
            </ol>
            
            <h4>5. Dependency Drift Detection</h4>
            <p>Detect when actual installed dependencies differ from expected:</p>
            
            <pre><code># Script to check for dependency drift
#!/bin/bash
# check_drift.sh

# Generate current dependencies
pip freeze > current_deps.txt

# Compare with requirements
diff -u requirements.txt current_deps.txt > drift.patch

if [ -s drift.patch ]; then
    echo "Warning: Dependency drift detected"
    cat drift.patch
    exit 1
else
    echo "All dependencies match requirements"
    exit 0
fi</code></pre>
        </section>

        <section class="additional-resources">
            <h3>Additional Resources</h3>
            
            <h4>Documentation</h4>
            <ul>
                <li><a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/" target="_blank">Docker Official: Dockerfile Best Practices</a></li>
                <li><a href="https://docs.docker.com/compose/" target="_blank">Docker Compose Documentation</a></li>
                <li><a href="https://docs.python.org/3/installing/index.html" target="_blank">Python Packaging User Guide</a></li>
                <li><a href="https://pip.pypa.io/en/stable/user_guide/" target="_blank">pip User Guide</a></li>
                <li><a href="https://python-poetry.org/docs/" target="_blank">Poetry Documentation</a></li>
            </ul>
            
            <h4>Tools</h4>
            <ul>
                <li><a href="https://github.com/pypa/pip-tools" target="_blank">pip-tools</a> - For dependency management</li>
                <li><a href="https://github.com/pyupio/safety" target="_blank">safety</a> - For vulnerability scanning</li>
                <li><a href="https://github.com/aquasecurity/trivy" target="_blank">Trivy</a> - For container scanning</li>
                <li><a href="https://github.com/naiquevin/pipdeptree" target="_blank">pipdeptree</a> - For dependency visualization</li>
                <li><a href="https://github.com/hadolint/hadolint" target="_blank">hadolint</a> - For Dockerfile linting</li>
            </ul>
            
            <h4>Articles and Guides</h4>
            <ul>
                <li><a href="https://pythonspeed.com/docker/" target="_blank">Python in Production with Docker</a></li>
                <li><a href="https://testdriven.io/blog/docker-best-practices/" target="_blank">Docker Best Practices for Python Development</a></li>
                <li><a href="https://snyk.io/blog/10-best-practices-to-containerize-python-applications-with-docker/" target="_blank">10 Best Practices to Containerize Python Applications</a></li>
                <li><a href="https://realpython.com/python-docker-packaging-guide/" target="_blank">Real Python: Packaging Python Applications with Docker</a></li>
            </ul>
            
            <h4>Books</h4>
            <ul>
                <li>"Docker for Python Developers" by Joshua Bernstein</li>
                <li>"Python for DevOps" by Noah Gift et al. (O'Reilly)</li>
                <li>"Docker Deep Dive" by Nigel Poulton</li>
                <li>"Python Packaging" by Tarek Ziadé</li>
            </ul>
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Python Full Stack Developer Course. All rights reserved.</p>
    </footer>
</body>
</html>
