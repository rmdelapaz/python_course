contract-tests:
  stage: test
  image: python:3.9
  script:
    - pip install -r requirements.txt
    - pip install pytest openapi-spec-validator pact-python
    - pytest tests/contract
  dependencies:
    - unit-tests

security-tests:
  stage: security
  image: python:3.9
  script:
    - pip install bandit safety
    - bandit -r app/ -o bandit-results.json
    - safety check -r requirements.txt
  artifacts:
    paths:
      - bandit-results.json
  dependencies:
    - unit-tests

deploy-staging:
  stage: deploy-staging
  image: python:3.9
  script:
    - echo "Deploying to staging environment..."
    # Add your deployment steps here
  environment:
    name: staging
    url: https://staging-api.example.com
  only:
    - develop
  dependencies:
    - unit-tests
    - integration-tests
    - contract-tests
    - security-tests

functional-tests:
  stage: test-staging
  image: python:3.9
  script:
    - pip install -r requirements.txt
    - pip install pytest requests
    - pytest tests/functional --host=https://staging-api.example.com
  environment:
    name: staging
    url: https://staging-api.example.com
  only:
    - develop
  dependencies:
    - deploy-staging

performance-tests:
  stage: performance
  image: python:3.9
  script:
    - pip install locust matplotlib pandas
    - mkdir -p performance-results
    - python tests/performance/run_locust.py --host=https://staging-api.example.com --users=50 --spawn-rate=5 --run-time=5m
  artifacts:
    paths:
      - performance-results/
  only:
    - main
  dependencies:
    - deploy-staging

deploy-production:
  stage: deploy-production
  image: python:3.9
  script:
    - echo "Deploying to production environment..."
    # Add your production deployment steps here
  environment:
    name: production
    url: https://api.example.com
  only:
    - main
  when: manual
  dependencies:
    - performance-tests</code></pre>
            </div>
            
            <div class="test-automation-best-practices">
                <h3>Best Practices for Test Automation in CI/CD</h3>
                <ul>
                    <li><strong>Fast Feedback Loop:</strong> Run the fastest tests first to provide quick feedback.</li>
                    <li><strong>Fail Fast:</strong> Configure the pipeline to stop at the first test failure.</li>
                    <li><strong>Parallel Execution:</strong> Run independent tests in parallel to save time.</li>
                    <li><strong>Proper Isolation:</strong> Ensure tests don't interfere with each other.</li>
                    <li><strong>Environment Consistency:</strong> Use containers/VMs to ensure consistent test environments.</li>
                    <li><strong>Artifact Retention:</strong> Save test results, logs, and coverage reports as artifacts.</li>
                    <li><strong>Automated Reporting:</strong> Generate and publish test reports automatically.</li>
                    <li><strong>Notification System:</strong> Alert the team when tests fail.</li>
                    <li><strong>Flaky Test Handling:</strong> Identify and fix or isolate flaky tests.</li>
                    <li><strong>Test Data Management:</strong> Ensure tests have the necessary data without conflicts.</li>
                </ul>
            </div>
            
            <div class="test-environments">
                <h3>Managing Test Environments</h3>
                <p>Proper test environment management is crucial for reliable test automation:</p>
                
                <h4>Environment Types</h4>
                <ul>
                    <li><strong>Development:</strong> For developers to test their changes locally</li>
                    <li><strong>CI:</strong> For running automated tests in the CI pipeline</li>
                    <li><strong>Staging:</strong> For testing integrations and running performance tests</li>
                    <li><strong>Production:</strong> The live environment</li>
                </ul>
                
                <h4>Environment Management Strategies</h4>
                <ul>
                    <li><strong>Containerization:</strong> Use Docker to create consistent environments</li>
                    <li><strong>Infrastructure as Code:</strong> Define environments with tools like Terraform or CloudFormation</li>
                    <li><strong>Database Management:</strong> Use migrations for schema changes and seed data for tests</li>
                    <li><strong>Service Virtualization:</strong> Mock external dependencies for isolated testing</li>
                    <li><strong>Environment Variables:</strong> Configure services differently per environment</li>
                </ul>
                
                <h4>Docker Compose for Local Testing</h4>
                <pre><code># docker-compose.yml for local testing
version: '3.8'

services:
  api:
    build: .
    ports:
      - "5000:5000"
    environment:
      - FLASK_APP=app.py
      - FLASK_ENV=development
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/api_test
      - SECRET_KEY=test-secret-key
    depends_on:
      - db
    volumes:
      - .:/app

  db:
    image: postgres:13
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=api_test
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  test:
    build:
      context: .
      dockerfile: Dockerfile.test
    environment:
      - FLASK_ENV=testing
      - DATABASE_URL=postgresql://postgres:postgres@db:5432/api_test
      - API_URL=http://api:5000
      - SECRET_KEY=test-secret-key
    depends_on:
      - api
      - db
    volumes:
      - .:/app
    command: pytest

volumes:
  postgres_data:</code></pre>
            </div>
        </section>

        <section class="test_driven_development">
            <h2>Test-Driven Development for APIs</h2>
            
            <p>Test-Driven Development (TDD) is a software development approach where tests are written before the implementation code. When applied to API development, TDD helps ensure that your API meets its requirements and maintains high quality as it evolves.</p>
            
            <p>Think of TDD like creating a detailed blueprint before building a house—you define what you want to achieve and verify that your construction meets those specifications at every step.</p>
            
            <div class="tdd-cycle">
                <h3>The TDD Cycle for API Development</h3>
                <ol>
                    <li><strong>Write a Test:</strong> Start by writing a test that defines the expected behavior of your API endpoint or component.</li>
                    <li><strong>Run the Test:</strong> Verify that the test fails (since you haven't implemented the functionality yet).</li>
                    <li><strong>Write the Implementation:</strong> Write the minimal code needed to make the test pass.</li>
                    <li><strong>Run All Tests:</strong> Verify that your implementation passes the new test and doesn't break existing functionality.</li>
                    <li><strong>Refactor:</strong> Clean up your code while ensuring all tests still pass.</li>
                    <li><strong>Repeat:</strong> Continue the cycle with the next piece of functionality.</li>
                </ol>
            </div>
            
            <div class="code-section">
                <h3>TDD Example for a User API in Flask</h3>
                <p>Let's walk through the TDD process for a user authentication API:</p>
                
                <h4>Step 1: Write the First Test</h4>
                <pre><code># tests/test_user_api.py
import pytest
import json
from app import create_app
from app.models import db as _db
from app.models.user import User

@pytest.fixture
def app():
    app = create_app('testing')
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///:memory:'
    
    with app.app_context():
        _db.create_all()
    
    yield app
    
    with app.app_context():
        _db.drop_all()

@pytest.fixture
def client(app):
    return app.test_client()

def test_register_user(client):
    """Test user registration endpoint"""
    # Define test data
    user_data = {
        "username": "testuser",
        "email": "test@example.com",
        "password": "Password123!"
    }
    
    # Make request to the endpoint (which doesn't exist yet)
    response = client.post(
        '/api/users/register',
        data=json.dumps(user_data),
        content_type='application/json'
    )
    
    # Assert expected behavior
    assert response.status_code == 201
    data = json.loads(response.data)
    assert data['username'] == 'testuser'
    assert data['email'] == 'test@example.com'
    assert 'id' in data
    
    # Verify user was created in database
    with app.app_context():
        user = User.query.filter_by(username='testuser').first()
        assert user is not None
        assert user.email == 'test@example.com'</code></pre>
                
                <h4>Step 2: Run the Test (Expect Failure)</h4>
                <pre><code>$ pytest tests/test_user_api.py
FAILED tests/test_user_api.py::test_register_user - ImportError: cannot import name 'create_app'</code></pre>
                
                <h4>Step 3: Implement Minimal Code to Make the Test Pass</h4>
                <pre><code># app/__init__.py
from flask import Flask
from flask_sqlalchemy import SQLAlchemy

db = SQLAlchemy()

def create_app(config_name='default'):
    app = Flask(__name__)
    
    if config_name == 'testing':
        app.config['TESTING'] = True
        app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
    
    db.init_app(app)
    
    from app.api import api_bp
    app.register_blueprint(api_bp, url_prefix='/api')
    
    return app

# app/models/user.py
from app import db
from werkzeug.security import generate_password_hash, check_password_hash

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(50), unique=True, nullable=False)
    email = db.Column(db.String(100), unique=True, nullable=False)
    password_hash = db.Column(db.String(128))
    
    def set_password(self, password):
        self.password_hash = generate_password_hash(password)
    
    def check_password(self, password):
        return check_password_hash(self.password_hash, password)

# app/api/__init__.py
from flask import Blueprint

api_bp = Blueprint('api', __name__)

from app.api import users

# app/api/users.py
from flask import request, jsonify
from app import db
from app.models.user import User
from app.api import api_bp

@api_bp.route('/users/register', methods=['POST'])
def register_user():
    data = request.get_json()
    
    if not data or 'username' not in data or 'email' not in data or 'password' not in data:
        return jsonify({"error": "Missing required fields"}), 400
    
    # Check if user already exists
    if User.query.filter_by(username=data['username']).first():
        return jsonify({"error": "Username already exists"}), 409
    
    if User.query.filter_by(email=data['email']).first():
        return jsonify({"error": "Email already exists"}), 409
    
    # Create new user
    user = User(
        username=data['username'],
        email=data['email']
    )
    user.set_password(data['password'])
    
    db.session.add(user)
    db.session.commit()
    
    return jsonify({
        "id": user.id,
        "username": user.username,
        "email": user.email
    }), 201</code></pre>
                
                <h4>Step 4: Run the Test Again (Should Pass)</h4>
                <pre><code>$ pytest tests/test_user_api.py
PASSED tests/test_user_api.py::test_register_user</code></pre>
                
                <h4>Step 5: Write the Next Test (for Login)</h4>
                <pre><code>def test_login_user(client, app):
    """Test user login endpoint"""
    # Create a test user
    with app.app_context():
        user = User(username='logintest', email='login@example.com')
        user.set_password('Password123!')
        db.session.add(user)
        db.session.commit()
    
    # Login with correct credentials
    response = client.post(
        '/api/auth/login',
        data=json.dumps({
            "username": "logintest",
            "password": "Password123!"
        }),
        content_type='application/json'
    )
    
    # Assert expected behavior
    assert response.status_code == 200
    data = json.loads(response.data)
    assert 'access_token' in data
    assert 'refresh_token' in data
    
    # Login with incorrect password
    response = client.post(
        '/api/auth/login',
        data=json.dumps({
            "username": "logintest",
            "password": "wrongpassword"
        }),
        content_type='application/json'
    )
    
    # Should fail
    assert response.status_code == 401</code></pre>
                
                <h4>Step 6: Run the Test (Expect Failure)</h4>
                <pre><code>$ pytest tests/test_user_api.py::test_login_user
FAILED tests/test_user_api.py::test_login_user - assert 404 == 200
E   +  where 404 = <Response streamed [404 NOT FOUND]>.status_code</code></pre>
                
                <h4>Step 7: Implement the Login Functionality</h4>
                <pre><code># app/api/auth.py
import datetime
import jwt
from flask import request, jsonify, current_app
from app import db
from app.models.user import User
from app.api import api_bp

@api_bp.route('/auth/login', methods=['POST'])
def login():
    data = request.get_json()
    
    if not data or 'username' not in data or 'password' not in data:
        return jsonify({"error": "Missing username or password"}), 400
    
    user = User.query.filter_by(username=data['username']).first()
    
    if not user or not user.check_password(data['password']):
        return jsonify({"error": "Invalid credentials"}), 401
    
    # Generate access token
    access_token = jwt.encode(
        {
            'user_id': user.id,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=15)
        },
        current_app.config.get('SECRET_KEY', 'dev-key'),
        algorithm='HS256'
    )
    
    # Generate refresh token
    refresh_token = jwt.encode(
        {
            'user_id': user.id,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(days=1)
        },
        current_app.config.get('SECRET_KEY', 'dev-key'),
        algorithm='HS256'
    )
    
    return jsonify({
        "access_token": access_token,
        "refresh_token": refresh_token,
        "user": {
            "id": user.id,
            "username": user.username
        }
    }), 200

# Update app/__init__.py to import the auth module
from app.api import api_bp

def create_app(config_name='default'):
    app = Flask(__name__)
    
    if config_name == 'testing':
        app.config['TESTING'] = True
        app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
        app.config['SECRET_KEY'] = 'test-secret-key'
    
    db.init_app(app)
    
    from app.api import api_bp
    app.register_blueprint(api_bp, url_prefix='/api')
    
    return app

# Update app/api/__init__.py to import the auth module
from flask import Blueprint

api_bp = Blueprint('api', __name__)

from app.api import users, auth</code></pre>
                
                <h4>Step 8: Run the Tests Again (Should Pass)</h4>
                <pre><code>$ pytest tests/test_user_api.py
PASSED tests/test_user_api.py::test_register_user
PASSED tests/test_user_api.py::test_login_user</code></pre>
                
                <h4>Step 9: Write the Next Test (Protected Endpoint)</h4>
                <pre><code>def test_protected_endpoint(client, app):
    """Test protected endpoint requiring authentication"""
    # Create a test user
    with app.app_context():
        user = User(username='protectedtest', email='protected@example.com')
        user.set_password('Password123!')
        db.session.add(user)
        db.session.commit()
    
    # Login to get token
    login_response = client.post(
        '/api/auth/login',
        data=json.dumps({
            "username": "protectedtest",
            "password": "Password123!"
        }),
        content_type='application/json'
    )
    
    data = json.loads(login_response.data)
    token = data['access_token']
    
    # Access protected endpoint with token
    response = client.get(
        '/api/users/profile',
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # Should succeed
    assert response.status_code == 200
    profile_data = json.loads(response.data)
    assert profile_data['username'] == 'protectedtest'
    
    # Access without token
    response = client.get('/api/users/profile')
    
    # Should fail
    assert response.status_code == 401
    
    # Access with invalid token
    response = client.get(
        '/api/users/profile',
        headers={"Authorization": "Bearer invalid_token"}
    )
    
    # Should fail
    assert response.status_code == 401</code></pre>
                
                <h4>Step 10: Continue the TDD Cycle</h4>
                <p>Continue implementing features, writing tests first, then implementing the code to make them pass.</p>
            </div>
            
            <div class="tdd-benefits">
                <h3>Benefits of TDD for API Development</h3>
                <ul>
                    <li><strong>Clear Requirements:</strong> Tests serve as executable specifications, making requirements explicit.</li>
                    <li><strong>Focused Development:</strong> Writing tests first helps focus on one feature at a time.</li>
                    <li><strong>Better Design:</strong> TDD encourages more modular, loosely coupled code.</li>
                    <li><strong>Regression Prevention:</strong> Tests catch regressions early when code is modified.</li>
                    <li><strong>Documentation:</strong> Tests serve as living documentation of how the API should behave.</li>
                    <li><strong>Confidence in Refactoring:</strong> Tests provide a safety net when refactoring or adding features.</li>
                    <li><strong>Higher Quality:</strong> TDD typically results in fewer bugs and higher quality code.</li>
                </ul>
            </div>
            
            <div class="tdd-challenges">
                <h3>Challenges and Tips for API TDD</h3>
                <ul>
                    <li><strong>Learning Curve:</strong> TDD requires practice and discipline to master.</li>
                    <li><strong>External Dependencies:</strong> Testing APIs with external dependencies can be challenging.
                        <ul>
                            <li>Solution: Use mocks, stubs, or test doubles for external services.</li>
                        </ul>
                    </li>
                    <li><strong>Database Interactions:</strong> Database tests can be slow and complex.
                        <ul>
                            <li>Solution: Use in-memory databases for testing or transaction rollbacks.</li>
                        </ul>
                    </li>
                    <li><strong>Authentication/Authorization:</strong> Authentication flows can be complex to test.
                        <ul>
                            <li>Solution: Use test fixtures to set up authenticated contexts.</li>
                        </ul>
                    </li>
                    <li><strong>Test Data Management:</strong> Managing test data across tests can be challenging.
                        <ul>
                            <li>Solution: Use factories or fixtures to generate test data consistently.</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </section>

        <section class="testing_tools">
            <h2>Python API Testing Tools and Frameworks</h2>
            
            <p>Python offers a rich ecosystem of tools and frameworks for API testing, catering to different testing needs and preferences. This section provides an overview of the most popular and useful tools to consider for your API testing strategy.</p>
            
            <div class="tools-comparison">
                <h3>Unit Testing Frameworks</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>pytest</td>
                            <td>Modern Python testing framework</td>
                            <td>
                                - Powerful fixture system<br>
                                - Concise syntax<br>
                                - Extensive plugin ecosystem<br>
                                - Parameterized tests
                            </td>
                            <td>Most Python projects, from small to large scale</td>
                        </tr>
                        <tr>
                            <td>unittest</td>
                            <td>Python's built-in testing framework</td>
                            <td>
                                - Built into Python standard library<br>
                                - xUnit style<br>
                                - Test discovery<br>
                                - Test runners
                            </td>
                            <td>Projects needing standard library compatibility</td>
                        </tr>
                        <tr>
                            <td>nose2</td>
                            <td>Extension of unittest</td>
                            <td>
                                - Plugin architecture<br>
                                - Backward compatibility with nose<br>
                                - Extended test discovery
                            </td>
                            <td>Projects transitioning from nose or unittest</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>API Client Libraries</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>requests</td>
                            <td>HTTP library for Python</td>
                            <td>
                                - Simple, elegant API<br>
                                - Session management<br>
                                - Authentication support<br>
                                - JSON handling
                            </td>
                            <td>General HTTP requests, API client testing</td>
                        </tr>
                        <tr>
                            <td>httpx</td>
                            <td>Next-generation HTTP client</td>
                            <td>
                                - Async/await support<br>
                                - Similar API to requests<br>
                                - HTTP/2 support<br>
                                - Type hints
                            </td>
                            <td>Modern Python projects, async testing</td>
                        </tr>
                        <tr>
                            <td>aiohttp</td>
                            <td>Async HTTP client/server</td>
                            <td>
                                - Fully async<br>
                                - WebSocket support<br>
                                - Server and client functionality
                            </td>
                            <td>Async-focused applications, high-performance testing</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>API Testing Frameworks</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Tavern</td>
                            <td>YAML-based API testing</td>
                            <td>
                                - YAML test definitions<br>
                                - pytest integration<br>
                                - Response validation<br>
                                - Simple, readable syntax
                            </td>
                            <td>Readable, declarative API tests</td>
                        </tr>
                        <tr>
                            <td>Postman/Newman</td>
                            <td>API testing platform</td>
                            <td>
                                - GUI for manual testing<br>
                                - Collection runner<br>
                                - Environment variables<br>
                                - Newman CLI for automation
                            </td>
                            <td>Mixed teams, comprehensive API testing</td>
                        </tr>
                        <tr>
                            <td>pytest-httpx</td>
                            <td>httpx mocking for pytest</td>
                            <td>
                                - Mock HTTP responses<br>
                                - Verify request parameters<br>
                                - Integration with pytest
                            </td>
                            <td>Mocking HTTP requests in pytest</td>
                        </tr>
                        <tr>
                            <td>Schemathesis</td>
                            <td>Property-based API testing</td>
                            <td>
                                - Generate tests from OpenAPI specs<br>
                                - Fuzz testing<br>
                                - Schema validation
                            </td>
                            <td>Automated test generation, contract testing</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>Mock and Stub Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>unittest.mock</td>
                            <td>Python's built-in mocking library</td>
                            <td>
                                - Part of standard library<br>
                                - Mock objects and functions<br>
                                - Patch decorators
                            </td>
                            <td>General mocking needs</td>
                        </tr>
                        <tr>
                            <td>responses</td>
                            <td>HTTP request mocking</td>
                            <td>
                                - Mock HTTP responses<br>
                                - Works with requests library<br>
                                - Exception simulation
                            </td>
                            <td>Mocking HTTP responses in tests</td>
                        </tr>
                        <tr>
                            <td>pytest-mock</td>
                            <td>pytest fixture for unittest.mock</td>
                            <td>
                                - Convenient fixture interface<br>
                                - Automatic teardown<br>
                                - Integration with pytest
                            </td>
                            <td>Simpler mocking in pytest</td>
                        </tr>
                        <tr>
                            <td>VCR.py</td>
                            <td>Record and replay HTTP interactions</td>
                            <td>
                                - Record real HTTP interactions<br>
                                - Replay in future test runs<br>
                                - Customizable cassette storage
                            </td>
                            <td>Integration testing with real services</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>Performance Testing Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Locust</td>
                            <td>Python-based load testing</td>
                            <td>
                                - Python scripts for test scenarios<br>
                                - Distributed load generation<br>
                                - Real-time web UI<br>
                                - Customizable user behavior
                            </td>
                            <td>Developer-friendly load testing</td>
                        </tr>
                        <tr>
                            <td>pytest-benchmark</td>
                            <td>Benchmarking plugin for pytest</td>
                            <td>
                                - Detailed timing statistics<br>
                                - Comparison between runs<br>
                                - Customizable fixtures
                            </td>
                            <td>Code benchmarking in pytest</td>
                        </tr>
                        <tr>
                            <td>Molotov</td>
                            <td>Async load testing</td>
                            <td>
                                - Asyncio-based<br>
                                - Coroutines for scenarios<br>
                                - Integration with pytest
                            </td>
                            <td>Async API load testing</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>Security Testing Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Bandit</td>
                            <td>Security linter for Python code</td>
                            <td>
                                - Static code analysis<br>
                                - Security vulnerability detection<br>
                                - Configurable rules
                            </td>
                            <td>Identifying common security issues in code</td>
                        </tr>
                        <tr>
                            <td>OWASP ZAP</td>
                            <td>Web application security scanner</td>
                            <td>
                                - Active and passive scanning<br>
                                - Automated security testing<br>
                                - Python API for automation
                            </td>
                            <td>Comprehensive security testing</td>
                        </tr>
                        <tr>
                            <td>Safety</td>
                            <td>Dependency vulnerability checker</td>
                            <td>
                                - Check for vulnerable dependencies<br>
                                - Integration with CI/CD<br>
                                - Database of known vulnerabilities
                            </td>
                            <td>Supply chain security</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="tool-selection">
                <h3>How to Choose the Right Tools</h3>
                <p>Consider these factors when selecting testing tools for your API projects:</p>
                
                <ul>
                    <li><strong>Project Requirements:</strong> Match tools to your specific testing needs.</li>
                    <li><strong>Team Expertise:</strong> Consider the learning curve and existing team knowledge.</li>
                    <li><strong>Integration:</strong> Ensure tools integrate well with your development workflow and CI/CD pipeline.</li>
                    <li><strong>Maintainability:</strong> Choose tools with active development and good documentation.</li>
                    <li><strong>Performance:</strong> Consider the speed and resource usage of the testing tools.</li>
                    <li><strong>Community Support:</strong> Larger communities mean better support and more resources.</li>
                </ul>
            </div>
        </section>

        <section class="conclusion">
            <h2>Conclusion and Next Steps</h2>
            
            <p>Testing is an essential aspect of API development, ensuring that your APIs are reliable, secure, performant, and meet their requirements. By implementing a comprehensive testing strategy that includes various types of tests at different levels, you can build high-quality APIs that stand the test of time.</p>
            
            <div class="summary">
                <h3>Key Takeaways</h3>
                <ul>
                    <li><strong>Follow the Testing Pyramid:</strong> Implement more unit tests than integration tests, and more integration tests than end-to-end tests.</li>
                    <li><strong>Use the Right Tools:</strong> Choose appropriate testing tools for different testing needs.</li>
                    <li><strong>Automate Testing:</strong> Integrate tests into your CI/CD pipeline for continuous validation.</li>
                    <li><strong>Test Beyond Functionality:</strong> Include security, performance, and contract testing in your strategy.</li>
                    <li><strong>Consider TDD:</strong> Test-Driven Development can lead to better design and higher quality code.</li>
                    <li><strong>Balance Coverage and Practicality:</strong> Aim for high test coverage while focusing on critical areas.</li>
                    <li><strong>Keep Tests Maintainable:</strong> Well-structured, readable tests are easier to maintain as your API evolves.</li>
                </ul>
            </div>
            
            <div class="next-steps">
                <h3>Next Steps in Your Testing Journey</h3>
                <ul>
                    <li><strong>Assess Your Current Testing:</strong> Evaluate your existing testing practices and identify areas for improvement.</li>
                    <li><strong>Start Small:</strong> Begin by implementing basic unit and integration tests if you don't have them already.</li>
                    <li><strong>Gradually Expand:</strong> Add more sophisticated tests (security, performance, contract) as your testing maturity increases.</li>
                    <li><strong>Automate:</strong> Set up CI/CD pipelines to run tests automatically.</li>
                    <li><strong>Monitor and Improve:</strong> Regularly review test results and refine your testing strategy.</li>
                    <li><strong>Foster a Testing Culture:</strong> Promote the importance of testing within your team or organization.</li>
                </ul>
            </div>
            
            <div class="resources">
                <h3>Further Learning Resources</h3>
                <ul>
                    <li><strong>Books:</strong>
                        <ul>
                            <li>"API Testing and Development with Postman" by Dave Westerveld</li>
                            <li>"Python Testing with pytest" by Brian Okken</li>
                            <li>"Test-Driven Development with Python" by Harry Percival</li>
                            <li>"Building Microservices" by Sam Newman (includes API testing strategies)</li>
                        </ul>
                    </li>
                    <li><strong>Online Resources:</strong>
                        <ul>
                            <li>OWASP API Security Top 10</li>
                            <li>pytest documentation</li>
                            <li>Postman Learning Center</li>
                            <li>Flask and Django REST Framework testing guides</li>
                        </ul>
                    </li>
                    <li><strong>Courses:</strong>
                        <ul>
                            <li>API Testing Foundations</li>
                            <li>Python Testing for Data Science</li>
                            <li>REST API Testing with Python</li>
                            <li>Web API Development and Documentation with Flask</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <p>Remember that testing is not a one-time activity but an ongoing process. As your API evolves, so should your testing strategy. By continuously refining and improving your tests, you can ensure that your API remains reliable, secure, and performant throughout its lifecycle.</p>
        </section>
    </main>

    <footer>
        <p>Python Full Stack Web Course - API Testing Guide</p>
    </footer>
</body>
</html> "Bearer {{access_token}}"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"username\": \"\",\n    \"email\": \"invalid-email\"\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/users",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "users"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 400 Bad Request\", function () {",
                                    "    pm.response.to.have.status(400);",
                                    "});",
                                    "",
                                    "pm.test(\"Response contains validation errors\", function () {",
                                    "    const responseJson = pm.response.json();",
                                    "    pm.expect(responseJson.errors).to.be.an('object');",
                                    "    pm.expect(responseJson.errors.username).to.be.an('array');",
                                    "    pm.expect(responseJson.errors.email).to.be.an('array');",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Resource Not Found",
                    "request": {
                        "method": "GET",
                        "header": [],
                        "url": {
                            "raw": "{{baseUrl}}/api/products/99999",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "99999"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 404 Not Found\", function () {",
                                    "    pm.response.to.have.status(404);",
                                    "});",
                                    "",
                                    "pm.test(\"Response contains error message\", function () {",
                                    "    const responseJson = pm.response.json();",
                                    "    pm.expect(responseJson.error).to.be.a('string');",
                                    "});"
                                ]
                            }
                        }
                    ]
                }
            ]
        }
    ],
    "event": [
        {
            "listen": "prerequest",
            "script": {
                "type": "text/javascript",
                "exec": [""]
            }
        },
        {
            "listen": "test",
            "script": {
                "type": "text/javascript",
                "exec": [""]
            }
        }
    ],
    "variable": [
        {
            "key": "baseUrl",
            "value": "http://localhost:5000",
            "type": "string"
        }
    ]
}</code></pre>

                <h3>Running Newman from Command Line</h3>
                <pre><code># Install Newman
npm install -g newman

# Run the collection with environment variables
newman run api_tests.json -e environment.json

# Generate HTML report
newman run api_tests.json -e environment.json -r html

# Run only a specific folder
newman run api_tests.json -e environment.json --folder "Products"</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Functional Testing Best Practices</h3>
                <ul>
                    <li><strong>Test Against API Contract:</strong> Focus on testing endpoints against their documented specifications.</li>
                    <li><strong>Cover Main Response Codes:</strong> Test successful responses (2xx), client errors (4xx), and server errors (5xx).</li>
                    <li><strong>Test Content Types:</strong> Verify the API responds with the correct content types.</li>
                    <li><strong>Validate Response Structure:</strong> Check that responses conform to the expected schema.</li>
                    <li><strong>Test Pagination and Filtering:</strong> Verify pagination, sorting, and filtering work as expected.</li>
                    <li><strong>Check Response Headers:</strong> Validate important headers (e.g., cache control, content type).</li>
                    <li><strong>Test Authentication/Authorization:</strong> Verify proper handling of unauthenticated or unauthorized requests.</li>
                    <li><strong>Test Input Validation:</strong> Check how the API handles invalid or malformed inputs.</li>
                    <li><strong>Clean Up Test Data:</strong> Remove test data after tests to avoid polluting the environment.</li>
                </ul>
            </div>
            
            <div class="tools-comparison">
                <h3>Functional Testing Tools Comparison</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Strengths</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>pytest + requests</td>
                            <td>
                                - Flexible and customizable<br>
                                - Integrates with Python test infrastructure<br>
                                - Programmatic control of tests
                            </td>
                            <td>Developer-centric testing, integration with CI/CD, complex test logic</td>
                        </tr>
                        <tr>
                            <td>Postman/Newman</td>
                            <td>
                                - Visual interface for test creation<br>
                                - Easy to learn and use<br>
                                - Strong reporting features
                            </td>
                            <td>Mixed teams, API documentation, less technical testers</td>
                        </tr>
                        <tr>
                            <td>Tavern</td>
                            <td>
                                - YAML-based tests<br>
                                - Simple, concise syntax<br>
                                - pytest integration
                            </td>
                            <td>Simple API testing, readable test cases</td>
                        </tr>
                        <tr>
                            <td>Karate DSL</td>
                            <td>
                                - Combines API test automation, mocks, performance testing<br>
                                - No programming required<br>
                                - Strong reporting
                            </td>
                            <td>Comprehensive API testing, testing by QA teams</td>
                        </tr>
                        <tr>
                            <td>Robot Framework</td>
                            <td>
                                - Keyword-driven testing<br>
                                - Extensive library support<br>
                                - Acceptance testing focus
                            </td>
                            <td>Teams already using Robot Framework, acceptance testing</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section class="contract_testing">
            <h2>Contract Testing</h2>
            
            <p>Contract testing ensures that the API adheres to its specified contract or specification. It focuses on verifying that the API's request and response formats, status codes, and behaviors match what's documented, providing a guarantee to consumers that the API will work as expected.</p>
            
            <p>Think of contract testing like checking a legal document against a template—it ensures that all the required clauses are present and correctly formatted, even if it doesn't verify that the content makes sense in all contexts.</p>
            
            <div class="how-it-works">
                <h3>How Contract Testing Works</h3>
                <ol>
                    <li>Define the contract (API specification)</li>
                    <li>Generate tests from the contract</li>
                    <li>Run tests against the API implementation</li>
                    <li>Verify that the implementation conforms to the contract</li>
                </ol>
                
                <p>Contract testing is particularly important in microservices architectures, where different teams may be responsible for different services that need to communicate with each other.</p>
            </div>
            
            <div class="openapi-testing">
                <h3>Testing Against OpenAPI Specifications</h3>
                <p>OpenAPI (formerly Swagger) is a common format for API specifications. Here's how to test your API against an OpenAPI spec:</p>
                
                <h4>Sample OpenAPI Specification (openapi.yml)</h4>
                <pre><code>openapi: 3.0.0
info:
  title: Product API
  version: 1.0.0
  description: API for managing products
paths:
  /api/products:
    get:
      summary: Get all products
      responses:
        '200':
          description: Successful operation
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/Product'
    post:
      summary: Create a new product
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ProductInput'
      responses:
        '201':
          description: Product created
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Product'
        '400':
          description: Invalid input
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
  /api/products/{id}:
    get:
      summary: Get a product by ID
      parameters:
        - name: id
          in: path
          required: true
          schema:
            type: integer
      responses:
        '200':
          description: Successful operation
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Product'
        '404':
          description: Product not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'
components:
  schemas:
    Product:
      type: object
      properties:
        id:
          type: integer
        name:
          type: string
        description:
          type: string
        price:
          type: number
          format: float
        category:
          type: string
        created_at:
          type: string
          format: date-time
      required:
        - id
        - name
        - price
    ProductInput:
      type: object
      properties:
        name:
          type: string
          minLength: 3
          maxLength: 100
        description:
          type: string
        price:
          type: number
          format: float
          minimum: 0.01
        category:
          type: string
      required:
        - name
        - price
    Error:
      type: object
      properties:
        error:
          type: string
        details:
          type: object</code></pre>
                
                <h4>Testing with OpenAPI-based Tools</h4>
                <pre><code>import pytest
from openapi_spec_validator import validate_spec
import yaml
import json
import os
import requests

# Base URL for the API
BASE_URL = "http://localhost:5000"

# Load OpenAPI specification
@pytest.fixture
def openapi_spec():
    """Load the OpenAPI specification"""
    with open('openapi.yml', 'r') as f:
        return yaml.safe_load(f)

# Validate OpenAPI specification
def test_openapi_spec_is_valid(openapi_spec):
    """Test that our OpenAPI spec is valid"""
    validate_spec(openapi_spec)

# Test API endpoints against OpenAPI spec
class TestAPIContract:
    def test_get_products_matches_contract(self, openapi_spec):
        """Test that GET /api/products matches the OpenAPI spec"""
        # Get the expected schema from the spec
        expected_schema = openapi_spec['paths']['/api/products']['get']['responses']['200']['content']['application/json']['schema']
        
        # Call the API
        response = requests.get(f"{BASE_URL}/api/products")
        
        # Check status code
        assert response.status_code == 200
        
        # Check content type
        assert 'application/json' in response.headers['Content-Type']
        
        # Parse response JSON
        response_data = response.json()
        
        # Validate response structure against schema
        assert isinstance(response_data, list), "Response should be an array"
        
        if response_data:  # If there are products, check the first one
            product = response_data[0]
            
            # Check required fields
            required_fields = expected_schema['items']['required']
            for field in required_fields:
                assert field in product, f"Required field '{field}' missing from response"
            
            # Check field types
            properties = expected_schema['items']['properties']
            for field, field_schema in properties.items():
                if field in product:
                    if field_schema['type'] == 'integer':
                        assert isinstance(product[field], int), f"Field '{field}' should be an integer"
                    elif field_schema['type'] == 'number':
                        assert isinstance(product[field], (int, float)), f"Field '{field}' should be a number"
                    elif field_schema['type'] == 'string':
                        assert isinstance(product[field], str), f"Field '{field}' should be a string"
                    elif field_schema['type'] == 'boolean':
                        assert isinstance(product[field], bool), f"Field '{field}' should be a boolean"
    
    def test_create_product_matches_contract(self, openapi_spec):
        """Test that POST /api/products matches the OpenAPI spec"""
        # Get auth token (assuming authentication is required)
        auth_response = requests.post(
            f"{BASE_URL}/api/auth/login",
            json={
                "username": "testuser",
                "password": "Password123!"
            }
        )
        token = auth_response.json()["access_token"]
        
        # Create test product data from the input schema
        input_schema = openapi_spec['components']['schemas']['ProductInput']
        test_product = {
            "name": "Contract Test Product",
            "description": "Testing against OpenAPI contract",
            "price": 19.99,
            "category": "test"
        }
        
        # Validate test data against input schema
        for field in input_schema['required']:
            assert field in test_product, f"Required field '{field}' missing from test data"
        
        # Call the API
        response = requests.post(
            f"{BASE_URL}/api/products",
            json=test_product,
            headers={"Authorization": f"Bearer {token}"}
        )
        
        # Check status code
        assert response.status_code == 201, f"Expected 201 Created, got {response.status_code}"
        
        # Check content type
        assert 'application/json' in response.headers['Content-Type']
        
        # Parse response JSON
        product = response.json()
        
        # Get the expected schema from the spec
        expected_schema = openapi_spec['components']['schemas']['Product']
        
        # Check required fields
        for field in expected_schema['required']:
            assert field in product, f"Required field '{field}' missing from response"
        
        # Clean up - delete the created product
        product_id = product['id']
        requests.delete(
            f"{BASE_URL}/api/products/{product_id}",
            headers={"Authorization": f"Bearer {token}"}
        )</code></pre>
            </div>
            
            <div class="pact-testing">
                <h3>Consumer-Driven Contract Testing with Pact</h3>
                <p>Pact is a contract testing tool that enables consumer-driven contract testing, where service consumers define the expectations for the providers they interact with.</p>
                
                <h4>Example: Consumer (Frontend) Pact</h4>
                <pre><code>from pact import Consumer, Provider
import pytest
import requests
import os
import atexit

# Configure Pact
pact = Consumer('ProductConsumer').has_pact_with(Provider('ProductService'))
pact.start_service()

# Register to stop service when tests finish
atexit.register(pact.stop_service)

def test_get_products():
    """Test the contract for getting products"""
    # Define the expected interaction
    (pact
     .given('products exist')
     .upon_receiving('a request for all products')
     .with_request('get', '/api/products')
     .will_respond_with(200, body=[
         {
             'id': 1,
             'name': 'Test Product',
             'price': 29.99,
             'category': 'test'
         }
     ]))
    
    # Execute the test
    with pact:
        # Make the request to the mock server
        url = f"{pact.uri}/api/products"
        response = requests.get(url)
        
        # Check the response
        assert response.status_code == 200
        body = response.json()
        assert isinstance(body, list)
        assert len(body) > 0
        assert 'id' in body[0]
        assert 'name' in body[0]
        assert 'price' in body[0]

def test_get_product_by_id():
    """Test the contract for getting a product by ID"""
    # Define the expected interaction
    (pact
     .given('a product with ID 1 exists')
     .upon_receiving('a request for a product with ID 1')
     .with_request('get', '/api/products/1')
     .will_respond_with(200, body={
         'id': 1,
         'name': 'Test Product',
         'description': 'Test Description',
         'price': 29.99,
         'category': 'test'
     }))
    
    # Execute the test
    with pact:
        # Make the request to the mock server
        url = f"{pact.uri}/api/products/1"
        response = requests.get(url)
        
        # Check the response
        assert response.status_code == 200
        body = response.json()
        assert body['id'] == 1
        assert body['name'] == 'Test Product'
        assert body['price'] == 29.99

def test_create_product():
    """Test the contract for creating a product"""
    # Define the expected interaction
    (pact
     .given('authenticated user')
     .upon_receiving('a request to create a product')
     .with_request(
         'post',
         '/api/products',
         headers={'Authorization': 'Bearer valid-token', 'Content-Type': 'application/json'},
         body={
             'name': 'New Product',
             'description': 'Product Description',
             'price': 19.99,
             'category': 'test'
         }
     )
     .will_respond_with(
         201,
         body={
             'id': 123,
             'name': 'New Product',
             'description': 'Product Description',
             'price': 19.99,
             'category': 'test',
             'created_at': '2023-06-01T12:00:00Z'
         }
     ))
    
    # Execute the test
    with pact:
        # Make the request to the mock server
        url = f"{pact.uri}/api/products"
        response = requests.post(
            url,
            json={
                'name': 'New Product',
                'description': 'Product Description',
                'price': 19.99,
                'category': 'test'
            },
            headers={
                'Authorization': 'Bearer valid-token',
                'Content-Type': 'application/json'
            }
        )
        
        # Check the response
        assert response.status_code == 201
        body = response.json()
        assert body['name'] == 'New Product'
        assert body['price'] == 19.99</code></pre>
                
                <h4>Example: Provider (Backend) Verification</h4>
                <pre><code>from pact import Verifier
import os
import pytest
import subprocess
import time
import signal

# Setup Flask app process for testing
@pytest.fixture(scope='session')
def api_process():
    """Start the Flask API for testing"""
    # Set test environment
    env = os.environ.copy()
    env['FLASK_ENV'] = 'test'
    env['DATABASE_URL'] = 'sqlite:///test.db'
    
    # Start the API process
    process = subprocess.Popen(
        ['python', 'app.py'],
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # Wait for the API to start
    time.sleep(2)
    
    yield process
    
    # Terminate the process after tests
    process.terminate()
    process.wait()

def test_product_service_provider_contracts(api_process):
    """Verify that the API fulfills its contracts"""
    verifier = Verifier()
    
    # Path to the Pact file generated by the consumer
    pact_file = './pacts/productconsumer-productservice.json'
    
    # URL of the API
    provider_base_url = 'http://localhost:5000'
    
    # Verify provider against consumer expectations
    output, success = verifier.verify_pacts(
        pact_file,
        provider='ProductService',
        provider_base_url=provider_base_url,
        provider_states_setup_url=f"{provider_base_url}/test/setup",
        verbose=True
    )
    
    assert success, f"Pact verification failed: {output}"</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Contract Testing Best Practices</h3>
                <ul>
                    <li><strong>Define Contracts Early:</strong> Create API specifications before implementation.</li>
                    <li><strong>Consumer-Driven Contracts:</strong> Let consumers define what they need from providers.</li>
                    <li><strong>Keep Contracts Minimal:</strong> Include only what's necessary to avoid over-constraining the implementation.</li>
                    <li><strong>Version Contracts:</strong> Maintain backward compatibility or version contracts when they change.</li>
                    <li><strong>Include in CI/CD:</strong> Run contract tests as part of your continuous integration pipeline.</li>
                    <li><strong>Use Contract for Documentation:</strong> Generate API documentation from the contract specification.</li>
                    <li><strong>Test Both Ways:</strong> Verify that providers meet the contract and that consumers use it correctly.</li>
                </ul>
            </div>
        </section>

        <section class="security_testing">
            <h2>Security Testing</h2>
            
            <p>Security testing is essential for ensuring that your API is protected against common vulnerabilities and threats. It identifies weaknesses that could be exploited by attackers to gain unauthorized access, extract sensitive data, or disrupt service.</p>
            
            <p>Think of security testing as performing a vulnerability assessment on your house—checking all doors, windows, and other entry points to ensure they're properly secured against potential intruders.</p>
            
            <div class="owasp-top10">
                <h3>OWASP API Security Top 10</h3>
                <p>The Open Web Application Security Project (OWASP) identifies the top 10 API security risks:</p>
                
                <ol>
                    <li><strong>Broken Object Level Authorization:</strong> APIs don't properly check if the user has permission to access specific objects.</li>
                    <li><strong>Broken Authentication:</strong> Authentication mechanisms are implemented incorrectly.</li>
                    <li><strong>Excessive Data Exposure:</strong> APIs return more data than necessary.</li>
                    <li><strong>Lack of Resources & Rate Limiting:</strong> APIs don't limit the number of requests, leading to DoS attacks.</li>
                    <li><strong>Broken Function Level Authorization:</strong> APIs don't properly restrict user access to functions.</li>
                    <li><strong>Mass Assignment:</strong> Client-provided data is directly bound to data models without proper filtering.</li>
                    <li><strong>Security Misconfiguration:</strong> Insecure default configurations, open cloud storage, verbose error messages.</li>
                    <li><strong>Injection:</strong> Untrusted data is included in queries without proper sanitization.</li>
                    <li><strong>Improper Assets Management:</strong> Old API versions remain unpatched or expose sensitive data.</li>
                    <li><strong>Insufficient Logging & Monitoring:</strong> Lack of logging and monitoring prevents detection of attacks.</li>
                </ol>
            </div>
            
            <div class="code-section">
                <h3>Basic Security Testing with Python</h3>
                <pre><code>import pytest
import requests
import json
import re
import time

# Base URL for the API
BASE_URL = "http://localhost:5000/api"

# Test data
TEST_USER = {
    "username": "securitytest",
    "email": "security@example.com",
    "password": "SecureP@ssw0rd"
}

# Fixture for authentication token
@pytest.fixture
def auth_token():
    """Get authentication token for testing"""
    # Register a user if not exists (similar to previous examples)
    # ...
    
    # Login to get token
    login_response = requests.post(
        f"{BASE_URL}/auth/login",
        json={
            "username": TEST_USER["username"],
            "password": TEST_USER["password"]
        }
    )
    
    return login_response.json()["access_token"]

class TestAPISecurity:
    def test_sql_injection_prevention(self):
        """Test SQL injection prevention in login endpoint"""
        # Attempt SQL injection in login
        injection_payloads = [
            {"username": "' OR '1'='1", "password": "password"},
            {"username": "admin' --", "password": "anything"},
            {"username": "admin'; DROP TABLE users; --", "password": "anything"}
        ]
        
        for payload in injection_payloads:
            response = requests.post(f"{BASE_URL}/auth/login", json=payload)
            assert response.status_code == 401, f"SQL Injection may be possible with payload: {payload}"
    
    def test_brute_force_protection(self):
        """Test brute force protection in login endpoint"""
        # Attempt multiple failed logins
        for i in range(10):
            response = requests.post(
                f"{BASE_URL}/auth/login",
                json={
                    "username": TEST_USER["username"],
                    "password": f"wrong_password_{i}"
                }
            )
        
        # Check if account is locked or rate limited
        response = requests.post(
            f"{BASE_URL}/auth/login",
            json={
                "username": TEST_USER["username"],
                "password": TEST_USER["password"]
            }
        )
        
        # API should either rate limit (429) or temporarily lock account (403)
        assert response.status_code in (403, 429), "API may be vulnerable to brute force attacks"
    
    def test_jwt_without_verification(self, auth_token):
        """Test if API accepts tampered JWTs"""
        # Create a tampered token by changing the payload without re-signing
        token_parts = auth_token.split('.')
        if len(token_parts) != 3:
            pytest.skip("Token is not a standard JWT")
        
        # Decode the payload
        import base64
        payload = json.loads(base64.b64decode(token_parts[1] + '==').decode('utf-8'))
        
        # Modify the payload (e.g., change user role to admin)
        payload['role'] = 'admin'
        
        # Re-encode the payload
        modified_payload = base64.b64encode(json.dumps(payload).encode('utf-8')).decode('utf-8').rstrip('=')
        
        # Create tampered token
        tampered_token = f"{token_parts[0]}.{modified_payload}.{token_parts[2]}"
        
        # Try to access protected endpoint with tampered token
        response = requests.get(
            f"{BASE_URL}/users/profile",
            headers={"Authorization": f"Bearer {tampered_token}"}
        )
        
        # Should be rejected with 401 Unauthorized
        assert response.status_code == 401, "API may accept tampered JWTs"
    
    def test_sensitive_data_exposure(self):
        """Test for sensitive data exposure in error messages"""
        # Try to trigger errors that might leak sensitive information
        test_cases = [
            # Invalid JSON
            lambda: requests.post(f"{BASE_URL}/auth/login", data="invalid json"),
            # Invalid route
            lambda: requests.get(f"{BASE_URL}/nonexistent_route"),
            # Invalid HTTP method
            lambda: requests.delete(f"{BASE_URL}/auth/login")
        ]
        
        sensitive_patterns = [
            r'(Exception:)',
            r'(stack trace)',
            r'(at [\w\.]+\(\))',  # Stack trace function calls
            r'(\/[\w\/\.]+\.py)',  # File paths
            r'(database|sql|query)',  # Database info
            r'(password|secret|key)',  # Sensitive fields
            r'(config)',  # Configuration info
        ]
        
        for test_case in test_cases:
            response = test_case()
            
            # Check status code (should be 4xx, not 5xx for invalid input)
            assert response.status_code < 500, "Server error on invalid input"
            
            # Check for sensitive information in response
            for pattern in sensitive_patterns:
                match = re.search(pattern, response.text, re.IGNORECASE)
                assert not match, f"Possible sensitive data leak: {match.group(0)}"
    
    def test_content_security_headers(self):
        """Test for security-related HTTP headers"""
        response = requests.get(f"{BASE_URL}/users")
        
        # Security headers that should be present
        security_headers = {
            'X-Content-Type-Options': 'nosniff',
            'X-Frame-Options': ['DENY', 'SAMEORIGIN'],
            'Content-Security-Policy': None,  # Just check existence
            'Strict-Transport-Security': None,
            'X-XSS-Protection': '1; mode=block'
        }
        
        for header, expected_value in security_headers.items():
            assert header in response.headers, f"Missing security header: {header}"
            
            if expected_value:
                if isinstance(expected_value, list):
                    assert response.headers[header] in expected_value, f"Invalid value for {header}"
                else:
                    assert response.headers[header] == expected_value, f"Invalid value for {header}"
    
    def test_rate_limiting(self):
        """Test if API implements rate limiting"""
        # Make multiple requests in quick succession
        start_time = time.time()
        responses = []
        
        for i in range(50):
            response = requests.get(f"{BASE_URL}/products")
            responses.append(response)
            
            # If we get rate limited, the test passes
            if response.status_code == 429:
                break
        
        # Check if we were rate limited
        rate_limited = any(r.status_code == 429 for r in responses)
        
        # Alternative check: Look for rate limit headers
        rate_limit_headers = any(
            'X-RateLimit-Limit' in r.headers or
            'RateLimit-Limit' in r.headers or
            'Retry-After' in r.headers
            for r in responses
        )
        
        assert rate_limited or rate_limit_headers, "API may not implement rate limiting"
    
    def test_cors_misconfiguration(self):
        """Test for CORS misconfiguration"""
        # Send OPTIONS request with suspicious origin
        headers = {
            'Origin': 'https://malicious-site.com',
            'Access-Control-Request-Method': 'POST',
            'Access-Control-Request-Headers': 'Content-Type, Authorization'
        }
        
        response = requests.options(f"{BASE_URL}/users", headers=headers)
        
        # Check if API allows this origin
        if 'Access-Control-Allow-Origin' in response.headers:
            allowed_origin = response.headers['Access-Control-Allow-Origin']
            assert allowed_origin != '*', "CORS allows all origins"
            assert allowed_origin != 'https://malicious-site.com', "CORS allows suspicious origin"</code></pre>

                <h3>Using OWASP ZAP for Automated Security Testing</h3>
                <pre><code>from zapv2 import ZAPv2
import time
import json

# API key from ZAP
apiKey = 'your-api-key'

# Target API
target = 'http://localhost:5000/api'

# Initialize ZAP
zap = ZAPv2(apikey=apiKey, proxies={'http': 'http://localhost:8080', 'https': 'http://localhost:8080'})

def test_api_with_zap():
    """Run a ZAP scan against the API"""
    # Set up a new session
    print('Creating new ZAP session...')
    zap.core.new_session(name='API Security Test', overwrite=True)
    
    # Define the context
    contextId = zap.context.new_context(contextname='API Context')
    zap.context.include_in_context('API Context', f'^{target}.*<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Testing APIs: A Comprehensive Guide</title>
    <link rel="stylesheet" href="/styles/main.css">
    <link rel="icon" href="/favicon.png">
</head>
<body>
    <header>
        <h1>Testing APIs: A Comprehensive Guide</h1>
        <p class="subtitle">Python Full Stack Web Course</p>
    </header>

    <main>
        <section class="introduction">
            <h2>Introduction to API Testing</h2>
            
            <p>Welcome to our comprehensive guide on API testing for Python Full Stack development! In today's interconnected world, APIs (Application Programming Interfaces) have become the critical nervous system of modern software, enabling applications to communicate and share data. However, like any crucial component, APIs must be rigorously tested to ensure they function correctly, securely, and efficiently.</p>
            
            <p>Think of API testing as the quality control department in a manufacturing plant. Just as QC inspectors examine products for defects before they reach customers, API testing identifies bugs, vulnerabilities, and performance issues before your API reaches production. Without this critical process, you risk exposing your users to frustrating experiences and your systems to potential security breaches.</p>
            
            <p>Throughout this tutorial, we'll explore the various aspects of API testing, from simple unit tests to complex integration and performance testing. We'll cover both theoretical concepts and practical implementations using Python's rich ecosystem of testing tools and frameworks. By the end, you'll have a comprehensive understanding of how to implement a robust testing strategy for your API projects.</p>
            
            <div class="key-concepts">
                <h3>Key Testing Concepts</h3>
                <ul>
                    <li><strong>Unit Testing:</strong> Testing individual components in isolation</li>
                    <li><strong>Integration Testing:</strong> Testing interactions between components</li>
                    <li><strong>Functional Testing:</strong> Testing complete API workflows</li>
                    <li><strong>Load Testing:</strong> Testing performance under expected load</li>
                    <li><strong>Security Testing:</strong> Testing for vulnerabilities and security issues</li>
                    <li><strong>Contract Testing:</strong> Ensuring the API adheres to its specification</li>
                    <li><strong>Test-Driven Development (TDD):</strong> Writing tests before implementation</li>
                </ul>
            </div>
        </section>

        <section class="testing_pyramid">
            <h2>The API Testing Pyramid</h2>
            
            <p>The Testing Pyramid is a conceptual framework that helps us understand the different levels of testing and their relative importance. Think of it as a dietary pyramid—it guides you on the proportion of different test types you should include in your testing strategy.</p>
            
            <div class="pyramid-illustration">
                <div class="pyramid-level" style="width: 80%; background-color: #e0f7fa;">
                    <strong>Unit Tests</strong>
                    <p>Fast, focused, numerous</p>
                </div>
                <div class="pyramid-level" style="width: 65%; background-color: #b2ebf2;">
                    <strong>Integration Tests</strong>
                    <p>Component interactions</p>
                </div>
                <div class="pyramid-level" style="width: 50%; background-color: #80deea;">
                    <strong>API Tests</strong>
                    <p>Testing API endpoints</p>
                </div>
                <div class="pyramid-level" style="width: 35%; background-color: #4dd0e1;">
                    <strong>End-to-End Tests</strong>
                    <p>Complete workflows</p>
                </div>
                <div class="pyramid-level" style="width: 20%; background-color: #26c6da;">
                    <strong>Manual/Exploratory Tests</strong>
                    <p>Human testing</p>
                </div>
            </div>
            
            <div class="pyramid-explanation">
                <h3>Why the Pyramid Shape?</h3>
                <p>The pyramid shape represents both quantity and speed:</p>
                <ul>
                    <li><strong>Unit Tests (Base):</strong> 
                        <ul>
                            <li>Most numerous (70-80% of your tests)</li>
                            <li>Fastest to run (milliseconds)</li>
                            <li>Cheapest to maintain</li>
                            <li>Highest level of isolation</li>
                        </ul>
                    </li>
                    <li><strong>Integration Tests (Middle):</strong>
                        <ul>
                            <li>Moderate number (15-20% of your tests)</li>
                            <li>Medium speed (seconds)</li>
                            <li>Test component interactions</li>
                        </ul>
                    </li>
                    <li><strong>API/Functional Tests (Middle-Upper):</strong>
                        <ul>
                            <li>Focus on API contract and behavior</li>
                            <li>Test complete endpoints</li>
                            <li>Ensure proper request/response handling</li>
                        </ul>
                    </li>
                    <li><strong>End-to-End Tests (Upper):</strong>
                        <ul>
                            <li>Fewer in number (5-10% of your tests)</li>
                            <li>Slower to run (seconds to minutes)</li>
                            <li>Test complete workflows</li>
                            <li>More brittle and costly to maintain</li>
                        </ul>
                    </li>
                    <li><strong>Manual Tests (Tip):</strong>
                        <ul>
                            <li>Least numerous</li>
                            <li>Slowest (minutes to hours)</li>
                            <li>Require human intervention</li>
                            <li>Cannot be easily automated</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <div class="pyramid-benefits">
                <h3>Benefits of Following the Pyramid</h3>
                <ul>
                    <li><strong>Faster Feedback:</strong> Issues are caught early in the development cycle</li>
                    <li><strong>Increased Efficiency:</strong> More tests at the levels that are cheaper to run and maintain</li>
                    <li><strong>Better Coverage:</strong> A balanced approach ensures comprehensive testing</li>
                    <li><strong>Improved Diagnosis:</strong> When a high-level test fails, lower-level tests help pinpoint the issue</li>
                    <li><strong>Sustainable Testing:</strong> The strategy remains feasible as your codebase grows</li>
                </ul>
            </div>
            
            <div class="anti-patterns">
                <h3>Testing Anti-Patterns to Avoid</h3>
                <ul>
                    <li><strong>Ice Cream Cone:</strong> More end-to-end tests than unit tests (inverted pyramid)</li>
                    <li><strong>Hourglass:</strong> Many unit and UI tests, but few integration tests</li>
                    <li><strong>Cupcake:</strong> Equal distribution of test types, ignoring the cost/benefit ratio</li>
                </ul>
            </div>
        </section>

        <section class="unit_testing">
            <h2>Unit Testing API Components</h2>
            
            <p>Unit testing forms the foundation of your API testing strategy. It involves testing individual components in isolation to ensure they function correctly. For APIs, units typically include controllers, services, models, utilities, and other discrete components that make up your API.</p>
            
            <p>Think of unit testing like checking individual Lego blocks before assembling them into a larger structure. You want to make sure each piece is correctly shaped and functional on its own before connecting it with others.</p>
            
            <div class="frameworks">
                <h3>Python Unit Testing Frameworks</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Framework</th>
                            <th>Description</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>pytest</td>
                            <td>Modern, flexible testing framework with powerful fixtures and plugins</td>
                            <td>Most Python projects; especially those needing advanced features</td>
                        </tr>
                        <tr>
                            <td>unittest</td>
                            <td>Python's built-in testing framework (xUnit style)</td>
                            <td>Simple projects; maintaining backward compatibility</td>
                        </tr>
                        <tr>
                            <td>nose2</td>
                            <td>Extended unittest with plugin support</td>
                            <td>Projects transitioning from unittest to pytest</td>
                        </tr>
                        <tr>
                            <td>doctest</td>
                            <td>Extract tests from docstrings</td>
                            <td>Simple projects; documentation-driven testing</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="code-section">
                <h3>Unit Testing Flask API Components with pytest</h3>
                <pre><code>import pytest
from app.services.user_service import UserService
from app.models.user import User
from app.exceptions import ValidationError
from unittest.mock import Mock, patch

# Test a service component
class TestUserService:
    def setup_method(self):
        """Setup before each test"""
        self.user_repository_mock = Mock()
        self.user_service = UserService(self.user_repository_mock)
    
    def test_get_user_by_id_returns_user_when_found(self):
        # Arrange
        user_id = 123
        expected_user = User(id=user_id, username="testuser", email="test@example.com")
        self.user_repository_mock.get_by_id.return_value = expected_user
        
        # Act
        result = self.user_service.get_user_by_id(user_id)
        
        # Assert
        assert result is not None
        assert result.id == user_id
        assert result.username == "testuser"
        self.user_repository_mock.get_by_id.assert_called_once_with(user_id)
    
    def test_get_user_by_id_returns_none_when_not_found(self):
        # Arrange
        user_id = 456
        self.user_repository_mock.get_by_id.return_value = None
        
        # Act
        result = self.user_service.get_user_by_id(user_id)
        
        # Assert
        assert result is None
        self.user_repository_mock.get_by_id.assert_called_once_with(user_id)
    
    def test_create_user_validates_email_format(self):
        # Arrange
        invalid_email = "not-an-email"
        
        # Act & Assert
        with pytest.raises(ValidationError) as excinfo:
            self.user_service.create_user("testuser", invalid_email, "password123")
        
        assert "Invalid email format" in str(excinfo.value)
        # Verify repository was never called
        self.user_repository_mock.save.assert_not_called()
    
    @patch('app.services.user_service.hash_password')
    def test_create_user_hashes_password(self, hash_password_mock):
        # Arrange
        username = "testuser"
        email = "test@example.com"
        password = "password123"
        hashed_password = "hashed_password_value"
        hash_password_mock.return_value = hashed_password
        
        # Act
        self.user_service.create_user(username, email, password)
        
        # Assert
        hash_password_mock.assert_called_once_with(password)
        # Verify user was saved with hashed password
        self.user_repository_mock.save.assert_called_once()
        saved_user = self.user_repository_mock.save.call_args[0][0]
        assert saved_user.username == username
        assert saved_user.email == email
        assert saved_user.password == hashed_password

# Test a utility function
def test_format_api_response():
    from app.utils.response_formatter import format_api_response
    
    # Test with success status
    result = format_api_response(data={"name": "Test"}, status="success")
    assert result["status"] == "success"
    assert result["data"]["name"] == "Test"
    assert "error" not in result
    
    # Test with error status
    result = format_api_response(error="Not found", status="error")
    assert result["status"] == "error"
    assert result["error"] == "Not found"
    assert "data" not in result

# Test a model/schema
def test_user_schema_validation():
    from app.schemas.user_schema import UserSchema
    
    # Test valid data
    valid_data = {"username": "testuser", "email": "test@example.com", "age": 25}
    schema = UserSchema()
    result = schema.load(valid_data)
    assert result["username"] == "testuser"
    assert result["email"] == "test@example.com"
    assert result["age"] == 25
    
    # Test invalid data (missing required field)
    invalid_data = {"username": "testuser"}
    with pytest.raises(ValidationError):
        schema.load(invalid_data)</code></pre>

                <h3>Unit Testing Django API Components</h3>
                <pre><code>from django.test import TestCase
from django.urls import reverse
from rest_framework import status
from unittest.mock import patch
from .models import Product
from .serializers import ProductSerializer
from .views import ProductViewSet

class ProductModelTests(TestCase):
    def test_product_creation(self):
        """Test product model creation and string representation"""
        product = Product.objects.create(
            name="Test Product",
            description="Test Description",
            price=99.99,
            stock=10
        )
        self.assertEqual(product.name, "Test Product")
        self.assertEqual(product.price, 99.99)
        self.assertEqual(str(product), "Test Product")
    
    def test_product_price_validation(self):
        """Test that products cannot have negative prices"""
        with self.assertRaises(Exception):
            Product.objects.create(
                name="Invalid Product",
                description="Product with negative price",
                price=-10.00,
                stock=5
            )

class ProductSerializerTests(TestCase):
    def test_valid_serializer(self):
        """Test serializer with valid data"""
        data = {
            'name': 'New Product',
            'description': 'Product Description',
            'price': 19.99,
            'stock': 5
        }
        serializer = ProductSerializer(data=data)
        self.assertTrue(serializer.is_valid())
    
    def test_invalid_serializer(self):
        """Test serializer with invalid data"""
        # Missing required fields
        data = {'name': 'Incomplete Product'}
        serializer = ProductSerializer(data=data)
        self.assertFalse(serializer.is_valid())
        self.assertIn('price', serializer.errors)
    
    def test_serializer_output(self):
        """Test serializer output format"""
        product = Product.objects.create(
            name="Test Product",
            description="Test Description",
            price=99.99,
            stock=10
        )
        serializer = ProductSerializer(product)
        data = serializer.data
        
        self.assertEqual(data['name'], "Test Product")
        self.assertEqual(float(data['price']), 99.99)
        # Check if created_at field is included
        self.assertIn('created_at', data)

class ProductViewSetTests(TestCase):
    @patch('api.services.product_service.get_all_products')
    def test_list_products(self, mock_get_all_products):
        """Test the list method of ProductViewSet"""
        # Arrange
        mock_products = [
            Product(id=1, name="Product 1", price=10.99, stock=5),
            Product(id=2, name="Product 2", price=20.99, stock=10)
        ]
        mock_get_all_products.return_value = mock_products
        
        view = ProductViewSet()
        view.request = None  # Simplified for unit testing
        
        # Act
        response = view.list(view.request)
        
        # Assert
        self.assertEqual(len(response.data), 2)
        mock_get_all_products.assert_called_once()
    
    @patch('api.services.product_service.get_product_by_id')
    def test_retrieve_product_not_found(self, mock_get_product):
        """Test retrieving a non-existent product"""
        # Arrange
        mock_get_product.return_value = None
        
        view = ProductViewSet()
        view.request = None  # Simplified for unit testing
        
        # Act & Assert
        with self.assertRaises(Exception):
            view.retrieve(view.request, pk=999)
        
        mock_get_product.assert_called_once_with(999)</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Unit Testing Best Practices</h3>
                <ul>
                    <li><strong>Test One Thing Per Test:</strong> Each test should verify a single behavior or edge case.</li>
                    <li><strong>Use Descriptive Test Names:</strong> Name tests to clearly indicate what they're testing and expected behavior.</li>
                    <li><strong>Follow AAA Pattern:</strong> Structure tests with Arrange, Act, Assert sections.</li>
                    <li><strong>Mock External Dependencies:</strong> Use mocking to isolate the unit being tested.</li>
                    <li><strong>Test Edge Cases:</strong> Include tests for boundary conditions and error scenarios.</li>
                    <li><strong>Keep Tests Independent:</strong> Tests should not depend on each other or shared state.</li>
                    <li><strong>Use Fixtures and Factories:</strong> For consistent test setup and data generation.</li>
                    <li><strong>Aim for High Coverage:</strong> Unit tests should cover most of your code.</li>
                    <li><strong>Keep Tests Fast:</strong> Unit tests should run in milliseconds.</li>
                </ul>
            </div>
            
            <div class="mocking">
                <h3>Effective Mocking Strategies</h3>
                <p>Mocking is essential for isolating the component you're testing. Here are some common scenarios:</p>
                
                <h4>What to Mock:</h4>
                <ul>
                    <li><strong>Database Interactions:</strong> Repository or ORM calls</li>
                    <li><strong>External API Calls:</strong> HTTP requests to third-party services</li>
                    <li><strong>File System Operations:</strong> Reading or writing files</li>
                    <li><strong>Time-dependent Functions:</strong> Functions that rely on current time</li>
                    <li><strong>Random Behaviors:</strong> Functions that generate random values</li>
                </ul>
                
                <h4>Mocking with pytest:</h4>
                <pre><code>import pytest
from unittest.mock import Mock, patch
import requests
from app.services.weather_service import WeatherService

# Mock a function
@patch('app.services.weather_service.get_current_time')
def test_get_forecast_includes_timestamp(mock_get_time):
    # Arrange
    mock_get_time.return_value = "2023-06-15T12:00:00Z"
    weather_service = WeatherService()
    
    # Act
    forecast = weather_service.get_forecast("London")
    
    # Assert
    assert forecast["timestamp"] == "2023-06-15T12:00:00Z"

# Mock an external API call
@patch('requests.get')
def test_get_forecast_calls_weather_api(mock_requests_get):
    # Arrange
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        "temp": 22.5,
        "humidity": 65,
        "condition": "Sunny"
    }
    mock_requests_get.return_value = mock_response
    
    weather_service = WeatherService()
    
    # Act
    forecast = weather_service.get_forecast("London")
    
    # Assert
    mock_requests_get.assert_called_once()
    assert "api.weather.com" in mock_requests_get.call_args[0][0]
    assert forecast["temp"] == 22.5

# Mock a class
@patch('app.repositories.user_repository.UserRepository')
def test_authenticate_user_success(mock_user_repo_class):
    # Arrange
    mock_user_repo = Mock()
    mock_user_repo_class.return_value = mock_user_repo
    
    # Set up the mock to return a user with matching password
    mock_user_repo.get_by_username.return_value = {
        "username": "testuser",
        "password_hash": "hashed_password",
        "is_active": True
    }
    
    # Mock the password verification
    with patch('app.services.auth_service.verify_password', return_value=True):
        from app.services.auth_service import AuthService
        auth_service = AuthService()
        
        # Act
        result = auth_service.authenticate("testuser", "password123")
        
        # Assert
        assert result["authenticated"] is True
        assert result["username"] == "testuser"</code></pre>
            </div>
        </section>

        <section class="integration_testing">
            <h2>Integration Testing</h2>
            
            <p>While unit tests examine individual components in isolation, integration tests verify that these components work correctly together. In the context of API testing, integration tests typically involve testing how controllers interact with services, how services interact with repositories, and how the API interacts with databases or external services.</p>
            
            <p>Think of integration testing like testing how different musical instruments sound together in an orchestra, rather than testing each instrument separately. It ensures that all the parts harmonize correctly when combined.</p>
            
            <div class="code-section">
                <h3>Integration Testing Flask APIs</h3>
                <pre><code>import pytest
import json
from app import create_app
from app.database import db as _db
from app.models.user import User

# Fixture for Flask test app
@pytest.fixture
def app():
    """Create and configure a Flask app for testing"""
    app = create_app('testing')
    app.config['TESTING'] = True
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///:memory:'
    
    # Create the database and tables
    with app.app_context():
        _db.create_all()
    
    yield app
    
    # Clean up
    with app.app_context():
        _db.drop_all()

# Fixture for database
@pytest.fixture
def db(app):
    """Database fixture"""
    with app.app_context():
        yield _db

# Fixture for test client
@pytest.fixture
def client(app):
    """Client for testing API endpoints"""
    return app.test_client()

# Fixture for test user
@pytest.fixture
def test_user(db):
    """Create a test user in the database"""
    with db.app.app_context():
        user = User(
            username='testuser',
            email='test@example.com'
        )
        user.set_password('password123')
        db.session.add(user)
        db.session.commit()
        return user

# Integration test for user creation and authentication flow
def test_register_and_login_integration(client, db):
    """Test the complete register & login flow"""
    # Register a new user
    register_response = client.post(
        '/api/users/register',
        data=json.dumps({
            'username': 'newuser',
            'email': 'new@example.com',
            'password': 'securepass123'
        }),
        content_type='application/json'
    )
    
    assert register_response.status_code == 201
    register_data = json.loads(register_response.data)
    assert register_data['username'] == 'newuser'
    
    # Verify user exists in database
    with db.app.app_context():
        user = User.query.filter_by(username='newuser').first()
        assert user is not None
        assert user.email == 'new@example.com'
        assert user.check_password('securepass123')
    
    # Login with created user
    login_response = client.post(
        '/api/auth/login',
        data=json.dumps({
            'username': 'newuser',
            'password': 'securepass123'
        }),
        content_type='application/json'
    )
    
    assert login_response.status_code == 200
    login_data = json.loads(login_response.data)
    assert 'access_token' in login_data
    assert 'refresh_token' in login_data
    
    # Use token to access protected endpoint
    profile_response = client.get(
        '/api/users/profile',
        headers={
            'Authorization': f"Bearer {login_data['access_token']}"
        }
    )
    
    assert profile_response.status_code == 200
    profile_data = json.loads(profile_response.data)
    assert profile_data['username'] == 'newuser'
    assert profile_data['email'] == 'new@example.com'

# Integration test for database operations
def test_product_crud_operations(client, db, test_user):
    """Test Create, Read, Update, Delete operations for products"""
    # Login first to get token
    login_response = client.post(
        '/api/auth/login',
        data=json.dumps({
            'username': 'testuser',
            'password': 'password123'
        }),
        content_type='application/json'
    )
    
    login_data = json.loads(login_response.data)
    token = login_data['access_token']
    headers = {'Authorization': f"Bearer {token}"}
    
    # Create a product
    create_response = client.post(
        '/api/products',
        headers=headers,
        data=json.dumps({
            'name': 'Test Product',
            'description': 'Test Description',
            'price': 29.99,
            'category': 'electronics'
        }),
        content_type='application/json'
    )
    
    assert create_response.status_code == 201
    product_data = json.loads(create_response.data)
    product_id = product_data['id']
    
    # Read the product
    get_response = client.get(f'/api/products/{product_id}')
    assert get_response.status_code == 200
    get_data = json.loads(get_response.data)
    assert get_data['name'] == 'Test Product'
    assert float(get_data['price']) == 29.99
    
    # Update the product
    update_response = client.put(
        f'/api/products/{product_id}',
        headers=headers,
        data=json.dumps({
            'name': 'Updated Product',
            'description': 'Updated Description',
            'price': 39.99,
            'category': 'electronics'
        }),
        content_type='application/json'
    )
    
    assert update_response.status_code == 200
    update_data = json.loads(update_response.data)
    assert update_data['name'] == 'Updated Product'
    assert float(update_data['price']) == 39.99
    
    # Verify in database
    with db.app.app_context():
        from app.models.product import Product
        product = Product.query.get(product_id)
        assert product.name == 'Updated Product'
        assert product.price == 39.99
    
    # Delete the product
    delete_response = client.delete(
        f'/api/products/{product_id}',
        headers=headers
    )
    
    assert delete_response.status_code == 204
    
    # Verify product is deleted
    get_after_delete = client.get(f'/api/products/{product_id}')
    assert get_after_delete.status_code == 404</code></pre>

                <h3>Integration Testing Django APIs</h3>
                <pre><code>from django.test import TestCase
from django.urls import reverse
from rest_framework.test import APIClient
from rest_framework import status
from .models import Product, Category, Order, OrderItem
from django.contrib.auth.models import User
import json

class ProductOrderIntegrationTest(TestCase):
    def setUp(self):
        # Create a test user
        self.user = User.objects.create_user(
            username='testuser',
            email='test@example.com',
            password='testpassword'
        )
        
        # Create API client
        self.client = APIClient()
        
        # Create categories
        self.category = Category.objects.create(
            name='Electronics',
            description='Electronic devices'
        )
        
        # Create products
        self.product1 = Product.objects.create(
            name='Laptop',
            description='Powerful laptop',
            price=999.99,
            stock=10,
            category=self.category
        )
        
        self.product2 = Product.objects.create(
            name='Phone',
            description='Smartphone',
            price=499.99,
            stock=20,
            category=self.category
        )
    
    def test_complete_order_flow(self):
        """Test the complete order flow from authentication to checkout"""
        # Step 1: Log in and get token
        login_url = reverse('token_obtain_pair')
        login_data = {
            'username': 'testuser',
            'password': 'testpassword'
        }
        login_response = self.client.post(login_url, login_data, format='json')
        self.assertEqual(login_response.status_code, status.HTTP_200_OK)
        
        # Extract token
        token = login_response.data['access']
        self.client.credentials(HTTP_AUTHORIZATION=f'Bearer {token}')
        
        # Step 2: Get product list
        products_url = reverse('product-list')
        products_response = self.client.get(products_url)
        self.assertEqual(products_response.status_code, status.HTTP_200_OK)
        self.assertEqual(len(products_response.data), 2)
        
        # Step 3: Create a new order (cart)
        orders_url = reverse('order-list')
        order_data = {'status': 'cart'}
        order_response = self.client.post(orders_url, order_data, format='json')
        self.assertEqual(order_response.status_code, status.HTTP_201_CREATED)
        order_id = order_response.data['id']
        
        # Step 4: Add items to the order
        items_url = reverse('orderitem-list')
        
        # Add first product
        item1_data = {
            'order': order_id,
            'product': self.product1.id,
            'quantity': 2
        }
        item1_response = self.client.post(items_url, item1_data, format='json')
        self.assertEqual(item1_response.status_code, status.HTTP_201_CREATED)
        
        # Add second product
        item2_data = {
            'order': order_id,
            'product': self.product2.id,
            'quantity': 1
        }
        item2_response = self.client.post(items_url, item2_data, format='json')
        self.assertEqual(item2_response.status_code, status.HTTP_201_CREATED)
        
        # Step 5: Get order details
        order_detail_url = reverse('order-detail', args=[order_id])
        order_detail_response = self.client.get(order_detail_url)
        self.assertEqual(order_detail_response.status_code, status.HTTP_200_OK)
        
        # Check order items
        self.assertEqual(len(order_detail_response.data['items']), 2)
        
        # Check order total
        expected_total = (self.product1.price * 2) + (self.product2.price * 1)
        self.assertEqual(float(order_detail_response.data['total']), expected_total)
        
        # Step 6: Update order to checkout
        checkout_data = {
            'status': 'processing',
            'shipping_address': '123 Test St, Test City, Test Country',
            'payment_method': 'credit_card'
        }
        checkout_response = self.client.patch(
            order_detail_url, 
            checkout_data,
            format='json'
        )
        self.assertEqual(checkout_response.status_code, status.HTTP_200_OK)
        self.assertEqual(checkout_response.data['status'], 'processing')
        
        # Step 7: Verify stock was reduced
        product1_url = reverse('product-detail', args=[self.product1.id])
        product1_response = self.client.get(product1_url)
        self.assertEqual(product1_response.status_code, status.HTTP_200_OK)
        self.assertEqual(product1_response.data['stock'], 8)  # 10 - 2
        
        product2_url = reverse('product-detail', args=[self.product2.id])
        product2_response = self.client.get(product2_url)
        self.assertEqual(product2_response.status_code, status.HTTP_200_OK)
        self.assertEqual(product2_response.data['stock'], 19)  # 20 - 1
        
        # Step 8: Verify in database
        updated_order = Order.objects.get(id=order_id)
        self.assertEqual(updated_order.status, 'processing')
        self.assertEqual(updated_order.shipping_address, '123 Test St, Test City, Test Country')
        
        # Product stock should be updated in database
        updated_product1 = Product.objects.get(id=self.product1.id)
        self.assertEqual(updated_product1.stock, 8)
        
        updated_product2 = Product.objects.get(id=self.product2.id)
        self.assertEqual(updated_product2.stock, 19)</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Integration Testing Best Practices</h3>
                <ul>
                    <li><strong>Focus on Component Interactions:</strong> Test how components work together, not individual behaviors.</li>
                    <li><strong>Use Test Databases:</strong> Use in-memory or dedicated test databases, not production databases.</li>
                    <li><strong>Test Complete Workflows:</strong> Test entire flows like registration-to-login or order-to-payment.</li>
                    <li><strong>Test Boundary Behaviors:</strong> Test edge cases where components meet.</li>
                    <li><strong>Isolate External Services:</strong> Mock external services that aren't part of the integration you're testing.</li>
                    <li><strong>Use Fixtures and Factories:</strong> For consistent test data across integration tests.</li>
                    <li><strong>Test Database Constraints:</strong> Verify that database constraints work as expected.</li>
                    <li><strong>Clean Up After Tests:</strong> Ensure each test starts with a clean state.</li>
                </ul>
            </div>
            
            <div class="test-doubles">
                <h3>Using Test Doubles</h3>
                <p>In integration testing, you often need to replace some components with test doubles:</p>
                
                <table>
                    <thead>
                        <tr>
                            <th>Type</th>
                            <th>Description</th>
                            <th>When to Use</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Dummy</td>
                            <td>Objects passed but never used</td>
                            <td>When parameter is required but not used</td>
                        </tr>
                        <tr>
                            <td>Stub</td>
                            <td>Provides canned responses</td>
                            <td>When you need specific values returned</td>
                        </tr>
                        <tr>
                            <td>Spy</td>
                            <td>Records calls made to it</td>
                            <td>When you need to verify calls were made</td>
                        </tr>
                        <tr>
                            <td>Mock</td>
                            <td>Pre-programmed with expectations</td>
                            <td>When you need to verify behavior</td>
                        </tr>
                        <tr>
                            <td>Fake</td>
                            <td>Working implementation, but not production-ready</td>
                            <td>When you need behavior but not the real component</td>
                        </tr>
                    </tbody>
                </table>
                
                <pre><code>import pytest
from unittest.mock import Mock, patch
import requests

# Example of different test doubles
class TestPaymentService:
    def test_process_payment_with_stub(self):
        """Using a stub for payment gateway"""
        # Create a stub that returns a successful response
        payment_gateway_stub = Mock()
        payment_gateway_stub.process_payment.return_value = {
            "status": "success",
            "transaction_id": "tx_12345",
            "amount": 100.00
        }
        
        # Use the stub in the payment service
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=payment_gateway_stub)
        
        # Process payment
        result = payment_service.process_payment("customer_1", 100.00, "visa")
        
        # Assert result
        assert result["success"] is True
        assert result["transaction_id"] == "tx_12345"
    
    def test_process_payment_with_spy(self):
        """Using a spy to verify calls"""
        # Create a spy
        payment_gateway_spy = Mock()
        payment_gateway_spy.process_payment.return_value = {
            "status": "success",
            "transaction_id": "tx_67890"
        }
        
        # Use the spy
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=payment_gateway_spy)
        
        # Process payment
        payment_service.process_payment("customer_1", 200.00, "mastercard")
        
        # Verify calls to the spy
        payment_gateway_spy.process_payment.assert_called_once()
        args, kwargs = payment_gateway_spy.process_payment.call_args
        assert kwargs["customer_id"] == "customer_1"
        assert kwargs["amount"] == 200.00
        assert kwargs["card_type"] == "mastercard"
    
    def test_process_payment_with_mock(self):
        """Using a mock with expectations"""
        # Create a mock with expectations
        payment_gateway_mock = Mock()
        payment_gateway_mock.process_payment.return_value = {
            "status": "success",
            "transaction_id": "tx_24680"
        }
        
        # Configure mock to raise exception for specific input
        def side_effect(customer_id, amount, card_type, **kwargs):
            if amount > 1000:
                raise ValueError("Amount exceeds maximum")
            return {"status": "success", "transaction_id": "tx_24680"}
        
        payment_gateway_mock.process_payment.side_effect = side_effect
        
        # Use the mock
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=payment_gateway_mock)
        
        # This should succeed
        result = payment_service.process_payment("customer_1", 500.00, "visa")
        assert result["success"] is True
        
        # This should fail
        with pytest.raises(ValueError) as excinfo:
            payment_service.process_payment("customer_1", 1500.00, "visa")
        assert "Amount exceeds maximum" in str(excinfo.value)
    
    def test_process_payment_with_fake(self):
        """Using a fake payment gateway"""
        # Create a fake payment gateway
        class FakePaymentGateway:
            def __init__(self):
                self.payments = []
            
            def process_payment(self, customer_id, amount, card_type, **kwargs):
                # Simulate some business logic
                if amount <= 0:
                    raise ValueError("Amount must be positive")
                
                # Record the payment
                payment = {
                    "customer_id": customer_id,
                    "amount": amount,
                    "card_type": card_type,
                    "transaction_id": f"fake_tx_{len(self.payments) + 1}"
                }
                self.payments.append(payment)
                
                # Return success response
                return {
                    "status": "success",
                    "transaction_id": payment["transaction_id"]
                }
        
        # Use the fake
        fake_gateway = FakePaymentGateway()
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=fake_gateway)
        
        # Process multiple payments
        result1 = payment_service.process_payment("customer_1", 100.00, "visa")
        result2 = payment_service.process_payment("customer_2", 200.00, "mastercard")
        
        # Verify results
        assert result1["transaction_id"] == "fake_tx_1"
        assert result2["transaction_id"] == "fake_tx_2"
        
        # Verify fake gateway state
        assert len(fake_gateway.payments) == 2
        assert fake_gateway.payments[0]["customer_id"] == "customer_1"
        assert fake_gateway.payments[1]["amount"] == 200.00</code></pre>
            </div>
        </section>

        <section class="functional_testing">
            <h2>Functional API Testing</h2>
            
            <p>Functional testing focuses on testing complete API endpoints against their requirements and specifications. It verifies that each endpoint correctly handles requests, processes data, and returns appropriate responses according to the API contract.</p>
            
            <p>Think of functional testing like test-driving a car—you're checking that all features work as advertised from the user's perspective, without necessarily being concerned with what's happening under the hood.</p>
            
            <div class="code-section">
                <h3>Functional Testing with pytest and requests</h3>
                <pre><code>import pytest
import requests
import json

# Base URL for the API
BASE_URL = "http://localhost:5000/api"

# Test data
TEST_USER = {
    "username": "functionaltest",
    "email": "functional@example.com",
    "password": "Password123!"
}

# Fixture for authentication token
@pytest.fixture
def auth_token():
    """Get authentication token for testing"""
    # Register a user if not exists
    try:
        register_response = requests.post(
            f"{BASE_URL}/users/register",
            json=TEST_USER
        )
        # If user already exists, just log in
        if register_response.status_code not in (201, 409):
            raise Exception(f"Failed to register test user: {register_response.text}")
    except Exception as e:
        print(f"Error during user registration: {str(e)}")
    
    # Login to get token
    login_response = requests.post(
        f"{BASE_URL}/auth/login",
        json={
            "username": TEST_USER["username"],
            "password": TEST_USER["password"]
        }
    )
    
    if login_response.status_code != 200:
        raise Exception(f"Failed to login: {login_response.text}")
    
    return login_response.json()["access_token"]

# Test user management endpoints
class TestUserAPI:
    def test_get_user_profile(self, auth_token):
        """Test getting user profile"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        response = requests.get(f"{BASE_URL}/users/profile", headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert data["username"] == TEST_USER["username"]
        assert data["email"] == TEST_USER["email"]
    
    def test_get_user_profile_unauthorized(self):
        """Test getting user profile without token"""
        response = requests.get(f"{BASE_URL}/users/profile")
        
        assert response.status_code == 401
    
    def test_update_user_profile(self, auth_token):
        """Test updating user profile"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        new_data = {"bio": "Updated bio for functional tests"}
        
        response = requests.patch(
            f"{BASE_URL}/users/profile",
            headers=headers,
            json=new_data
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["bio"] == new_data["bio"]
        
        # Verify the update persisted
        verify_response = requests.get(f"{BASE_URL}/users/profile", headers=headers)
        assert verify_response.status_code == 200
        verify_data = verify_response.json()
        assert verify_data["bio"] == new_data["bio"]

# Test product API endpoints
class TestProductAPI:
    @pytest.fixture
    def product_id(self, auth_token):
        """Create a test product and return its ID"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        product_data = {
            "name": "Test Product",
            "description": "Product for functional tests",
            "price": 19.99,
            "category": "test"
        }
        
        response = requests.post(
            f"{BASE_URL}/products",
            headers=headers,
            json=product_data
        )
        
        assert response.status_code == 201
        product_id = response.json()["id"]
        
        yield product_id
        
        # Clean up - delete the product after tests
        delete_response = requests.delete(
            f"{BASE_URL}/products/{product_id}",
            headers=headers
        )
        assert delete_response.status_code in (200, 204, 404)
    
    def test_get_products(self):
        """Test getting product list"""
        response = requests.get(f"{BASE_URL}/products")
        
        assert response.status_code == 200
        data = response.json()
        assert isinstance(data, list)
    
    def test_get_product_by_id(self, product_id):
        """Test getting a specific product"""
        response = requests.get(f"{BASE_URL}/products/{product_id}")
        
        assert response.status_code == 200
        data = response.json()
        assert data["id"] == product_id
        assert data["name"] == "Test Product"
    
    def test_get_nonexistent_product(self):
        """Test getting a product that doesn't exist"""
        response = requests.get(f"{BASE_URL}/products/999999")
        
        assert response.status_code == 404
    
    def test_search_products(self, product_id):
        """Test searching for products"""
        # Search by name
        response = requests.get(f"{BASE_URL}/products?search=Test Product")
        
        assert response.status_code == 200
        data = response.json()
        assert len(data) > 0
        assert any(p["id"] == product_id for p in data)
        
        # Search by category
        response = requests.get(f"{BASE_URL}/products?category=test")
        
        assert response.status_code == 200
        data = response.json()
        assert len(data) > 0
        assert any(p["id"] == product_id for p in data)
    
    def test_update_product(self, auth_token, product_id):
        """Test updating a product"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        update_data = {
            "name": "Updated Test Product",
            "price": 29.99
        }
        
        response = requests.patch(
            f"{BASE_URL}/products/{product_id}",
            headers=headers,
            json=update_data
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["name"] == update_data["name"]
        assert float(data["price"]) == update_data["price"]

# Test order API endpoints
class TestOrderAPI:
    @pytest.fixture
    def product_for_order(self, auth_token):
        """Create a test product for ordering"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        product_data = {
            "name": "Order Test Product",
            "description": "Product for order tests",
            "price": 15.99,
            "category": "test",
            "stock": 10
        }
        
        response = requests.post(
            f"{BASE_URL}/products",
            headers=headers,
            json=product_data
        )
        
        assert response.status_code == 201
        return response.json()
    
    def test_create_order(self, auth_token, product_for_order):
        """Test creating an order"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        
        # Create order
        order_data = {
            "items": [
                {
                    "product_id": product_for_order["id"],
                    "quantity": 2
                }
            ],
            "shipping_address": "123 Test St, Test City",
            "payment_method": "credit_card"
        }
        
        response = requests.post(
            f"{BASE_URL}/orders",
            headers=headers,
            json=order_data
        )
        
        assert response.status_code == 201
        data = response.json()
        assert "id" in data
        assert len(data["items"]) == 1
        assert data["items"][0]["product_id"] == product_for_order["id"]
        assert data["items"][0]["quantity"] == 2
        assert data["total"] == 2 * float(product_for_order["price"])
        
        # Verify product stock was reduced
        product_response = requests.get(f"{BASE_URL}/products/{product_for_order['id']}")
        assert product_response.status_code == 200
        updated_product = product_response.json()
        assert updated_product["stock"] == product_for_order["stock"] - 2
        
        # Get order by ID
        order_id = data["id"]
        get_response = requests.get(
            f"{BASE_URL}/orders/{order_id}",
            headers=headers
        )
        
        assert get_response.status_code == 200
        get_data = get_response.json()
        assert get_data["id"] == order_id</code></pre>

                <h3>Functional Testing with Postman/Newman</h3>
                <p>Postman provides a user-friendly interface for API testing and can be automated with Newman.</p>
                <p>Example Postman Collection (exported as JSON):</p>
                <pre><code>{
    "info": {
        "name": "API Functional Tests",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Authentication",
            "item": [
                {
                    "name": "Register User",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"username\": \"postmanuser\",\n    \"email\": \"postman@example.com\",\n    \"password\": \"Password123!\"\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/users/register",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "users", "register"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "// Check if registration was successful or user already exists",
                                    "pm.test(\"Status code is 201 (Created) or 409 (Conflict)\", function () {",
                                    "    pm.expect(pm.response.code).to.be.oneOf([201, 409]);",
                                    "});",
                                    "",
                                    "if (pm.response.code === 201) {",
                                    "    const responseJson = pm.response.json();",
                                    "    ",
                                    "    pm.test(\"Response contains user info\", function () {",
                                    "        pm.expect(responseJson.username).to.equal(\"postmanuser\");",
                                    "        pm.expect(responseJson.email).to.equal(\"postman@example.com\");",
                                    "    });",
                                    "}"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Login",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"username\": \"postmanuser\",\n    \"password\": \"Password123!\"\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/auth/login",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "auth", "login"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 200\", function () {",
                                    "    pm.response.to.have.status(200);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Response contains access token\", function () {",
                                    "    pm.expect(responseJson.access_token).to.be.a('string');",
                                    "    pm.expect(responseJson.refresh_token).to.be.a('string');",
                                    "});",
                                    "",
                                    "// Store token for later use",
                                    "pm.environment.set(\"access_token\", responseJson.access_token);",
                                    "pm.environment.set(\"refresh_token\", responseJson.refresh_token);"
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Products",
            "item": [
                {
                    "name": "Create Product",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            },
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"name\": \"Postman Test Product\",\n    \"description\": \"Product created through Postman tests\",\n    \"price\": 24.99,\n    \"category\": \"test\",\n    \"stock\": 50\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/products",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 201\", function () {",
                                    "    pm.response.to.have.status(201);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Product was created with correct data\", function () {",
                                    "    pm.expect(responseJson.name).to.equal(\"Postman Test Product\");",
                                    "    pm.expect(responseJson.price).to.equal(24.99);",
                                    "    pm.expect(responseJson.stock).to.equal(50);",
                                    "});",
                                    "",
                                    "// Store product ID for later tests",
                                    "pm.environment.set(\"product_id\", responseJson.id);"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Get Product",
                    "request": {
                        "method": "GET",
                        "header": [],
                        "url": {
                            "raw": "{{baseUrl}}/api/products/{{product_id}}",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "{{product_id}}"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 200\", function () {",
                                    "    pm.response.to.have.status(200);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Product has correct data\", function () {",
                                    "    pm.expect(responseJson.id).to.equal(pm.environment.get(\"product_id\"));",
                                    "    pm.expect(responseJson.name).to.equal(\"Postman Test Product\");",
                                    "    pm.expect(responseJson.price).to.equal(24.99);",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Update Product",
                    "request": {
                        "method": "PATCH",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            },
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"name\": \"Updated Postman Product\",\n    \"price\": 29.99\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/products/{{product_id}}",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "{{product_id}}"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 200\", function () {",
                                    "    pm.response.to.have.status(200);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Product was updated correctly\", function () {",
                                    "    pm.expect(responseJson.name).to.equal(\"Updated Postman Product\");",
                                    "    pm.expect(responseJson.price).to.equal(29.99);",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Delete Product",
                    "request": {
                        "method": "DELETE",
                        "header": [
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                            }
                        ],
                        "url": {
                            "raw": "{{baseUrl}}/api/products/{{product_id}}",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "{{product_id}}"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 204\", function () {",
                                    "    pm.response.to.have.status(204);",
                                    "});",
                                    "",
                                    "// Verify product is deleted",
                                    "pm.sendRequest({",
                                    "    url: pm.environment.get(\"baseUrl\") + \"/api/products/\" + pm.environment.get(\"product_id\"),",
                                    "    method: 'GET'",
                                    "}, function (err, res) {",
                                    "    pm.test(\"Product should no longer exist\", function () {",
                                    "        pm.expect(res.code).to.equal(404);",
                                    "    });",
                                    "});"
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Error Handling",
            "item": [
                {
                    "name": "Invalid Authentication",
                    "request": {
                        "method": "GET",
                        "header": [
                            {
                                "key": "Authorization",
                                "value": "Bearer invalid_token"
                            }
                        ],
                        "url": {
                            "raw": "{{baseUrl}}/api/users/profile",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "users", "profile"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 401 Unauthorized\", function () {",
                                    "    pm.response.to.have.status(401);",
                                    "});",
                                    "",
                                    "pm.test(\"Response has error message\", function () {",
                                    "    const responseJson = pm.response.json();",
                                    "    pm.expect(responseJson.error).to.be.a('string');",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Invalid Input",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            },
                            {
                                "key": "Authorization",
                                "value":)
    
    # Configure authentication
    auth_url = f'{target}/auth/login'
    auth_data = json.dumps({
        'username': 'testuser',
        'password': 'Password123!'
    })
    
    print('Setting up authentication...')
    zap.authentication.set_authentication_method(
        contextid=contextId,
        authenticationmethod='jsonBasedAuthentication',
        authenticationmethodconfigparams=f'loginUrl={auth_url}&loginRequestData={auth_data}'
    )
    
    # Set up auth token handling
    print('Setting up authorization header handling...')
    zap.replacer.add_rule(
        description='Add Auth Token',
        enabled=True,
        matchtype='REQ_HEADER',
        matchstring='Authorization',
        matchregex=False,
        replacement='Bearer {{token}}'  # Token will be captured and used
    )
    
    # Spider the API (discovery)
    print('Spidering the API...')
    scanid = zap.spider.scan(target, recurse=True)
    
    # Wait for the spider to complete
    while int(zap.spider.status(scanid)) < 100:
        print(f'Spider progress: {zap.spider.status(scanid)}%')
        time.sleep(5)
    
    # Perform active scan
    print('Running active scan...')
    scan_id = zap.ascan.scan(target)
    
    # Wait for the scan to complete
    while int(zap.ascan.status(scan_id)) < 100:
        print(f'Scan progress: {zap.ascan.status(scan_id)}%')
        time.sleep(5)
    
    # Get the alerts
    print('Generating report...')
    alerts = zap.core.alerts(baseurl=target)
    
    # Print the alerts
    for alert in alerts:
        print(f"Alert: {alert['name']}")
        print(f"Risk: {alert['risk']}")
        print(f"URL: {alert['url']}")
        print(f"Parameter: {alert['param']}")
        print(f"Description: {alert['description']}")
        print(f"Solution: {alert['solution']}")
        print("---")
    
    # Generate an HTML report
    report_path = 'zap_report.html'
    with open(report_path, 'w') as f:
        f.write(zap.core.htmlreport())
    
    print(f'Report saved to {report_path}')
    
    # Check for high-risk issues
    high_risks = [a for a in alerts if a['risk'] == 'High']
    assert len(high_risks) == 0, f"Found {len(high_risks)} high-risk security issues"

if __name__ == '__main__':
    test_api_with_zap()</code></pre>
            </div>
            
            <div class="security-tools">
                <h3>API Security Testing Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Type</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>OWASP ZAP</td>
                            <td>Dynamic Application Security Testing (DAST)</td>
                            <td>Automated vulnerability scanning, active and passive scanning</td>
                        </tr>
                        <tr>
                            <td>Burp Suite</td>
                            <td>Web Proxy and Scanner</td>
                            <td>Manual and automated security testing, intercepting and modifying requests</td>
                        </tr>
                        <tr>
                            <td>SQLmap</td>
                            <td>SQL Injection Testing</td>
                            <td>Automated detection and exploitation of SQL injection vulnerabilities</td>
                        </tr>
                        <tr>
                            <td>Bandit</td>
                            <td>Static Application Security Testing (SAST)</td>
                            <td>Python-specific static code analysis to find security issues</td>
                        </tr>
                        <tr>
                            <td>JWT_Tool</td>
                            <td>JWT Security Testing</td>
                            <td>Testing JWT implementations for vulnerabilities</td>
                        </tr>
                        <tr>
                            <td>OWASP Dependency-Check</td>
                            <td>Dependency Scanner</td>
                            <td>Identifying known vulnerabilities in project dependencies</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="best-practices">
                <h3>API Security Testing Best Practices</h3>
                <ul>
                    <li><strong>Shift Left:</strong> Integrate security testing early in the development process.</li>
                    <li><strong>Combine Testing Types:</strong> Use both SAST (static) and DAST (dynamic) testing tools.</li>
                    <li><strong>Automate Security Tests:</strong> Include security scans in your CI/CD pipeline.</li>
                    <li><strong>Test Authentication:</strong> Thoroughly test all authentication mechanisms.</li>
                    <li><strong>Verify Access Controls:</strong> Test both vertical (role-based) and horizontal (object-level) access controls.</li>
                    <li><strong>Test Input Validation:</strong> Check for injection vulnerabilities with various input types.</li>
                    <li><strong>Check Error Handling:</strong> Ensure errors don't leak sensitive information.</li>
                    <li><strong>Review Security Headers:</strong> Verify all necessary security headers are present.</li>
                    <li><strong>Test Rate Limiting:</strong> Verify protection against brute force and DoS attacks.</li>
                    <li><strong>Regular Dependency Reviews:</strong> Check for vulnerabilities in dependencies regularly.</li>
                </ul>
            </div>
        </section>

        <section class="performance_testing">
            <h2>Performance Testing</h2>
            
            <p>Performance testing ensures that your API can handle expected loads, responds within acceptable timeframes, and uses resources efficiently. It helps identify bottlenecks, capacity limits, and areas for optimization before they impact users in production.</p>
            
            <p>Think of performance testing like stress-testing a bridge—you want to know how much weight it can bear and where it might fail under pressure before it's put into actual use.</p>
            
            <div class="types-of-performance-tests">
                <h3>Types of Performance Tests</h3>
                <ul>
                    <li><strong>Load Testing:</strong> Testing how the system performs under expected load</li>
                    <li><strong>Stress Testing:</strong> Testing system behavior beyond normal or peak load</li>
                    <li><strong>Endurance Testing:</strong> Testing system behavior under sustained load over time</li>
                    <li><strong>Spike Testing:</strong> Testing system response to sudden large spikes in load</li>
                    <li><strong>Volume Testing:</strong> Testing with large amounts of data</li>
                    <li><strong>Scalability Testing:</strong> Testing how the system scales with increasing load</li>
                </ul>
            </div>
            
            <div class="code-section">
                <h3>Load Testing with Locust</h3>
                <pre><code>from locust import HttpUser, task, between
import json
import random

class APIUser(HttpUser):
    # Wait between 1 and 5 seconds between tasks
    wait_time = between(1, 5)
    
    def on_start(self):
        """Setup before starting tests"""
        # Login to get authentication token
        response = self.client.post(
            "/api/auth/login",
            json={
                "username": "loadtest",
                "password": "Password123!"
            }
        )
        
        # Check if login was successful
        if response.status_code == 200:
            data = response.json()
            self.token = data.get("access_token")
            self.auth_headers = {"Authorization": f"Bearer {self.token}"}
        else:
            # If login fails, try to register first
            self.client.post(
                "/api/users/register",
                json={
                    "username": "loadtest",
                    "email": "loadtest@example.com",
                    "password": "Password123!"
                }
            )
            
            # Try login again
            response = self.client.post(
                "/api/auth/login",
                json={
                    "username": "loadtest",
                    "password": "Password123!"
                }
            )
            
            data = response.json()
            self.token = data.get("access_token")
            self.auth_headers = {"Authorization": f"Bearer {self.token}"}
    
    @task(10)  # Higher weight = more frequent
    def get_products(self):
        """Test getting products list"""
        # Add query parameters randomly to simulate different user behaviors
        params = {}
        
        # Sometimes add category filter
        if random.random() < 0.3:
            categories = ["electronics", "clothing", "books", "food"]
            params["category"] = random.choice(categories)
        
        # Sometimes add search term
        if random.random() < 0.2:
            search_terms = ["phone", "laptop", "book", "shirt"]
            params["search"] = random.choice(search_terms)
        
        # Sometimes add pagination
        if random.random() < 0.5:
            params["page"] = random.randint(1, 5)
            params["per_page"] = random.choice([10, 20, 50])
        
        # Make the request
        self.client.get("/api/products", params=params)
    
    @task(5)
    def get_product_details(self):
        """Test getting details for a specific product"""
        # Get a random product ID between 1 and 20
        # In a real test, you might want to get actual IDs from the API
        product_id = random.randint(1, 20)
        self.client.get(f"/api/products/{product_id}")
    
    @task(2)
    def create_product(self):
        """Test creating a new product"""
        product_data = {
            "name": f"Load Test Product {random.randint(1000, 9999)}",
            "description": "Product created during load testing",
            "price": round(random.uniform(9.99, 99.99), 2),
            "category": random.choice(["test", "load", "performance"]),
            "stock": random.randint(1, 100)
        }
        
        self.client.post(
            "/api/products",
            json=product_data,
            headers=self.auth_headers
        )
    
    @task(1)
    def place_order(self):
        """Test placing an order"""
        # First, get available products
        response = self.client.get("/api/products")
        if response.status_code != 200:
            return
        
        products = response.json()
        if not products:
            return
        
        # Select 1-3 random products
        num_items = random.randint(1, 3)
        selected_products = random.sample(products, min(num_items, len(products)))
        
        # Create order items
        items = []
        for product in selected_products:
            items.append({
                "product_id": product["id"],
                "quantity": random.randint(1, 5)
            })
        
        # Place order
        order_data = {
            "items": items,
            "shipping_address": "123 Load Test St",
            "payment_method": random.choice(["credit_card", "paypal", "bank_transfer"])
        }
        
        self.client.post(
            "/api/orders",
            json=order_data,
            headers=self.auth_headers
        )</code></pre>

                <h3>Running and Analyzing Locust Tests</h3>
                <pre><code># Run from command line
locust -f locustfile.py --host=http://localhost:5000

# Or run programmatically
import subprocess
import time
import json
import matplotlib.pyplot as plt
import pandas as pd

def run_locust_test(users, spawn_rate, run_time):
    """Run a Locust test with specific parameters"""
    # Start Locust in headless mode
    process = subprocess.Popen([
        "locust",
        "-f", "locustfile.py",
        "--host=http://localhost:5000",
        "--headless",
        "-u", str(users),
        "-r", str(spawn_rate),
        "--run-time", run_time,
        "--csv=results"
    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    # Wait for process to complete
    stdout, stderr = process.communicate()
    
    # Check if there were any errors
    if process.returncode != 0:
        print(f"Error running Locust: {stderr.decode()}")
        return None
    
    # Load and return results
    results = {
        "requests": pd.read_csv("results_stats.csv"),
        "failures": pd.read_csv("results_failures.csv"),
        "exceptions": pd.read_csv("results_exceptions.csv")
    }
    
    return results

def analyze_results(results):
    """Analyze Locust test results"""
    if not results:
        return
    
    requests = results["requests"]
    
    # Summary statistics
    print("\nPerformance Test Results:")
    print("-" * 50)
    print(f"Total Requests: {requests['Total'].sum()}")
    print(f"Failed Requests: {requests['Fails'].sum()}")
    print(f"Median Response Time: {requests['Median Response Time'].median():.2f} ms")
    print(f"95th Percentile: {requests['95%'].mean():.2f} ms")
    print(f"Requests/sec: {requests['Requests/s'].mean():.2f}")
    
    # Endpoint performance
    endpoint_stats = requests.groupby("Name").agg({
        "Median Response Time": "mean",
        "95%": "mean",
        "Requests/s": "mean",
        "Fails": "sum",
        "Total": "sum"
    }).sort_values("Median Response Time", ascending=False)
    
    print("\nEndpoint Performance:")
    print("-" * 50)
    print(endpoint_stats)
    
    # Check for failures
    failures = results["failures"]
    if len(failures) > 0:
        print("\nFailures:")
        print("-" * 50)
        print(failures)
    
    # Plot response times
    plt.figure(figsize=(10, 6))
    plt.bar(requests["Name"], requests["Median Response Time"])
    plt.xticks(rotation=45, ha="right")
    plt.ylabel("Median Response Time (ms)")
    plt.title("Endpoint Response Times")
    plt.tight_layout()
    plt.savefig("response_times.png")
    
    # Plot throughput
    plt.figure(figsize=(10, 6))
    plt.bar(requests["Name"], requests["Requests/s"])
    plt.xticks(rotation=45, ha="right")
    plt.ylabel("Requests per Second")
    plt.title("Endpoint Throughput")
    plt.tight_layout()
    plt.savefig("throughput.png")

# Run with different user loads
test_configs = [
    {"users": 10, "spawn_rate": 1, "run_time": "1m"},
    {"users": 50, "spawn_rate": 5, "run_time": "1m"},
    {"users": 100, "spawn_rate": 10, "run_time": "1m"},
    {"users": 200, "spawn_rate": 20, "run_time": "1m"}
]

results_by_load = {}

for config in test_configs:
    print(f"\nRunning test with {config['users']} users...")
    results = run_locust_test(config["users"], config["spawn_rate"], config["run_time"])
    results_by_load[config["users"]] = results
    analyze_results(results)
    
    # Wait between tests
    time.sleep(30)

# Compare results across different loads
user_counts = list(results_by_load.keys())
median_response_times = [results_by_load[u]["requests"]["Median Response Time"].mean() for u in user_counts]
request_rates = [results_by_load[u]["requests"]["Requests/s"].mean() for u in user_counts]

plt.figure(figsize=(10, 6))
plt.plot(user_counts, median_response_times, 'o-')
plt.xlabel("Number of Users")
plt.ylabel("Median Response Time (ms)")
plt.title("Scalability: Response Time vs Load")
plt.grid(True)
plt.savefig("scalability_response_time.png")

plt.figure(figsize=(10, 6))
plt.plot(user_counts, request_rates, 'o-')
plt.xlabel("Number of Users")
plt.ylabel("Requests per Second")
plt.title("Scalability: Throughput vs Load")
plt.grid(True)
plt.savefig("scalability_throughput.png")</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Performance Testing Best Practices</h3>
                <ul>
                    <li><strong>Define Clear Metrics:</strong> Set specific KPIs like response time, throughput, and error rate.</li>
                    <li><strong>Test Realistic Scenarios:</strong> Create test cases that mimic real user behavior.</li>
                    <li><strong>Establish Baselines:</strong> Measure performance under normal conditions as a benchmark.</li>
                    <li><strong>Isolate Test Environment:</strong> Test in an environment that mirrors production but is isolated.</li>
                    <li><strong>Monitor Resource Usage:</strong> Track CPU, memory, database connections, and network usage.</li>
                    <li><strong>Test Incrementally:</strong> Start with a small load and gradually increase.</li>
                    <li><strong>Test Regularly:</strong> Run performance tests after significant changes and before releases.</li>
                    <li><strong>Automate Performance Tests:</strong> Include them in your CI/CD pipeline when possible.</li>
                    <li><strong>Analyze Bottlenecks:</strong> Determine if issues are in code, database, network, or infrastructure.</li>
                    <li><strong>Focus on Key Endpoints:</strong> Prioritize testing for critical and resource-intensive endpoints.</li>
                </ul>
            </div>
            
            <div class="performance-tools">
                <h3>API Performance Testing Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Type</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Locust</td>
                            <td>Python-based load testing</td>
                            <td>Developer-friendly, Python scripters, realistic user scenarios</td>
                        </tr>
                        <tr>
                            <td>JMeter</td>
                            <td>Java-based load testing</td>
                            <td>Comprehensive testing, GUI-based test creation, complex scenarios</td>
                        </tr>
                        <tr>
                            <td>Artillery</td>
                            <td>Node.js-based load testing</td>
                            <td>JavaScript developers, microservices, cloud-native applications</td>
                        </tr>
                        <tr>
                            <td>Gatling</td>
                            <td>Scala-based load testing</td>
                            <td>High-performance testing, continuous load testing, detailed metrics</td>
                        </tr>
                        <tr>
                            <td>k6</td>
                            <td>Modern load testing</td>
                            <td>Developer-centric testing, CI/CD integration, cloud testing</td>
                        </tr>
                        <tr>
                            <td>Taurus</td>
                            <td>Test automation framework</td>
                            <td>Simplifying complex testing tools, CI/CD integration, YAML-based config</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <section class="ci_cd_integration">
            <h2>Integrating Tests into CI/CD Pipelines</h2>
            
            <p>Integrating your API tests into Continuous Integration and Continuous Delivery/Deployment (CI/CD) pipelines ensures that tests are run automatically with every code change, providing quick feedback and preventing regressions.</p>
            
            <p>Think of CI/CD integration like an automated quality control system in a factory—each product gets inspected at various stages of production without requiring manual intervention, ensuring consistent quality.</p>
            
            <div class="pipeline-stages">
                <h3>Test Stages in a CI/CD Pipeline</h3>
                <ul>
                    <li><strong>Unit Tests:</strong> Run for every commit, should be fast (seconds to minutes)</li>
                    <li><strong>Integration Tests:</strong> Run for PRs/merges, can be slower (minutes)</li>
                    <li><strong>Contract Tests:</strong> Run when API contracts change or during PR validation</li>
                    <li><strong>Functional Tests:</strong> Run for staging deployments, validate core flows</li>
                    <li><strong>Security Tests:</strong> Run regularly, especially for security-sensitive changes</li>
                    <li><strong>Performance Tests:</strong> Run on a schedule or before major releases</li>
                </ul>
            </div>
            
            <div class="code-section">
                <h3>GitHub Actions Workflow for API Testing</h3>
                <pre><code># .github/workflows/api-tests.yml
name: API Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    
    services:
      # Add database service if needed
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov flake8
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Run unit tests
      run: |
        pytest tests/unit --cov=app --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v1
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
  
  integration-tests:
    needs: unit-tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest pytest-cov
    
    - name: Run integration tests
      run: |
        # Start the API server
        python app.py &
        # Wait for server to start
        sleep 5
        # Run integration tests
        pytest tests/integration
  
  contract-tests:
    needs: integration-tests
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest openapi-spec-validator
    
    - name: Check API contract
      run: |
        # Validate OpenAPI spec
        pytest tests/contract/test_openapi_spec.py
    
    - name: Run Pact tests
      if: success()
      run: |
        # Install Pact
        pip install pact-python
        # Run Pact tests
        pytest tests/contract/test_pact.py
  
  security-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety
    
    - name: Static security analysis with Bandit
      run: |
        bandit -r app/ -f json -o bandit-results.json
    
    - name: Check dependencies for vulnerabilities
      run: |
        safety check -r requirements.txt
    
    - name: Upload security reports
      uses: actions/upload-artifact@v2
      with:
        name: security-reports
        path: |
          bandit-results.json
  
  deploy-staging:
    if: github.event_name == 'push' && github.ref == 'refs/heads/develop'
    needs: [unit-tests, integration-tests, contract-tests, security-tests]
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add your deployment steps here
    
    - name: Run functional tests on staging
      run: |
        echo "Running functional tests against staging..."
        # Add your functional test steps here
  
  performance-tests:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: deploy-staging
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install locust matplotlib pandas
    
    - name: Run performance tests
      run: |
        # Run Locust in headless mode
        locust -f tests/performance/locustfile.py --host=https://staging-api.example.com --headless -u 50 -r 5 --run-time 5m --csv=results
    
    - name: Upload performance test results
      uses: actions/upload-artifact@v2
      with:
        name: performance-results
        path: |
          results*.csv
  
  deploy-production:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: [performance-tests]
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add your production deployment steps here</code></pre>

                <h3>GitLab CI/CD Pipeline Configuration</h3>
                <pre><code># .gitlab-ci.yml
stages:
  - lint
  - test
  - security
  - deploy-staging
  - test-staging
  - performance
  - deploy-production

variables:
  POSTGRES_DB: test_db
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: postgres
  POSTGRES_HOST: postgres

lint:
  stage: lint
  image: python:3.9
  script:
    - pip install flake8
    - flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
    - flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

unit-tests:
  stage: test
  image: python:3.9
  services:
    - postgres:13
  script:
    - pip install -r requirements.txt
    - pip install pytest pytest-cov
    - pytest tests/unit --cov=app --cov-report=xml
  artifacts:
    paths:
      - coverage.xml
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml

integration-tests:
  stage: test
  image: python:3.9
  services:
    - postgres:13
  script:
    - pip install -r requirements.txt
    - pip install pytest
    - python app.py &
    - sleep 5
    - pytest tests/integration
  dependencies:
    - unit-tests

contract-tests:
  stage: test
  image: python:3.9
  script:
    - pip install -r requirements.txt
    - pip install pytest openapi-spec-validator pact-python
    - pytest tests/contract
  dependencies:
    - unit-tests<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Testing APIs: A Comprehensive Guide</title>
    <link rel="stylesheet" href="/styles/main.css">
    <link rel="icon" href="/favicon.png">
</head>
<body>
    <header>
        <h1>Testing APIs: A Comprehensive Guide</h1>
        <p class="subtitle">Python Full Stack Web Course</p>
    </header>

    <main>
        <section class="introduction">
            <h2>Introduction to API Testing</h2>
            
            <p>Welcome to our comprehensive guide on API testing for Python Full Stack development! In today's interconnected world, APIs (Application Programming Interfaces) have become the critical nervous system of modern software, enabling applications to communicate and share data. However, like any crucial component, APIs must be rigorously tested to ensure they function correctly, securely, and efficiently.</p>
            
            <p>Think of API testing as the quality control department in a manufacturing plant. Just as QC inspectors examine products for defects before they reach customers, API testing identifies bugs, vulnerabilities, and performance issues before your API reaches production. Without this critical process, you risk exposing your users to frustrating experiences and your systems to potential security breaches.</p>
            
            <p>Throughout this tutorial, we'll explore the various aspects of API testing, from simple unit tests to complex integration and performance testing. We'll cover both theoretical concepts and practical implementations using Python's rich ecosystem of testing tools and frameworks. By the end, you'll have a comprehensive understanding of how to implement a robust testing strategy for your API projects.</p>
            
            <div class="key-concepts">
                <h3>Key Testing Concepts</h3>
                <ul>
                    <li><strong>Unit Testing:</strong> Testing individual components in isolation</li>
                    <li><strong>Integration Testing:</strong> Testing interactions between components</li>
                    <li><strong>Functional Testing:</strong> Testing complete API workflows</li>
                    <li><strong>Load Testing:</strong> Testing performance under expected load</li>
                    <li><strong>Security Testing:</strong> Testing for vulnerabilities and security issues</li>
                    <li><strong>Contract Testing:</strong> Ensuring the API adheres to its specification</li>
                    <li><strong>Test-Driven Development (TDD):</strong> Writing tests before implementation</li>
                </ul>
            </div>
        </section>

        <section class="testing_pyramid">
            <h2>The API Testing Pyramid</h2>
            
            <p>The Testing Pyramid is a conceptual framework that helps us understand the different levels of testing and their relative importance. Think of it as a dietary pyramid—it guides you on the proportion of different test types you should include in your testing strategy.</p>
            
            <div class="pyramid-illustration">
                <div class="pyramid-level" style="width: 80%; background-color: #e0f7fa;">
                    <strong>Unit Tests</strong>
                    <p>Fast, focused, numerous</p>
                </div>
                <div class="pyramid-level" style="width: 65%; background-color: #b2ebf2;">
                    <strong>Integration Tests</strong>
                    <p>Component interactions</p>
                </div>
                <div class="pyramid-level" style="width: 50%; background-color: #80deea;">
                    <strong>API Tests</strong>
                    <p>Testing API endpoints</p>
                </div>
                <div class="pyramid-level" style="width: 35%; background-color: #4dd0e1;">
                    <strong>End-to-End Tests</strong>
                    <p>Complete workflows</p>
                </div>
                <div class="pyramid-level" style="width: 20%; background-color: #26c6da;">
                    <strong>Manual/Exploratory Tests</strong>
                    <p>Human testing</p>
                </div>
            </div>
            
            <div class="pyramid-explanation">
                <h3>Why the Pyramid Shape?</h3>
                <p>The pyramid shape represents both quantity and speed:</p>
                <ul>
                    <li><strong>Unit Tests (Base):</strong> 
                        <ul>
                            <li>Most numerous (70-80% of your tests)</li>
                            <li>Fastest to run (milliseconds)</li>
                            <li>Cheapest to maintain</li>
                            <li>Highest level of isolation</li>
                        </ul>
                    </li>
                    <li><strong>Integration Tests (Middle):</strong>
                        <ul>
                            <li>Moderate number (15-20% of your tests)</li>
                            <li>Medium speed (seconds)</li>
                            <li>Test component interactions</li>
                        </ul>
                    </li>
                    <li><strong>API/Functional Tests (Middle-Upper):</strong>
                        <ul>
                            <li>Focus on API contract and behavior</li>
                            <li>Test complete endpoints</li>
                            <li>Ensure proper request/response handling</li>
                        </ul>
                    </li>
                    <li><strong>End-to-End Tests (Upper):</strong>
                        <ul>
                            <li>Fewer in number (5-10% of your tests)</li>
                            <li>Slower to run (seconds to minutes)</li>
                            <li>Test complete workflows</li>
                            <li>More brittle and costly to maintain</li>
                        </ul>
                    </li>
                    <li><strong>Manual Tests (Tip):</strong>
                        <ul>
                            <li>Least numerous</li>
                            <li>Slowest (minutes to hours)</li>
                            <li>Require human intervention</li>
                            <li>Cannot be easily automated</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <div class="pyramid-benefits">
                <h3>Benefits of Following the Pyramid</h3>
                <ul>
                    <li><strong>Faster Feedback:</strong> Issues are caught early in the development cycle</li>
                    <li><strong>Increased Efficiency:</strong> More tests at the levels that are cheaper to run and maintain</li>
                    <li><strong>Better Coverage:</strong> A balanced approach ensures comprehensive testing</li>
                    <li><strong>Improved Diagnosis:</strong> When a high-level test fails, lower-level tests help pinpoint the issue</li>
                    <li><strong>Sustainable Testing:</strong> The strategy remains feasible as your codebase grows</li>
                </ul>
            </div>
            
            <div class="anti-patterns">
                <h3>Testing Anti-Patterns to Avoid</h3>
                <ul>
                    <li><strong>Ice Cream Cone:</strong> More end-to-end tests than unit tests (inverted pyramid)</li>
                    <li><strong>Hourglass:</strong> Many unit and UI tests, but few integration tests</li>
                    <li><strong>Cupcake:</strong> Equal distribution of test types, ignoring the cost/benefit ratio</li>
                </ul>
            </div>
        </section>

        <section class="unit_testing">
            <h2>Unit Testing API Components</h2>
            
            <p>Unit testing forms the foundation of your API testing strategy. It involves testing individual components in isolation to ensure they function correctly. For APIs, units typically include controllers, services, models, utilities, and other discrete components that make up your API.</p>
            
            <p>Think of unit testing like checking individual Lego blocks before assembling them into a larger structure. You want to make sure each piece is correctly shaped and functional on its own before connecting it with others.</p>
            
            <div class="frameworks">
                <h3>Python Unit Testing Frameworks</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Framework</th>
                            <th>Description</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>pytest</td>
                            <td>Modern, flexible testing framework with powerful fixtures and plugins</td>
                            <td>Most Python projects; especially those needing advanced features</td>
                        </tr>
                        <tr>
                            <td>unittest</td>
                            <td>Python's built-in testing framework (xUnit style)</td>
                            <td>Simple projects; maintaining backward compatibility</td>
                        </tr>
                        <tr>
                            <td>nose2</td>
                            <td>Extended unittest with plugin support</td>
                            <td>Projects transitioning from unittest to pytest</td>
                        </tr>
                        <tr>
                            <td>doctest</td>
                            <td>Extract tests from docstrings</td>
                            <td>Simple projects; documentation-driven testing</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="code-section">
                <h3>Unit Testing Flask API Components with pytest</h3>
                <pre><code>import pytest
from app.services.user_service import UserService
from app.models.user import User
from app.exceptions import ValidationError
from unittest.mock import Mock, patch

# Test a service component
class TestUserService:
    def setup_method(self):
        """Setup before each test"""
        self.user_repository_mock = Mock()
        self.user_service = UserService(self.user_repository_mock)
    
    def test_get_user_by_id_returns_user_when_found(self):
        # Arrange
        user_id = 123
        expected_user = User(id=user_id, username="testuser", email="test@example.com")
        self.user_repository_mock.get_by_id.return_value = expected_user
        
        # Act
        result = self.user_service.get_user_by_id(user_id)
        
        # Assert
        assert result is not None
        assert result.id == user_id
        assert result.username == "testuser"
        self.user_repository_mock.get_by_id.assert_called_once_with(user_id)
    
    def test_get_user_by_id_returns_none_when_not_found(self):
        # Arrange
        user_id = 456
        self.user_repository_mock.get_by_id.return_value = None
        
        # Act
        result = self.user_service.get_user_by_id(user_id)
        
        # Assert
        assert result is None
        self.user_repository_mock.get_by_id.assert_called_once_with(user_id)
    
    def test_create_user_validates_email_format(self):
        # Arrange
        invalid_email = "not-an-email"
        
        # Act & Assert
        with pytest.raises(ValidationError) as excinfo:
            self.user_service.create_user("testuser", invalid_email, "password123")
        
        assert "Invalid email format" in str(excinfo.value)
        # Verify repository was never called
        self.user_repository_mock.save.assert_not_called()
    
    @patch('app.services.user_service.hash_password')
    def test_create_user_hashes_password(self, hash_password_mock):
        # Arrange
        username = "testuser"
        email = "test@example.com"
        password = "password123"
        hashed_password = "hashed_password_value"
        hash_password_mock.return_value = hashed_password
        
        # Act
        self.user_service.create_user(username, email, password)
        
        # Assert
        hash_password_mock.assert_called_once_with(password)
        # Verify user was saved with hashed password
        self.user_repository_mock.save.assert_called_once()
        saved_user = self.user_repository_mock.save.call_args[0][0]
        assert saved_user.username == username
        assert saved_user.email == email
        assert saved_user.password == hashed_password

# Test a utility function
def test_format_api_response():
    from app.utils.response_formatter import format_api_response
    
    # Test with success status
    result = format_api_response(data={"name": "Test"}, status="success")
    assert result["status"] == "success"
    assert result["data"]["name"] == "Test"
    assert "error" not in result
    
    # Test with error status
    result = format_api_response(error="Not found", status="error")
    assert result["status"] == "error"
    assert result["error"] == "Not found"
    assert "data" not in result

# Test a model/schema
def test_user_schema_validation():
    from app.schemas.user_schema import UserSchema
    
    # Test valid data
    valid_data = {"username": "testuser", "email": "test@example.com", "age": 25}
    schema = UserSchema()
    result = schema.load(valid_data)
    assert result["username"] == "testuser"
    assert result["email"] == "test@example.com"
    assert result["age"] == 25
    
    # Test invalid data (missing required field)
    invalid_data = {"username": "testuser"}
    with pytest.raises(ValidationError):
        schema.load(invalid_data)</code></pre>

                <h3>Unit Testing Django API Components</h3>
                <pre><code>from django.test import TestCase
from django.urls import reverse
from rest_framework import status
from unittest.mock import patch
from .models import Product
from .serializers import ProductSerializer
from .views import ProductViewSet

class ProductModelTests(TestCase):
    def test_product_creation(self):
        """Test product model creation and string representation"""
        product = Product.objects.create(
            name="Test Product",
            description="Test Description",
            price=99.99,
            stock=10
        )
        self.assertEqual(product.name, "Test Product")
        self.assertEqual(product.price, 99.99)
        self.assertEqual(str(product), "Test Product")
    
    def test_product_price_validation(self):
        """Test that products cannot have negative prices"""
        with self.assertRaises(Exception):
            Product.objects.create(
                name="Invalid Product",
                description="Product with negative price",
                price=-10.00,
                stock=5
            )

class ProductSerializerTests(TestCase):
    def test_valid_serializer(self):
        """Test serializer with valid data"""
        data = {
            'name': 'New Product',
            'description': 'Product Description',
            'price': 19.99,
            'stock': 5
        }
        serializer = ProductSerializer(data=data)
        self.assertTrue(serializer.is_valid())
    
    def test_invalid_serializer(self):
        """Test serializer with invalid data"""
        # Missing required fields
        data = {'name': 'Incomplete Product'}
        serializer = ProductSerializer(data=data)
        self.assertFalse(serializer.is_valid())
        self.assertIn('price', serializer.errors)
    
    def test_serializer_output(self):
        """Test serializer output format"""
        product = Product.objects.create(
            name="Test Product",
            description="Test Description",
            price=99.99,
            stock=10
        )
        serializer = ProductSerializer(product)
        data = serializer.data
        
        self.assertEqual(data['name'], "Test Product")
        self.assertEqual(float(data['price']), 99.99)
        # Check if created_at field is included
        self.assertIn('created_at', data)

class ProductViewSetTests(TestCase):
    @patch('api.services.product_service.get_all_products')
    def test_list_products(self, mock_get_all_products):
        """Test the list method of ProductViewSet"""
        # Arrange
        mock_products = [
            Product(id=1, name="Product 1", price=10.99, stock=5),
            Product(id=2, name="Product 2", price=20.99, stock=10)
        ]
        mock_get_all_products.return_value = mock_products
        
        view = ProductViewSet()
        view.request = None  # Simplified for unit testing
        
        # Act
        response = view.list(view.request)
        
        # Assert
        self.assertEqual(len(response.data), 2)
        mock_get_all_products.assert_called_once()
    
    @patch('api.services.product_service.get_product_by_id')
    def test_retrieve_product_not_found(self, mock_get_product):
        """Test retrieving a non-existent product"""
        # Arrange
        mock_get_product.return_value = None
        
        view = ProductViewSet()
        view.request = None  # Simplified for unit testing
        
        # Act & Assert
        with self.assertRaises(Exception):
            view.retrieve(view.request, pk=999)
        
        mock_get_product.assert_called_once_with(999)</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Unit Testing Best Practices</h3>
                <ul>
                    <li><strong>Test One Thing Per Test:</strong> Each test should verify a single behavior or edge case.</li>
                    <li><strong>Use Descriptive Test Names:</strong> Name tests to clearly indicate what they're testing and expected behavior.</li>
                    <li><strong>Follow AAA Pattern:</strong> Structure tests with Arrange, Act, Assert sections.</li>
                    <li><strong>Mock External Dependencies:</strong> Use mocking to isolate the unit being tested.</li>
                    <li><strong>Test Edge Cases:</strong> Include tests for boundary conditions and error scenarios.</li>
                    <li><strong>Keep Tests Independent:</strong> Tests should not depend on each other or shared state.</li>
                    <li><strong>Use Fixtures and Factories:</strong> For consistent test setup and data generation.</li>
                    <li><strong>Aim for High Coverage:</strong> Unit tests should cover most of your code.</li>
                    <li><strong>Keep Tests Fast:</strong> Unit tests should run in milliseconds.</li>
                </ul>
            </div>
            
            <div class="mocking">
                <h3>Effective Mocking Strategies</h3>
                <p>Mocking is essential for isolating the component you're testing. Here are some common scenarios:</p>
                
                <h4>What to Mock:</h4>
                <ul>
                    <li><strong>Database Interactions:</strong> Repository or ORM calls</li>
                    <li><strong>External API Calls:</strong> HTTP requests to third-party services</li>
                    <li><strong>File System Operations:</strong> Reading or writing files</li>
                    <li><strong>Time-dependent Functions:</strong> Functions that rely on current time</li>
                    <li><strong>Random Behaviors:</strong> Functions that generate random values</li>
                </ul>
                
                <h4>Mocking with pytest:</h4>
                <pre><code>import pytest
from unittest.mock import Mock, patch
import requests
from app.services.weather_service import WeatherService

# Mock a function
@patch('app.services.weather_service.get_current_time')
def test_get_forecast_includes_timestamp(mock_get_time):
    # Arrange
    mock_get_time.return_value = "2023-06-15T12:00:00Z"
    weather_service = WeatherService()
    
    # Act
    forecast = weather_service.get_forecast("London")
    
    # Assert
    assert forecast["timestamp"] == "2023-06-15T12:00:00Z"

# Mock an external API call
@patch('requests.get')
def test_get_forecast_calls_weather_api(mock_requests_get):
    # Arrange
    mock_response = Mock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        "temp": 22.5,
        "humidity": 65,
        "condition": "Sunny"
    }
    mock_requests_get.return_value = mock_response
    
    weather_service = WeatherService()
    
    # Act
    forecast = weather_service.get_forecast("London")
    
    # Assert
    mock_requests_get.assert_called_once()
    assert "api.weather.com" in mock_requests_get.call_args[0][0]
    assert forecast["temp"] == 22.5

# Mock a class
@patch('app.repositories.user_repository.UserRepository')
def test_authenticate_user_success(mock_user_repo_class):
    # Arrange
    mock_user_repo = Mock()
    mock_user_repo_class.return_value = mock_user_repo
    
    # Set up the mock to return a user with matching password
    mock_user_repo.get_by_username.return_value = {
        "username": "testuser",
        "password_hash": "hashed_password",
        "is_active": True
    }
    
    # Mock the password verification
    with patch('app.services.auth_service.verify_password', return_value=True):
        from app.services.auth_service import AuthService
        auth_service = AuthService()
        
        # Act
        result = auth_service.authenticate("testuser", "password123")
        
        # Assert
        assert result["authenticated"] is True
        assert result["username"] == "testuser"</code></pre>
            </div>
        </section>

        <section class="integration_testing">
            <h2>Integration Testing</h2>
            
            <p>While unit tests examine individual components in isolation, integration tests verify that these components work correctly together. In the context of API testing, integration tests typically involve testing how controllers interact with services, how services interact with repositories, and how the API interacts with databases or external services.</p>
            
            <p>Think of integration testing like testing how different musical instruments sound together in an orchestra, rather than testing each instrument separately. It ensures that all the parts harmonize correctly when combined.</p>
            
            <div class="code-section">
                <h3>Integration Testing Flask APIs</h3>
                <pre><code>import pytest
import json
from app import create_app
from app.database import db as _db
from app.models.user import User

# Fixture for Flask test app
@pytest.fixture
def app():
    """Create and configure a Flask app for testing"""
    app = create_app('testing')
    app.config['TESTING'] = True
    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///:memory:'
    
    # Create the database and tables
    with app.app_context():
        _db.create_all()
    
    yield app
    
    # Clean up
    with app.app_context():
        _db.drop_all()

# Fixture for database
@pytest.fixture
def db(app):
    """Database fixture"""
    with app.app_context():
        yield _db

# Fixture for test client
@pytest.fixture
def client(app):
    """Client for testing API endpoints"""
    return app.test_client()

# Fixture for test user
@pytest.fixture
def test_user(db):
    """Create a test user in the database"""
    with db.app.app_context():
        user = User(
            username='testuser',
            email='test@example.com'
        )
        user.set_password('password123')
        db.session.add(user)
        db.session.commit()
        return user

# Integration test for user creation and authentication flow
def test_register_and_login_integration(client, db):
    """Test the complete register & login flow"""
    # Register a new user
    register_response = client.post(
        '/api/users/register',
        data=json.dumps({
            'username': 'newuser',
            'email': 'new@example.com',
            'password': 'securepass123'
        }),
        content_type='application/json'
    )
    
    assert register_response.status_code == 201
    register_data = json.loads(register_response.data)
    assert register_data['username'] == 'newuser'
    
    # Verify user exists in database
    with db.app.app_context():
        user = User.query.filter_by(username='newuser').first()
        assert user is not None
        assert user.email == 'new@example.com'
        assert user.check_password('securepass123')
    
    # Login with created user
    login_response = client.post(
        '/api/auth/login',
        data=json.dumps({
            'username': 'newuser',
            'password': 'securepass123'
        }),
        content_type='application/json'
    )
    
    assert login_response.status_code == 200
    login_data = json.loads(login_response.data)
    assert 'access_token' in login_data
    assert 'refresh_token' in login_data
    
    # Use token to access protected endpoint
    profile_response = client.get(
        '/api/users/profile',
        headers={
            'Authorization': f"Bearer {login_data['access_token']}"
        }
    )
    
    assert profile_response.status_code == 200
    profile_data = json.loads(profile_response.data)
    assert profile_data['username'] == 'newuser'
    assert profile_data['email'] == 'new@example.com'

# Integration test for database operations
def test_product_crud_operations(client, db, test_user):
    """Test Create, Read, Update, Delete operations for products"""
    # Login first to get token
    login_response = client.post(
        '/api/auth/login',
        data=json.dumps({
            'username': 'testuser',
            'password': 'password123'
        }),
        content_type='application/json'
    )
    
    login_data = json.loads(login_response.data)
    token = login_data['access_token']
    headers = {'Authorization': f"Bearer {token}"}
    
    # Create a product
    create_response = client.post(
        '/api/products',
        headers=headers,
        data=json.dumps({
            'name': 'Test Product',
            'description': 'Test Description',
            'price': 29.99,
            'category': 'electronics'
        }),
        content_type='application/json'
    )
    
    assert create_response.status_code == 201
    product_data = json.loads(create_response.data)
    product_id = product_data['id']
    
    # Read the product
    get_response = client.get(f'/api/products/{product_id}')
    assert get_response.status_code == 200
    get_data = json.loads(get_response.data)
    assert get_data['name'] == 'Test Product'
    assert float(get_data['price']) == 29.99
    
    # Update the product
    update_response = client.put(
        f'/api/products/{product_id}',
        headers=headers,
        data=json.dumps({
            'name': 'Updated Product',
            'description': 'Updated Description',
            'price': 39.99,
            'category': 'electronics'
        }),
        content_type='application/json'
    )
    
    assert update_response.status_code == 200
    update_data = json.loads(update_response.data)
    assert update_data['name'] == 'Updated Product'
    assert float(update_data['price']) == 39.99
    
    # Verify in database
    with db.app.app_context():
        from app.models.product import Product
        product = Product.query.get(product_id)
        assert product.name == 'Updated Product'
        assert product.price == 39.99
    
    # Delete the product
    delete_response = client.delete(
        f'/api/products/{product_id}',
        headers=headers
    )
    
    assert delete_response.status_code == 204
    
    # Verify product is deleted
    get_after_delete = client.get(f'/api/products/{product_id}')
    assert get_after_delete.status_code == 404</code></pre>

                <h3>Integration Testing Django APIs</h3>
                <pre><code>from django.test import TestCase
from django.urls import reverse
from rest_framework.test import APIClient
from rest_framework import status
from .models import Product, Category, Order, OrderItem
from django.contrib.auth.models import User
import json

class ProductOrderIntegrationTest(TestCase):
    def setUp(self):
        # Create a test user
        self.user = User.objects.create_user(
            username='testuser',
            email='test@example.com',
            password='testpassword'
        )
        
        # Create API client
        self.client = APIClient()
        
        # Create categories
        self.category = Category.objects.create(
            name='Electronics',
            description='Electronic devices'
        )
        
        # Create products
        self.product1 = Product.objects.create(
            name='Laptop',
            description='Powerful laptop',
            price=999.99,
            stock=10,
            category=self.category
        )
        
        self.product2 = Product.objects.create(
            name='Phone',
            description='Smartphone',
            price=499.99,
            stock=20,
            category=self.category
        )
    
    def test_complete_order_flow(self):
        """Test the complete order flow from authentication to checkout"""
        # Step 1: Log in and get token
        login_url = reverse('token_obtain_pair')
        login_data = {
            'username': 'testuser',
            'password': 'testpassword'
        }
        login_response = self.client.post(login_url, login_data, format='json')
        self.assertEqual(login_response.status_code, status.HTTP_200_OK)
        
        # Extract token
        token = login_response.data['access']
        self.client.credentials(HTTP_AUTHORIZATION=f'Bearer {token}')
        
        # Step 2: Get product list
        products_url = reverse('product-list')
        products_response = self.client.get(products_url)
        self.assertEqual(products_response.status_code, status.HTTP_200_OK)
        self.assertEqual(len(products_response.data), 2)
        
        # Step 3: Create a new order (cart)
        orders_url = reverse('order-list')
        order_data = {'status': 'cart'}
        order_response = self.client.post(orders_url, order_data, format='json')
        self.assertEqual(order_response.status_code, status.HTTP_201_CREATED)
        order_id = order_response.data['id']
        
        # Step 4: Add items to the order
        items_url = reverse('orderitem-list')
        
        # Add first product
        item1_data = {
            'order': order_id,
            'product': self.product1.id,
            'quantity': 2
        }
        item1_response = self.client.post(items_url, item1_data, format='json')
        self.assertEqual(item1_response.status_code, status.HTTP_201_CREATED)
        
        # Add second product
        item2_data = {
            'order': order_id,
            'product': self.product2.id,
            'quantity': 1
        }
        item2_response = self.client.post(items_url, item2_data, format='json')
        self.assertEqual(item2_response.status_code, status.HTTP_201_CREATED)
        
        # Step 5: Get order details
        order_detail_url = reverse('order-detail', args=[order_id])
        order_detail_response = self.client.get(order_detail_url)
        self.assertEqual(order_detail_response.status_code, status.HTTP_200_OK)
        
        # Check order items
        self.assertEqual(len(order_detail_response.data['items']), 2)
        
        # Check order total
        expected_total = (self.product1.price * 2) + (self.product2.price * 1)
        self.assertEqual(float(order_detail_response.data['total']), expected_total)
        
        # Step 6: Update order to checkout
        checkout_data = {
            'status': 'processing',
            'shipping_address': '123 Test St, Test City, Test Country',
            'payment_method': 'credit_card'
        }
        checkout_response = self.client.patch(
            order_detail_url, 
            checkout_data,
            format='json'
        )
        self.assertEqual(checkout_response.status_code, status.HTTP_200_OK)
        self.assertEqual(checkout_response.data['status'], 'processing')
        
        # Step 7: Verify stock was reduced
        product1_url = reverse('product-detail', args=[self.product1.id])
        product1_response = self.client.get(product1_url)
        self.assertEqual(product1_response.status_code, status.HTTP_200_OK)
        self.assertEqual(product1_response.data['stock'], 8)  # 10 - 2
        
        product2_url = reverse('product-detail', args=[self.product2.id])
        product2_response = self.client.get(product2_url)
        self.assertEqual(product2_response.status_code, status.HTTP_200_OK)
        self.assertEqual(product2_response.data['stock'], 19)  # 20 - 1
        
        # Step 8: Verify in database
        updated_order = Order.objects.get(id=order_id)
        self.assertEqual(updated_order.status, 'processing')
        self.assertEqual(updated_order.shipping_address, '123 Test St, Test City, Test Country')
        
        # Product stock should be updated in database
        updated_product1 = Product.objects.get(id=self.product1.id)
        self.assertEqual(updated_product1.stock, 8)
        
        updated_product2 = Product.objects.get(id=self.product2.id)
        self.assertEqual(updated_product2.stock, 19)</code></pre>
            </div>
            
            <div class="best-practices">
                <h3>Integration Testing Best Practices</h3>
                <ul>
                    <li><strong>Focus on Component Interactions:</strong> Test how components work together, not individual behaviors.</li>
                    <li><strong>Use Test Databases:</strong> Use in-memory or dedicated test databases, not production databases.</li>
                    <li><strong>Test Complete Workflows:</strong> Test entire flows like registration-to-login or order-to-payment.</li>
                    <li><strong>Test Boundary Behaviors:</strong> Test edge cases where components meet.</li>
                    <li><strong>Isolate External Services:</strong> Mock external services that aren't part of the integration you're testing.</li>
                    <li><strong>Use Fixtures and Factories:</strong> For consistent test data across integration tests.</li>
                    <li><strong>Test Database Constraints:</strong> Verify that database constraints work as expected.</li>
                    <li><strong>Clean Up After Tests:</strong> Ensure each test starts with a clean state.</li>
                </ul>
            </div>
            
            <div class="test-doubles">
                <h3>Using Test Doubles</h3>
                <p>In integration testing, you often need to replace some components with test doubles:</p>
                
                <table>
                    <thead>
                        <tr>
                            <th>Type</th>
                            <th>Description</th>
                            <th>When to Use</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Dummy</td>
                            <td>Objects passed but never used</td>
                            <td>When parameter is required but not used</td>
                        </tr>
                        <tr>
                            <td>Stub</td>
                            <td>Provides canned responses</td>
                            <td>When you need specific values returned</td>
                        </tr>
                        <tr>
                            <td>Spy</td>
                            <td>Records calls made to it</td>
                            <td>When you need to verify calls were made</td>
                        </tr>
                        <tr>
                            <td>Mock</td>
                            <td>Pre-programmed with expectations</td>
                            <td>When you need to verify behavior</td>
                        </tr>
                        <tr>
                            <td>Fake</td>
                            <td>Working implementation, but not production-ready</td>
                            <td>When you need behavior but not the real component</td>
                        </tr>
                    </tbody>
                </table>
                
                <pre><code>import pytest
from unittest.mock import Mock, patch
import requests

# Example of different test doubles
class TestPaymentService:
    def test_process_payment_with_stub(self):
        """Using a stub for payment gateway"""
        # Create a stub that returns a successful response
        payment_gateway_stub = Mock()
        payment_gateway_stub.process_payment.return_value = {
            "status": "success",
            "transaction_id": "tx_12345",
            "amount": 100.00
        }
        
        # Use the stub in the payment service
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=payment_gateway_stub)
        
        # Process payment
        result = payment_service.process_payment("customer_1", 100.00, "visa")
        
        # Assert result
        assert result["success"] is True
        assert result["transaction_id"] == "tx_12345"
    
    def test_process_payment_with_spy(self):
        """Using a spy to verify calls"""
        # Create a spy
        payment_gateway_spy = Mock()
        payment_gateway_spy.process_payment.return_value = {
            "status": "success",
            "transaction_id": "tx_67890"
        }
        
        # Use the spy
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=payment_gateway_spy)
        
        # Process payment
        payment_service.process_payment("customer_1", 200.00, "mastercard")
        
        # Verify calls to the spy
        payment_gateway_spy.process_payment.assert_called_once()
        args, kwargs = payment_gateway_spy.process_payment.call_args
        assert kwargs["customer_id"] == "customer_1"
        assert kwargs["amount"] == 200.00
        assert kwargs["card_type"] == "mastercard"
    
    def test_process_payment_with_mock(self):
        """Using a mock with expectations"""
        # Create a mock with expectations
        payment_gateway_mock = Mock()
        payment_gateway_mock.process_payment.return_value = {
            "status": "success",
            "transaction_id": "tx_24680"
        }
        
        # Configure mock to raise exception for specific input
        def side_effect(customer_id, amount, card_type, **kwargs):
            if amount > 1000:
                raise ValueError("Amount exceeds maximum")
            return {"status": "success", "transaction_id": "tx_24680"}
        
        payment_gateway_mock.process_payment.side_effect = side_effect
        
        # Use the mock
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=payment_gateway_mock)
        
        # This should succeed
        result = payment_service.process_payment("customer_1", 500.00, "visa")
        assert result["success"] is True
        
        # This should fail
        with pytest.raises(ValueError) as excinfo:
            payment_service.process_payment("customer_1", 1500.00, "visa")
        assert "Amount exceeds maximum" in str(excinfo.value)
    
    def test_process_payment_with_fake(self):
        """Using a fake payment gateway"""
        # Create a fake payment gateway
        class FakePaymentGateway:
            def __init__(self):
                self.payments = []
            
            def process_payment(self, customer_id, amount, card_type, **kwargs):
                # Simulate some business logic
                if amount <= 0:
                    raise ValueError("Amount must be positive")
                
                # Record the payment
                payment = {
                    "customer_id": customer_id,
                    "amount": amount,
                    "card_type": card_type,
                    "transaction_id": f"fake_tx_{len(self.payments) + 1}"
                }
                self.payments.append(payment)
                
                # Return success response
                return {
                    "status": "success",
                    "transaction_id": payment["transaction_id"]
                }
        
        # Use the fake
        fake_gateway = FakePaymentGateway()
        from app.services.payment_service import PaymentService
        payment_service = PaymentService(payment_gateway=fake_gateway)
        
        # Process multiple payments
        result1 = payment_service.process_payment("customer_1", 100.00, "visa")
        result2 = payment_service.process_payment("customer_2", 200.00, "mastercard")
        
        # Verify results
        assert result1["transaction_id"] == "fake_tx_1"
        assert result2["transaction_id"] == "fake_tx_2"
        
        # Verify fake gateway state
        assert len(fake_gateway.payments) == 2
        assert fake_gateway.payments[0]["customer_id"] == "customer_1"
        assert fake_gateway.payments[1]["amount"] == 200.00</code></pre>
            </div>
        </section>

        <section class="functional_testing">
            <h2>Functional API Testing</h2>
            
            <p>Functional testing focuses on testing complete API endpoints against their requirements and specifications. It verifies that each endpoint correctly handles requests, processes data, and returns appropriate responses according to the API contract.</p>
            
            <p>Think of functional testing like test-driving a car—you're checking that all features work as advertised from the user's perspective, without necessarily being concerned with what's happening under the hood.</p>
            
            <div class="code-section">
                <h3>Functional Testing with pytest and requests</h3>
                <pre><code>import pytest
import requests
import json

# Base URL for the API
BASE_URL = "http://localhost:5000/api"

# Test data
TEST_USER = {
    "username": "functionaltest",
    "email": "functional@example.com",
    "password": "Password123!"
}

# Fixture for authentication token
@pytest.fixture
def auth_token():
    """Get authentication token for testing"""
    # Register a user if not exists
    try:
        register_response = requests.post(
            f"{BASE_URL}/users/register",
            json=TEST_USER
        )
        # If user already exists, just log in
        if register_response.status_code not in (201, 409):
            raise Exception(f"Failed to register test user: {register_response.text}")
    except Exception as e:
        print(f"Error during user registration: {str(e)}")
    
    # Login to get token
    login_response = requests.post(
        f"{BASE_URL}/auth/login",
        json={
            "username": TEST_USER["username"],
            "password": TEST_USER["password"]
        }
    )
    
    if login_response.status_code != 200:
        raise Exception(f"Failed to login: {login_response.text}")
    
    return login_response.json()["access_token"]

# Test user management endpoints
class TestUserAPI:
    def test_get_user_profile(self, auth_token):
        """Test getting user profile"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        response = requests.get(f"{BASE_URL}/users/profile", headers=headers)
        
        assert response.status_code == 200
        data = response.json()
        assert data["username"] == TEST_USER["username"]
        assert data["email"] == TEST_USER["email"]
    
    def test_get_user_profile_unauthorized(self):
        """Test getting user profile without token"""
        response = requests.get(f"{BASE_URL}/users/profile")
        
        assert response.status_code == 401
    
    def test_update_user_profile(self, auth_token):
        """Test updating user profile"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        new_data = {"bio": "Updated bio for functional tests"}
        
        response = requests.patch(
            f"{BASE_URL}/users/profile",
            headers=headers,
            json=new_data
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["bio"] == new_data["bio"]
        
        # Verify the update persisted
        verify_response = requests.get(f"{BASE_URL}/users/profile", headers=headers)
        assert verify_response.status_code == 200
        verify_data = verify_response.json()
        assert verify_data["bio"] == new_data["bio"]

# Test product API endpoints
class TestProductAPI:
    @pytest.fixture
    def product_id(self, auth_token):
        """Create a test product and return its ID"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        product_data = {
            "name": "Test Product",
            "description": "Product for functional tests",
            "price": 19.99,
            "category": "test"
        }
        
        response = requests.post(
            f"{BASE_URL}/products",
            headers=headers,
            json=product_data
        )
        
        assert response.status_code == 201
        product_id = response.json()["id"]
        
        yield product_id
        
        # Clean up - delete the product after tests
        delete_response = requests.delete(
            f"{BASE_URL}/products/{product_id}",
            headers=headers
        )
        assert delete_response.status_code in (200, 204, 404)
    
    def test_get_products(self):
        """Test getting product list"""
        response = requests.get(f"{BASE_URL}/products")
        
        assert response.status_code == 200
        data = response.json()
        assert isinstance(data, list)
    
    def test_get_product_by_id(self, product_id):
        """Test getting a specific product"""
        response = requests.get(f"{BASE_URL}/products/{product_id}")
        
        assert response.status_code == 200
        data = response.json()
        assert data["id"] == product_id
        assert data["name"] == "Test Product"
    
    def test_get_nonexistent_product(self):
        """Test getting a product that doesn't exist"""
        response = requests.get(f"{BASE_URL}/products/999999")
        
        assert response.status_code == 404
    
    def test_search_products(self, product_id):
        """Test searching for products"""
        # Search by name
        response = requests.get(f"{BASE_URL}/products?search=Test Product")
        
        assert response.status_code == 200
        data = response.json()
        assert len(data) > 0
        assert any(p["id"] == product_id for p in data)
        
        # Search by category
        response = requests.get(f"{BASE_URL}/products?category=test")
        
        assert response.status_code == 200
        data = response.json()
        assert len(data) > 0
        assert any(p["id"] == product_id for p in data)
    
    def test_update_product(self, auth_token, product_id):
        """Test updating a product"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        update_data = {
            "name": "Updated Test Product",
            "price": 29.99
        }
        
        response = requests.patch(
            f"{BASE_URL}/products/{product_id}",
            headers=headers,
            json=update_data
        )
        
        assert response.status_code == 200
        data = response.json()
        assert data["name"] == update_data["name"]
        assert float(data["price"]) == update_data["price"]

# Test order API endpoints
class TestOrderAPI:
    @pytest.fixture
    def product_for_order(self, auth_token):
        """Create a test product for ordering"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        product_data = {
            "name": "Order Test Product",
            "description": "Product for order tests",
            "price": 15.99,
            "category": "test",
            "stock": 10
        }
        
        response = requests.post(
            f"{BASE_URL}/products",
            headers=headers,
            json=product_data
        )
        
        assert response.status_code == 201
        return response.json()
    
    def test_create_order(self, auth_token, product_for_order):
        """Test creating an order"""
        headers = {"Authorization": f"Bearer {auth_token}"}
        
        # Create order
        order_data = {
            "items": [
                {
                    "product_id": product_for_order["id"],
                    "quantity": 2
                }
            ],
            "shipping_address": "123 Test St, Test City",
            "payment_method": "credit_card"
        }
        
        response = requests.post(
            f"{BASE_URL}/orders",
            headers=headers,
            json=order_data
        )
        
        assert response.status_code == 201
        data = response.json()
        assert "id" in data
        assert len(data["items"]) == 1
        assert data["items"][0]["product_id"] == product_for_order["id"]
        assert data["items"][0]["quantity"] == 2
        assert data["total"] == 2 * float(product_for_order["price"])
        
        # Verify product stock was reduced
        product_response = requests.get(f"{BASE_URL}/products/{product_for_order['id']}")
        assert product_response.status_code == 200
        updated_product = product_response.json()
        assert updated_product["stock"] == product_for_order["stock"] - 2
        
        # Get order by ID
        order_id = data["id"]
        get_response = requests.get(
            f"{BASE_URL}/orders/{order_id}",
            headers=headers
        )
        
        assert get_response.status_code == 200
        get_data = get_response.json()
        assert get_data["id"] == order_id</code></pre>

                <h3>Functional Testing with Postman/Newman</h3>
                <p>Postman provides a user-friendly interface for API testing and can be automated with Newman.</p>
                <p>Example Postman Collection (exported as JSON):</p>
                <pre><code>{
    "info": {
        "name": "API Functional Tests",
        "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json"
    },
    "item": [
        {
            "name": "Authentication",
            "item": [
                {
                    "name": "Register User",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"username\": \"postmanuser\",\n    \"email\": \"postman@example.com\",\n    \"password\": \"Password123!\"\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/users/register",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "users", "register"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "// Check if registration was successful or user already exists",
                                    "pm.test(\"Status code is 201 (Created) or 409 (Conflict)\", function () {",
                                    "    pm.expect(pm.response.code).to.be.oneOf([201, 409]);",
                                    "});",
                                    "",
                                    "if (pm.response.code === 201) {",
                                    "    const responseJson = pm.response.json();",
                                    "    ",
                                    "    pm.test(\"Response contains user info\", function () {",
                                    "        pm.expect(responseJson.username).to.equal(\"postmanuser\");",
                                    "        pm.expect(responseJson.email).to.equal(\"postman@example.com\");",
                                    "    });",
                                    "}"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Login",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"username\": \"postmanuser\",\n    \"password\": \"Password123!\"\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/auth/login",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "auth", "login"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 200\", function () {",
                                    "    pm.response.to.have.status(200);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Response contains access token\", function () {",
                                    "    pm.expect(responseJson.access_token).to.be.a('string');",
                                    "    pm.expect(responseJson.refresh_token).to.be.a('string');",
                                    "});",
                                    "",
                                    "// Store token for later use",
                                    "pm.environment.set(\"access_token\", responseJson.access_token);",
                                    "pm.environment.set(\"refresh_token\", responseJson.refresh_token);"
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Products",
            "item": [
                {
                    "name": "Create Product",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            },
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"name\": \"Postman Test Product\",\n    \"description\": \"Product created through Postman tests\",\n    \"price\": 24.99,\n    \"category\": \"test\",\n    \"stock\": 50\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/products",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 201\", function () {",
                                    "    pm.response.to.have.status(201);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Product was created with correct data\", function () {",
                                    "    pm.expect(responseJson.name).to.equal(\"Postman Test Product\");",
                                    "    pm.expect(responseJson.price).to.equal(24.99);",
                                    "    pm.expect(responseJson.stock).to.equal(50);",
                                    "});",
                                    "",
                                    "// Store product ID for later tests",
                                    "pm.environment.set(\"product_id\", responseJson.id);"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Get Product",
                    "request": {
                        "method": "GET",
                        "header": [],
                        "url": {
                            "raw": "{{baseUrl}}/api/products/{{product_id}}",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "{{product_id}}"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 200\", function () {",
                                    "    pm.response.to.have.status(200);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Product has correct data\", function () {",
                                    "    pm.expect(responseJson.id).to.equal(pm.environment.get(\"product_id\"));",
                                    "    pm.expect(responseJson.name).to.equal(\"Postman Test Product\");",
                                    "    pm.expect(responseJson.price).to.equal(24.99);",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Update Product",
                    "request": {
                        "method": "PATCH",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            },
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                            }
                        ],
                        "body": {
                            "mode": "raw",
                            "raw": "{\n    \"name\": \"Updated Postman Product\",\n    \"price\": 29.99\n}"
                        },
                        "url": {
                            "raw": "{{baseUrl}}/api/products/{{product_id}}",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "{{product_id}}"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 200\", function () {",
                                    "    pm.response.to.have.status(200);",
                                    "});",
                                    "",
                                    "const responseJson = pm.response.json();",
                                    "",
                                    "pm.test(\"Product was updated correctly\", function () {",
                                    "    pm.expect(responseJson.name).to.equal(\"Updated Postman Product\");",
                                    "    pm.expect(responseJson.price).to.equal(29.99);",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Delete Product",
                    "request": {
                        "method": "DELETE",
                        "header": [
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                            }
                        ],
                        "url": {
                            "raw": "{{baseUrl}}/api/products/{{product_id}}",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "products", "{{product_id}}"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 204\", function () {",
                                    "    pm.response.to.have.status(204);",
                                    "});",
                                    "",
                                    "// Verify product is deleted",
                                    "pm.sendRequest({",
                                    "    url: pm.environment.get(\"baseUrl\") + \"/api/products/\" + pm.environment.get(\"product_id\"),",
                                    "    method: 'GET'",
                                    "}, function (err, res) {",
                                    "    pm.test(\"Product should no longer exist\", function () {",
                                    "        pm.expect(res.code).to.equal(404);",
                                    "    });",
                                    "});"
                                ]
                            }
                        }
                    ]
                }
            ]
        },
        {
            "name": "Error Handling",
            "item": [
                {
                    "name": "Invalid Authentication",
                    "request": {
                        "method": "GET",
                        "header": [
                            {
                                "key": "Authorization",
                                "value": "Bearer invalid_token"
                            }
                        ],
                        "url": {
                            "raw": "{{baseUrl}}/api/users/profile",
                            "host": ["{{baseUrl}}"],
                            "path": ["api", "users", "profile"]
                        }
                    },
                    "event": [
                        {
                            "listen": "test",
                            "script": {
                                "exec": [
                                    "pm.test(\"Status code is 401 Unauthorized\", function () {",
                                    "    pm.response.to.have.status(401);",
                                    "});",
                                    "",
                                    "pm.test(\"Response has error message\", function () {",
                                    "    const responseJson = pm.response.json();",
                                    "    pm.expect(responseJson.error).to.be.a('string');",
                                    "});"
                                ]
                            }
                        }
                    ]
                },
                {
                    "name": "Invalid Input",
                    "request": {
                        "method": "POST",
                        "header": [
                            {
                                "key": "Content-Type",
                                "value": "application/json"
                            },
                            {
                                "key": "Authorization",
                                "value": "Bearer {{access_token}}"
                              }
                          ],
                          "body": {
                              "mode": "raw",
                              "raw": "{\n    \"username\": \"\",\n    \"email\": \"invalid-email\"\n}"
                          },
                          "url": {
                              "raw": "{{baseUrl}}/api/users",
                              "host": ["{{baseUrl}}"],
                              "path": ["api", "users"]
                          }
                      },
                      "event": [
                          {
                              "listen": "test",
                              "script": {
                                  "exec": [
                                      "pm.test(\"Status code is 400 Bad Request\", function () {",
                                      "    pm.response.to.have.status(400);",
                                      "});",
                                      "",
                                      "pm.test(\"Response contains validation errors\", function () {",
                                      "    const responseJson = pm.response.json();",
                                      "    pm.expect(responseJson.errors).to.be.an('object');",
                                      "    pm.expect(responseJson.errors.username).to.be.an('array');",
                                      "    pm.expect(responseJson.errors.email).to.be.an('array');",
                                      "});"
                                  ]
                              }
                          }
                      ]
                  },
                  {
                      "name": "Resource Not Found",
                      "request": {
                          "method": "GET",
                          "header": [],
                          "url": {
                              "raw": "{{baseUrl}}/api/products/99999",
                              "host": ["{{baseUrl}}"],
                              "path": ["api", "products", "99999"]
                          }
                      },
                      "event": [
                          {
                              "listen": "test",
                              "script": {
                                  "exec": [
                                      "pm.test(\"Status code is 404 Not Found\", function () {",
                                      "    pm.response.to.have.status(404);",
                                      "});",
                                      "",
                                      "pm.test(\"Response contains error message\", function () {",
                                      "    const responseJson = pm.response.json();",
                                      "    pm.expect(responseJson.error).to.be.a('string');",
                                      "});"
                                  ]
                              }
                          }
                      ]
                  }
              ]
          }
      ],
      "event": [
          {
              "listen": "prerequest",
              "script": {
                  "type": "text/javascript",
                  "exec": [""]
              }
          },
          {
              "listen": "test",
              "script": {
                  "type": "text/javascript",
                  "exec": [""]
              }
          }
      ],
      "variable": [
          {
              "key": "baseUrl",
              "value": "http://localhost:5000",
              "type": "string"
          }
      ]
  }</code></pre>
  
                  <h3>Running Newman from Command Line</h3>
                  <pre><code># Install Newman
  npm install -g newman
  
  # Run the collection with environment variables
  newman run api_tests.json -e environment.json
  
  # Generate HTML report
  newman run api_tests.json -e environment.json -r html
  
  # Run only a specific folder
  newman run api_tests.json -e environment.json --folder "Products"</code></pre>
              </div>
              
              <div class="best-practices">
                  <h3>Functional Testing Best Practices</h3>
                  <ul>
                      <li><strong>Test Against API Contract:</strong> Focus on testing endpoints against their documented specifications.</li>
                      <li><strong>Cover Main Response Codes:</strong> Test successful responses (2xx), client errors (4xx), and server errors (5xx).</li>
                      <li><strong>Test Content Types:</strong> Verify the API responds with the correct content types.</li>
                      <li><strong>Validate Response Structure:</strong> Check that responses conform to the expected schema.</li>
                      <li><strong>Test Pagination and Filtering:</strong> Verify pagination, sorting, and filtering work as expected.</li>
                      <li><strong>Check Response Headers:</strong> Validate important headers (e.g., cache control, content type).</li>
                      <li><strong>Test Authentication/Authorization:</strong> Verify proper handling of unauthenticated or unauthorized requests.</li>
                      <li><strong>Test Input Validation:</strong> Check how the API handles invalid or malformed inputs.</li>
                      <li><strong>Clean Up Test Data:</strong> Remove test data after tests to avoid polluting the environment.</li>
                  </ul>
              </div>
              
              <div class="tools-comparison">
                  <h3>Functional Testing Tools Comparison</h3>
                  <table>
                      <thead>
                          <tr>
                              <th>Tool</th>
                              <th>Strengths</th>
                              <th>Best For</th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td>pytest + requests</td>
                              <td>
                                  - Flexible and customizable<br>
                                  - Integrates with Python test infrastructure<br>
                                  - Programmatic control of tests
                              </td>
                              <td>Developer-centric testing, integration with CI/CD, complex test logic</td>
                          </tr>
                          <tr>
                              <td>Postman/Newman</td>
                              <td>
                                  - Visual interface for test creation<br>
                                  - Easy to learn and use<br>
                                  - Strong reporting features
                              </td>
                              <td>Mixed teams, API documentation, less technical testers</td>
                          </tr>
                          <tr>
                              <td>Tavern</td>
                              <td>
                                  - YAML-based tests<br>
                                  - Simple, concise syntax<br>
                                  - pytest integration
                              </td>
                              <td>Simple API testing, readable test cases</td>
                          </tr>
                          <tr>
                              <td>Karate DSL</td>
                              <td>
                                  - Combines API test automation, mocks, performance testing<br>
                                  - No programming required<br>
                                  - Strong reporting
                              </td>
                              <td>Comprehensive API testing, testing by QA teams</td>
                          </tr>
                          <tr>
                              <td>Robot Framework</td>
                              <td>
                                  - Keyword-driven testing<br>
                                  - Extensive library support<br>
                                  - Acceptance testing focus
                              </td>
                              <td>Teams already using Robot Framework, acceptance testing</td>
                          </tr>
                      </tbody>
                  </table>
              </div>
          </section>
  
          <section class="contract_testing">
              <h2>Contract Testing</h2>
              
              <p>Contract testing ensures that the API adheres to its specified contract or specification. It focuses on verifying that the API's request and response formats, status codes, and behaviors match what's documented, providing a guarantee to consumers that the API will work as expected.</p>
              
              <p>Think of contract testing like checking a legal document against a template—it ensures that all the required clauses are present and correctly formatted, even if it doesn't verify that the content makes sense in all contexts.</p>
              
              <div class="how-it-works">
                  <h3>How Contract Testing Works</h3>
                  <ol>
                      <li>Define the contract (API specification)</li>
                      <li>Generate tests from the contract</li>
                      <li>Run tests against the API implementation</li>
                      <li>Verify that the implementation conforms to the contract</li>
                  </ol>
                  
                  <p>Contract testing is particularly important in microservices architectures, where different teams may be responsible for different services that need to communicate with each other.</p>
              </div>
              
              <div class="openapi-testing">
                  <h3>Testing Against OpenAPI Specifications</h3>
                  <p>OpenAPI (formerly Swagger) is a common format for API specifications. Here's how to test your API against an OpenAPI spec:</p>
                  
                  <h4>Sample OpenAPI Specification (openapi.yml)</h4>
                  <pre><code>openapi: 3.0.0
  info:
    title: Product API
    version: 1.0.0
    description: API for managing products
  paths:
    /api/products:
      get:
        summary: Get all products
        responses:
          '200':
            description: Successful operation
            content:
              application/json:
                schema:
                  type: array
                  items:
                    $ref: '#/components/schemas/Product'
      post:
        summary: Create a new product
        requestBody:
          required: true
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ProductInput'
        responses:
          '201':
            description: Product created
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/Product'
          '400':
            description: Invalid input
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/Error'
    /api/products/{id}:
      get:
        summary: Get a product by ID
        parameters:
          - name: id
            in: path
            required: true
            schema:
              type: integer
        responses:
          '200':
            description: Successful operation
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/Product'
          '404':
            description: Product not found
            content:
              application/json:
                schema:
                  $ref: '#/components/schemas/Error'
  components:
    schemas:
      Product:
        type: object
        properties:
          id:
            type: integer
          name:
            type: string
          description:
            type: string
          price:
            type: number
            format: float
          category:
            type: string
          created_at:
            type: string
            format: date-time
        required:
          - id
          - name
          - price
      ProductInput:
        type: object
        properties:
          name:
            type: string
            minLength: 3
            maxLength: 100
          description:
            type: string
          price:
            type: number
            format: float
            minimum: 0.01
          category:
            type: string
        required:
          - name
          - price
      Error:
        type: object
        properties:
          error:
            type: string
          details:
            type: object</code></pre>
            <h4>Testing with OpenAPI-based Tools</h4>
            <pre><code>import pytest
from openapi_spec_validator import validate_spec
import yaml
import json
import os
import requests

# Base URL for the API
BASE_URL = "http://localhost:5000"

# Load OpenAPI specification
@pytest.fixture
def openapi_spec():
"""Load the OpenAPI specification"""
with open('openapi.yml', 'r') as f:
    return yaml.safe_load(f)

# Validate OpenAPI specification
def test_openapi_spec_is_valid(openapi_spec):
"""Test that our OpenAPI spec is valid"""
validate_spec(openapi_spec)

# Test API endpoints against OpenAPI spec
class TestAPIContract:
def test_get_products_matches_contract(self, openapi_spec):
    """Test that GET /api/products matches the OpenAPI spec"""
    # Get the expected schema from the spec
    expected_schema = openapi_spec['paths']['/api/products']['get']['responses']['200']['content']['application/json']['schema']
    
    # Call the API
    response = requests.get(f"{BASE_URL}/api/products")
    
    # Check status code
    assert response.status_code == 200
    
    # Check content type
    assert 'application/json' in response.headers['Content-Type']
    
    # Parse response JSON
    response_data = response.json()
    
    # Validate response structure against schema
    assert isinstance(response_data, list), "Response should be an array"
    
    if response_data:  # If there are products, check the first one
        product = response_data[0]
        
        # Check required fields
        required_fields = expected_schema['items']['required']
        for field in required_fields:
            assert field in product, f"Required field '{field}' missing from response"
        
        # Check field types
        properties = expected_schema['items']['properties']
        for field, field_schema in properties.items():
            if field in product:
                if field_schema['type'] == 'integer':
                    assert isinstance(product[field], int), f"Field '{field}' should be an integer"
                elif field_schema['type'] == 'number':
                    assert isinstance(product[field], (int, float)), f"Field '{field}' should be a number"
                elif field_schema['type'] == 'string':
                    assert isinstance(product[field], str), f"Field '{field}' should be a string"
                elif field_schema['type'] == 'boolean':
                    assert isinstance(product[field], bool), f"Field '{field}' should be a boolean"

def test_create_product_matches_contract(self, openapi_spec):
    """Test that POST /api/products matches the OpenAPI spec"""
    # Get auth token (assuming authentication is required)
    auth_response = requests.post(
        f"{BASE_URL}/api/auth/login",
        json={
            "username": "testuser",
            "password": "Password123!"
        }
    )
    token = auth_response.json()["access_token"]
    
    # Create test product data from the input schema
    input_schema = openapi_spec['components']['schemas']['ProductInput']
    test_product = {
        "name": "Contract Test Product",
        "description": "Testing against OpenAPI contract",
        "price": 19.99,
        "category": "test"
    }
    
    # Validate test data against input schema
    for field in input_schema['required']:
        assert field in test_product, f"Required field '{field}' missing from test data"
    
    # Call the API
    response = requests.post(
        f"{BASE_URL}/api/products",
        json=test_product,
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # Check status code
    assert response.status_code == 201, f"Expected 201 Created, got {response.status_code}"
    
    # Check content type
    assert 'application/json' in response.headers['Content-Type']
    
    # Parse response JSON
    product = response.json()
    
    # Get the expected schema from the spec
    expected_schema = openapi_spec['components']['schemas']['Product']
    
    # Check required fields
    for field in expected_schema['required']:
        assert field in product, f"Required field '{field}' missing from response"
    
    # Clean up - delete the created product
    product_id = product['id']
    requests.delete(
        f"{BASE_URL}/api/products/{product_id}",
        headers={"Authorization": f"Bearer {token}"}
    )</code></pre>
        </div>
        
        <div class="pact-testing">
            <h3>Consumer-Driven Contract Testing with Pact</h3>
            <p>Pact is a contract testing tool that enables consumer-driven contract testing, where service consumers define the expectations for the providers they interact with.</p>
            
            <h4>Example: Consumer (Frontend) Pact</h4>
            <pre><code>from pact import Consumer, Provider
import pytest
import requests
import os
import atexit

# Configure Pact
pact = Consumer('ProductConsumer').has_pact_with(Provider('ProductService'))
pact.start_service()

# Register to stop service when tests finish
atexit.register(pact.stop_service)

def test_get_products():
"""Test the contract for getting products"""
# Define the expected interaction
(pact
 .given('products exist')
 .upon_receiving('a request for all products')
 .with_request('get', '/api/products')
 .will_respond_with(200, body=[
     {
         'id': 1,
         'name': 'Test Product',
         'price': 29.99,
         'category': 'test'
     }
 ]))

# Execute the test
with pact:
    # Make the request to the mock server
    url = f"{pact.uri}/api/products"
    response = requests.get(url)
    
    # Check the response
    assert response.status_code == 200
    body = response.json()
    assert isinstance(body, list)
    assert len(body) > 0
    assert 'id' in body[0]
    assert 'name' in body[0]
    assert 'price' in body[0]

def test_get_product_by_id():
"""Test the contract for getting a product by ID"""
# Define the expected interaction
(pact
 .given('a product with ID 1 exists')
 .upon_receiving('a request for a product with ID 1')
 .with_request('get', '/api/products/1')
 .will_respond_with(200, body={
     'id': 1,
     'name': 'Test Product',
     'description': 'Test Description',
     'price': 29.99,
     'category': 'test'
 }))

# Execute the test
with pact:
    # Make the request to the mock server
    url = f"{pact.uri}/api/products/1"
    response = requests.get(url)
    
    # Check the response
    assert response.status_code == 200
    body = response.json()
    assert body['id'] == 1
    assert body['name'] == 'Test Product'
    assert body['price'] == 29.99

def test_create_product():
"""Test the contract for creating a product"""
# Define the expected interaction
(pact
 .given('authenticated user')
 .upon_receiving('a request to create a product')
 .with_request(
     'post',
     '/api/products',
     headers={'Authorization': 'Bearer valid-token', 'Content-Type': 'application/json'},
     body={
         'name': 'New Product',
         'description': 'Product Description',
         'price': 19.99,
         'category': 'test'
     }
 )
 .will_respond_with(
     201,
     body={
         'id': 123,
         'name': 'New Product',
         'description': 'Product Description',
         'price': 19.99,
         'category': 'test',
         'created_at': '2023-06-01T12:00:00Z'
     }
 ))

# Execute the test
with pact:
    # Make the request to the mock server
    url = f"{pact.uri}/api/products"
    response = requests.post(
        url,
        json={
            'name': 'New Product',
            'description': 'Product Description',
            'price': 19.99,
            'category': 'test'
        },
        headers={
            'Authorization': 'Bearer valid-token',
            'Content-Type': 'application/json'
        }
    )
    
    # Check the response
    assert response.status_code == 201
    body = response.json()
    assert body['name'] == 'New Product'
    assert body['price'] == 19.99</code></pre>
            
            <h4>Example: Provider (Backend) Verification</h4>
            <pre><code>from pact import Verifier
import os
import pytest
import subprocess
import time
import signal

# Setup Flask app process for testing
@pytest.fixture(scope='session')
def api_process():
"""Start the Flask API for testing"""
# Set test environment
env = os.environ.copy()
env['FLASK_ENV'] = 'test'
env['DATABASE_URL'] = 'sqlite:///test.db'

# Start the API process
process = subprocess.Popen(
    ['python', 'app.py'],
    env=env,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)

# Wait for the API to start
time.sleep(2)

yield process

# Terminate the process after tests
process.terminate()
process.wait()

def test_product_service_provider_contracts(api_process):
"""Verify that the API fulfills its contracts"""
verifier = Verifier()

# Path to the Pact file generated by the consumer
pact_file = './pacts/productconsumer-productservice.json'

# URL of the API
provider_base_url = 'http://localhost:5000'

# Verify provider against consumer expectations
output, success = verifier.verify_pacts(
    pact_file,
    provider='ProductService',
    provider_base_url=provider_base_url,
    provider_states_setup_url=f"{provider_base_url}/test/setup",
    verbose=True
)

assert success, f"Pact verification failed: {output}"</code></pre>
        </div>
        
        <div class="best-practices">
            <h3>Contract Testing Best Practices</h3>
            <ul>
                <li><strong>Define Contracts Early:</strong> Create API specifications before implementation.</li>
                <li><strong>Consumer-Driven Contracts:</strong> Let consumers define what they need from providers.</li>
                <li><strong>Keep Contracts Minimal:</strong> Include only what's necessary to avoid over-constraining the implementation.</li>
                <li><strong>Version Contracts:</strong> Maintain backward compatibility or version contracts when they change.</li>
                <li><strong>Include in CI/CD:</strong> Run contract tests as part of your continuous integration pipeline.</li>
                <li><strong>Use Contract for Documentation:</strong> Generate API documentation from the contract specification.</li>
                <li><strong>Test Both Ways:</strong> Verify that providers meet the contract and that consumers use it correctly.</li>
            </ul>
        </div>
    </section>

    <section class="security_testing">
        <h2>Security Testing</h2>
        
        <p>Security testing is essential for ensuring that your API is protected against common vulnerabilities and threats. It identifies weaknesses that could be exploited by attackers to gain unauthorized access, extract sensitive data, or disrupt service.</p>
        
        <p>Think of security testing as performing a vulnerability assessment on your house—checking all doors, windows, and other entry points to ensure they're properly secured against potential intruders.</p>
        
        <div class="owasp-top10">
            <h3>OWASP API Security Top 10</h3>
            <p>The Open Web Application Security Project (OWASP) identifies the top 10 API security risks:</p>
            
            <ol>
                <li><strong>Broken Object Level Authorization:</strong> APIs don't properly check if the user has permission to access specific objects.</li>
                <li><strong>Broken Authentication:</strong> Authentication mechanisms are implemented incorrectly.</li>
                <li><strong>Excessive Data Exposure:</strong> APIs return more data than necessary.</li>
                <li><strong>Lack of Resources & Rate Limiting:</strong> APIs don't limit the number of requests, leading to DoS attacks.</li>
                <li><strong>Broken Function Level Authorization:</strong> APIs don't properly restrict user access to functions.</li>
                <li><strong>Mass Assignment:</strong> Client-provided data is directly bound to data models without proper filtering.</li>
                <li><strong>Security Misconfiguration:</strong> Insecure default configurations, open cloud storage, verbose error messages.</li>
                <li><strong>Injection:</strong> Untrusted data is included in queries without proper sanitization.</li>
                <li><strong>Improper Assets Management:</strong> Old API versions remain unpatched or expose sensitive data.</li>
                <li><strong>Insufficient Logging & Monitoring:</strong> Lack of logging and monitoring prevents detection of attacks.</li>
            </ol>
        </div>
        
        <div class="code-section">
            <h3>Basic Security Testing with Python</h3>
            <pre><code>import pytest
import requests
import json
import re
import time

# Base URL for the API
BASE_URL = "http://localhost:5000/api"

# Test data
TEST_USER = {
"username": "securitytest",
"email": "security@example.com",
"password": "SecureP@ssw0rd"
}

# Fixture for authentication token
@pytest.fixture
def auth_token():
"""Get authentication token for testing"""
# Register a user if not exists (similar to previous examples)
# ...

# Login to get token
login_response = requests.post(
    f"{BASE_URL}/auth/login",
    json={
        "username": TEST_USER["username"],
        "password": TEST_USER["password"]
    }
)

return login_response.json()["access_token"]

class TestAPISecurity:
def test_sql_injection_prevention(self):
    """Test SQL injection prevention in login endpoint"""
    # Attempt SQL injection in login
    injection_payloads = [
        {"username": "' OR '1'='1", "password": "password"},
        {"username": "admin' --", "password": "anything"},
        {"username": "admin'; DROP TABLE users; --", "password": "anything"}
    ]
    
    for payload in injection_payloads:
        response = requests.post(f"{BASE_URL}/auth/login", json=payload)
        assert response.status_code == 401, f"SQL Injection may be possible with payload: {payload}"

def test_brute_force_protection(self):
    """Test brute force protection in login endpoint"""
    # Attempt multiple failed logins
    for i in range(10):
        response = requests.post(
            f"{BASE_URL}/auth/login",
            json={
                "username": TEST_USER["username"],
                "password": f"wrong_password_{i}"
            }
        )
    
    # Check if account is locked or rate limited
    response = requests.post(
        f"{BASE_URL}/auth/login",
        json={
            "username": TEST_USER["username"],
            "password": TEST_USER["password"]
        }
    )
    
    # API should either rate limit (429) or temporarily lock account (403)
    assert response.status_code in (403, 429), "API may be vulnerable to brute force attacks"

def test_jwt_without_verification(self, auth_token):
    """Test if API accepts tampered JWTs"""
    # Create a tampered token by changing the payload without re-signing
    token_parts = auth_token.split('.')
    if len(token_parts) != 3:
        pytest.skip("Token is not a standard JWT")
    
    # Decode the payload
    import base64
    payload = json.loads(base64.b64decode(token_parts[1] + '==').decode('utf-8'))
    
    # Modify the payload (e.g., change user role to admin)
    payload['role'] = 'admin'
    
    # Re-encode the payload
    modified_payload = base64.b64encode(json.dumps(payload).encode('utf-8')).decode('utf-8').rstrip('=')
    
    # Create tampered token
    tampered_token = f"{token_parts[0]}.{modified_payload}.{token_parts[2]}"
    
    # Try to access protected endpoint with tampered token
    response = requests.get(
        f"{BASE_URL}/users/profile",
        headers={"Authorization": f"Bearer {tampered_token}"}
    )
    
    # Should be rejected with 401 Unauthorized
    assert response.status_code == 401, "API may accept tampered JWTs"

def test_sensitive_data_exposure(self):
    """Test for sensitive data exposure in error messages"""
    # Try to trigger errors that might leak sensitive information
    test_cases = [
        # Invalid JSON
        lambda: requests.post(f"{BASE_URL}/auth/login", data="invalid json"),
        # Invalid route
        lambda: requests.get(f"{BASE_URL}/nonexistent_route"),
        # Invalid HTTP method
        lambda: requests.delete(f"{BASE_URL}/auth/login")
    ]
    
    sensitive_patterns = [
        r'(Exception:)',
        r'(stack trace)',
        r'(at [\w\.]+\(\))',  # Stack trace function calls
        r'(\/[\w\/\.]+\.py)',  # File paths
        r'(database|sql|query)',  # Database info
        r'(password|secret|key)',  # Sensitive fields
        r'(config)',  # Configuration info
    ]
    
    for test_case in test_cases:
        response = test_case()
        
        # Check status code (should be 4xx, not 5xx for invalid input)
        assert response.status_code < 500, "Server error on invalid input"
        
        # Check for sensitive information in response
        for pattern in sensitive_patterns:
            match = re.search(pattern, response.text, re.IGNORECASE)
            assert not match, f"Possible sensitive data leak: {match.group(0)}"

def test_content_security_headers(self):
    """Test for security-related HTTP headers"""
    response = requests.get(f"{BASE_URL}/users")
    
    # Security headers that should be present
    security_headers = {
        'X-Content-Type-Options': 'nosniff',
        'X-Frame-Options': ['DENY', 'SAMEORIGIN'],
        'Content-Security-Policy': None,  # Just check existence
        'Strict-Transport-Security': None,
        'X-XSS-Protection': '1; mode=block'
    }
    
    for header, expected_value in security_headers.items():
        assert header in response.headers, f"Missing security header: {header}"
        
        if expected_value:
            if isinstance(expected_value, list):
                assert response.headers[header] in expected_value, f"Invalid value for {header}"
            else:
                assert response.headers[header] == expected_value, f"Invalid value for {header}"

def test_rate_limiting(self):
    """Test if API implements rate limiting"""
    # Make multiple requests in quick succession
    start_time = time.time()
    responses = []
    
    for i in range(50):
        response = requests.get(f"{BASE_URL}/products")
        responses.append(response)
        
        # If we get rate limited, the test passes
        if response.status_code == 429:
            break
    
    # Check if we were rate limited
    rate_limited = any(r.status_code == 429 for r in responses)
    
    # Alternative check: Look for rate limit headers
    rate_limit_headers = any(
        'X-RateLimit-Limit' in r.headers or
        'RateLimit-Limit' in r.headers or
        'Retry-After' in r.headers
        for r in responses
    )
    
    assert rate_limited or rate_limit_headers, "API may not implement rate limiting"

def test_cors_misconfiguration(self):
    """Test for CORS misconfiguration"""
    # Send OPTIONS request with suspicious origin
    headers = {
        'Origin': 'https://malicious-site.com',
        'Access-Control-Request-Method': 'POST',
        'Access-Control-Request-Headers': 'Content-Type, Authorization'
    }
    
    response = requests.options(f"{BASE_URL}/users", headers=headers)
    
    # Check if API allows this origin
    if 'Access-Control-Allow-Origin' in response.headers:
        allowed_origin = response.headers['Access-Control-Allow-Origin']
        assert allowed_origin != '*', "CORS allows all origins"
        assert allowed_origin != 'https://malicious-site.com', "CORS allows suspicious origin"</code></pre>

            <h3>Using OWASP ZAP for Automated Security Testing</h3>
            <pre><code>from zapv2 import ZAPv2
import time
import json

# API key from ZAP
apiKey = 'your-api-key'

# Target API
target = 'http://localhost:5000/api'

# Initialize ZAP
zap = ZAPv2(apikey=apiKey, proxies={'http': 'http://localhost:8080', 'https': 'http://localhost:8080'})

def test_api_with_zap():
"""Run a ZAP scan against the API"""
# Set up a new session
print('Creating new ZAP session...')
zap.core.new_session(name='API Security Test', overwrite=True)

# Define the context
contextId = zap.context.new_context(contextname='API Context')
zap.context.include_in_context('API Context', f'^{target}.*$')

# Configure authentication
auth_url = f'{target}/auth/login'
auth_data = json.dumps({
    'username': 'testuser',
    'password': 'Password123!'
})

print('Setting up authentication...')
zap.authentication.set_authentication_method(
    contextid=contextId,
    authenticationmethod='jsonBasedAuthentication',
    authenticationmethodconfigparams=f'loginUrl={auth_url}&loginRequestData={auth_data}'
)

# Set up auth token handling
print('Setting up authorization header handling...')
zap.replacer.add_rule(
    description='Add Auth Token',
    enabled=True,
    matchtype='REQ_HEADER',
    matchstring='Authorization',
    matchregex=False,
    replacement='Bearer {{token}}'  # Token will be captured and used
)

# Spider the API (discovery)
print('Spidering the API...')
scanid = zap.spider.scan(target, recurse=True)

# Wait for the spider to complete
while int(zap.spider.status(scanid)) < 100:
    print(f'Spider progress: {zap.spider.status(scanid)}%')
    time.sleep(5)

# Perform active scan
print('Running active scan...')
scan_id = zap.ascan.scan(target)

# Wait for the scan to complete
while int(zap.ascan.status(scan_id)) < 100:
    print(f'Scan progress: {zap.ascan.status(scan_id)}%')
    time.sleep(5)

# Get the alerts
print('Generating report...')
alerts = zap.core.alerts(baseurl=target)

# Print the alerts
for alert in alerts:
    print(f"Alert: {alert['name']}")
    print(f"Risk: {alert['risk']}")
    print(f"URL: {alert['url']}")
    print(f"Parameter: {alert['param']}")
    print(f"Description: {alert['description']}")
    print(f"Solution: {alert['solution']}")
    print("---")

# Generate an HTML report
report_path = 'zap_report.html'
with open(report_path, 'w') as f:
    f.write(zap.core.htmlreport())

print(f'Report saved to {report_path}')

# Check for high-risk issues
high_risks = [a for a in alerts if a['risk'] == 'High']
assert len(high_risks) == 0, f"Found {len(high_risks)} high-risk security issues"

if __name__ == '__main__':
test_api_with_zap()</code></pre>
        </div>
        
        <div class="security-tools">
            <h3>API Security Testing Tools</h3>
            <table>
                <thead>
                    <tr>
                        <th>Tool</th>
                        <th>Type</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>OWASP ZAP</td>
                        <td>Dynamic Application Security Testing (DAST)</td>
                        <td>Automated vulnerability scanning, active and passive scanning</td>
                    </tr>
                    <tr>
                        <td>Burp Suite</td>
                        <td>Web Proxy and Scanner</td>
                        <td>Manual and automated security testing, intercepting and modifying requests</td>
                    </tr>
                    <tr>
                        <td>SQLmap</td>
                        <td>SQL Injection Testing</td>
                        <td>Automated detection and exploitation of SQL injection vulnerabilities</td>
                    </tr>
                    <tr>
                        <td>Bandit</td>
                        <td>Static Application Security Testing (SAST)</td>
                        <td>Python-specific static code analysis to find security issues</td>
                    </tr>
                    <tr>
                        <td>JWT_Tool</td>
                        <td>JWT Security Testing</td>
                        <td>Testing JWT implementations for vulnerabilities</td>
                    </tr>
                    <tr>
                        <td>OWASP Dependency-Check</td>
                        <td>Dependency Scanner</td>
                        <td>Identifying known vulnerabilities in project dependencies</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="best-practices">
            <h3>API Security Testing Best Practices</h3>
            <ul>
                <li><strong>Shift Left:</strong> Integrate security testing early in the development process.</li>
                <li><strong>Combine Testing Types:</strong> Use both SAST (static) and DAST (dynamic) testing tools.</li>
                <li><strong>Automate Security Tests:</strong> Include security scans in your CI/CD pipeline.</li>
                <li><strong>Test Authentication:</strong> Thoroughly test all authentication mechanisms.</li>
                <li><strong>Verify Access Controls:</strong> Test both vertical (role-based) and horizontal (object-level) access controls.</li>
                <li><strong>Test Input Validation:</strong> Check for injection vulnerabilities with various input types.</li>
                <li><strong>Check Error Handling:</strong> Ensure errors don't leak sensitive information.</li>
                <li><strong>Review Security Headers:</strong> Verify all necessary security headers are present.</li>
                <li><strong>Test Rate Limiting:</strong> Verify protection against brute force and DoS attacks.</li>
                <li><strong>Regular Dependency Reviews:</strong> Check for vulnerabilities in dependencies regularly.</li>
            </ul>
        </div>
    </section>

    <section class="performance_testing">
        <h2>Performance Testing</h2>
        
        <p>Performance testing ensures that your API can handle expected loads, responds within acceptable timeframes, and uses resources efficiently. It helps identify bottlenecks, capacity limits, and areas for optimization before they impact users in production.</p>
        
        <p>Think of performance testing like stress-testing a bridge—you want to know how much weight it can bear and where it might fail under pressure before it's put into actual use.</p>
        
        <div class="types-of-performance-tests">
            <h3>Types of Performance Tests</h3>
            <ul>
                <li><strong>Load Testing:</strong> Testing how the system performs under expected load</li>
                <li><strong>Stress Testing:</strong> Testing system behavior beyond normal or peak load</li>
                <li><strong>Endurance Testing:</strong> Testing system behavior under sustained load over time</li>
                <li><strong>Spike Testing:</strong> Testing system response to sudden large spikes in load</li>
                <li><strong>Volume Testing:</strong> Testing with large amounts of data</li>
                <li><strong>Scalability Testing:</strong> Testing how the system scales with increasing load</li>
            </ul>
        </div>
        
        <div class="code-section">
            <h3>Load Testing with Locust</h3>
            <pre><code>from locust import HttpUser, task, between
import json
import random

class APIUser(HttpUser):
# Wait between 1 and 5 seconds between tasks
wait_time = between(1, 5)

def on_start(self):
    """Setup before starting tests"""
    # Login to get authentication token
    response = self.client.post(
        "/api/auth/login",
        json={
            "username": "loadtest",
            "password": "Password123!"
        }
    )
    
    # Check if login was successful
    if response.status_code == 200:
        data = response.json()
        self.token = data.get("access_token")
        self.auth_headers = {"Authorization": f"Bearer {self.token}"}
    else:
        # If login fails, try to register first
        self.client.post(
            "/api/users/register",
            json={
                "username": "loadtest",
                "email": "loadtest@example.com",
                "password": "Password123!"
            }
        )
        
        # Try login again
        response = self.client.post(
            "/api/auth/login",
            json={
                "username": "loadtest",
                "password": "Password123!"
            }
        )
        
        data = response.json()
        self.token = data.get("access_token")
        self.auth_headers = {"Authorization": f"Bearer {self.token}"}

@task(10)  # Higher weight = more frequent
def get_products(self):
    """Test getting products list"""
    # Add query parameters randomly to simulate different user behaviors
    params = {}
    
    # Sometimes add category filter
    if random.random() < 0.3:
        categories = ["electronics", "clothing", "books", "food"]
        params["category"] = random.choice(categories)
    
    # Sometimes add search term
    if random.random() < 0.2:
        search_terms = ["phone", "laptop", "book", "shirt"]
        params["search"] = random.choice(search_terms)
    
    # Sometimes add pagination
    if random.random() < 0.5:
        params["page"] = random.randint(1, 5)
        params["per_page"] = random.choice([10, 20, 50])
    
    # Make the request
    self.client.get("/api/products", params=params)

@task(5)
def get_product_details(self):
    """Test getting details for a specific product"""
    # Get a random product ID between 1 and 20
    # In a real test, you might want to get actual IDs from the API
    product_id = random.randint(1, 20)
    self.client.get(f"/api/products/{product_id}")

@task(2)
def create_product(self):
    """Test creating a new product"""
    product_data = {
        "name": f"Load Test Product {random.randint(1000, 9999)}",
        "description": "Product created during load testing",
        "price": round(random.uniform(9.99, 99.99), 2),
        "category": random.choice(["test", "load", "performance"]),
        "stock": random.randint(1, 100)
    }
    
    self.client.post(
        "/api/products",
        json=product_data,
        headers=self.auth_headers
    )

@task(1)
def place_order(self):
    """Test placing an order"""
    # First, get available products
    response = self.client.get("/api/products")
    if response.status_code != 200:
        return
    
    products = response.json()
    if not products:
        return
    
    # Select 1-3 random products
    num_items = random.randint(1, 3)
    selected_products = random.sample(products, min(num_items, len(products)))
    
    # Create order items
    items = []
    for product in selected_products:
        items.append({
            "product_id": product["id"],
            "quantity": random.randint(1, 5)
        })
    
    # Place order
    order_data = {
        "items": items,
        "shipping_address": "123 Load Test St",
        "payment_method": random.choice(["credit_card", "paypal", "bank_transfer"])
    }
    
    self.client.post(
        "/api/orders",
        json=order_data,
        headers=self.auth_headers
    )</code></pre>

            <h3>Running and Analyzing Locust Tests</h3>
            <pre><code># Run from command line
locust -f locustfile.py --host=http://localhost:5000

# Or run programmatically
import subprocess
import time
import json
import matplotlib.pyplot as plt
import pandas as pd

def run_locust_test(users, spawn_rate, run_time):
"""Run a Locust test with specific parameters"""
# Start Locust in headless mode
process = subprocess.Popen([
    "locust",
    "-f", "locustfile.py",
    "--host=http://localhost:5000",
    "--headless",
    "-u", str(users),
    "-r", str(spawn_rate),
    "--run-time", run_time,
    "--csv=results"
], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

# Wait for process to complete
stdout, stderr = process.communicate()

# Check if there were any errors
if process.returncode != 0:
    print(f"Error running Locust: {stderr.decode()}")
    return None

# Load and return results
results = {
    "requests": pd.read_csv("results_stats.csv"),
    "failures": pd.read_csv("results_failures.csv"),
    "exceptions": pd.read_csv("results_exceptions.csv")
}

return results

def analyze_results(results):
"""Analyze Locust test results"""
if not results:
    return

requests = results["requests"]

# Summary statistics
print("\nPerformance Test Results:")
print("-" * 50)
print(f"Total Requests: {requests['Total'].sum()}")
print(f"Failed Requests: {requests['Fails'].sum()}")
print(f"Median Response Time: {requests['Median Response Time'].median():.2f} ms")
print(f"95th Percentile: {requests['95%'].mean():.2f} ms")
print(f"Requests/sec: {requests['Requests/s'].mean():.2f}")

# Endpoint performance
endpoint_stats = requests.groupby("Name").agg({
    "Median Response Time": "mean",
    "95%": "mean",
    "Requests/s": "mean",
    "Fails": "sum",
    "Total": "sum"
}).sort_values("Median Response Time", ascending=False)

print("\nEndpoint Performance:")
print("-" * 50)
print(endpoint_stats)

# Check for failures
failures = results["failures"]
if len(failures) > 0:
    print("\nFailures:")
    print("-" * 50)
    print(failures)

# Plot response times
plt.figure(figsize=(10, 6))
plt.bar(requests["Name"], requests["Median Response Time"])
plt.xticks(rotation=45, ha="right")
plt.ylabel("Median Response Time (ms)")
plt.title("Endpoint Response Times")
plt.tight_layout()
plt.savefig("response_times.png")

# Plot throughput
plt.figure(figsize=(10, 6))
plt.bar(requests["Name"], requests["Requests/s"])
plt.xticks(rotation=45, ha="right")
plt.ylabel("Requests per Second")
plt.title("Endpoint Throughput")
plt.tight_layout()
plt.savefig("throughput.png")

# Run with different user loads
test_configs = [
{"users": 10, "spawn_rate": 1, "run_time": "1m"},
{"users": 50, "spawn_rate": 5, "run_time": "1m"},
{"users": 100, "spawn_rate": 10, "run_time": "1m"},
{"users": 200, "spawn_rate": 20, "run_time": "1m"}
]

results_by_load = {}

for config in test_configs:
print(f"\nRunning test with {config['users']} users...")
results = run_locust_test(config["users"], config["spawn_rate"], config["run_time"])
results_by_load[config["users"]] = results
analyze_results(results)

# Wait between tests
time.sleep(30)

# Compare results across different loads
user_counts = list(results_by_load.keys())
median_response_times = [results_by_load[u]["requests"]["Median Response Time"].mean() for u in user_counts]
request_rates = [results_by_load[u]["requests"]["Requests/s"].mean() for u in user_counts]

plt.figure(figsize=(10, 6))
plt.plot(user_counts, median_response_times, 'o-')
plt.xlabel("Number of Users")
plt.ylabel("Median Response Time (ms)")
plt.title("Scalability: Response Time vs Load")
plt.grid(True)
plt.savefig("scalability_response_time.png")

plt.figure(figsize=(10, 6))
plt.plot(user_counts, request_rates, 'o-')
plt.xlabel("Number of Users")
plt.ylabel("Requests per Second")
plt.title("Scalability: Throughput vs Load")
plt.grid(True)
plt.savefig("scalability_throughput.png")</code></pre>
        </div>
        
        <div class="best-practices">
            <h3>Performance Testing Best Practices</h3>
            <ul>
                <li><strong>Define Clear Metrics:</strong> Set specific KPIs like response time, throughput, and error rate.</li>
                <li><strong>Test Realistic Scenarios:</strong> Create test cases that mimic real user behavior.</li>
                <li><strong>Establish Baselines:</strong> Measure performance under normal conditions as a benchmark.</li>
                <li><strong>Isolate Test Environment:</strong> Test in an environment that mirrors production but is isolated.</li>
                <li><strong>Monitor Resource Usage:</strong> Track CPU, memory, database connections, and network usage.</li>
                <li><strong>Test Incrementally:</strong> Start with a small load and gradually increase.</li>
                <li><strong>Test Regularly:</strong> Run performance tests after significant changes and before releases.</li>
                <li><strong>Automate Performance Tests:</strong> Include them in your CI/CD pipeline when possible.</li>
                <li><strong>Analyze Bottlenecks:</strong> Determine if issues are in code, database, network, or infrastructure.</li>
                <li><strong>Focus on Key Endpoints:</strong> Prioritize testing for critical and resource-intensive endpoints.</li>
            </ul>
        </div>
        
        <div class="performance-tools">
            <h3>API Performance Testing Tools</h3>
            <table>
                <thead>
                    <tr>
                        <th>Tool</th>
                        <th>Type</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Locust</td>
                        <td>Python-based load testing</td>
                        <td>Developer-friendly, Python scripters, realistic user scenarios</td>
                    </tr>
                    <tr>
                        <td>JMeter</td>
                        <td>Java-based load testing</td>
                        <td>Comprehensive testing, GUI-based test creation, complex scenarios</td>
                    </tr>
                    <tr>
                        <td>Artillery</td>
                        <td>Node.js-based load testing</td>
                        <td>JavaScript developers, microservices, cloud-native applications</td>
                    </tr>
                    <tr>
                        <td>Gatling</td>
                        <td>Scala-based load testing</td>
                        <td>High-performance testing, continuous load testing, detailed metrics</td>
                    </tr>
                    <tr>
                        <td>k6</td>
                        <td>Modern load testing</td>
                        <td>Developer-centric testing, CI/CD integration, cloud testing</td>
                    </tr>
                    <tr>
                        <td>Taurus</td>
                        <td>Test automation framework</td>
                        <td>Simplifying complex testing tools, CI/CD integration, YAML-based config</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </section>

    <section class="ci_cd_integration">
        <h2>Integrating Tests into CI/CD Pipelines</h2>
        
        <p>Integrating your API tests into Continuous Integration and Continuous Delivery/Deployment (CI/CD) pipelines ensures that tests are run automatically with every code change, providing quick feedback and preventing regressions.</p>
        
        <p>Think of CI/CD integration like an automated quality control system in a factory—each product gets inspected at various stages of production without requiring manual intervention, ensuring consistent quality.</p>
        
        <div class="pipeline-stages">
            <h3>Test Stages in a CI/CD Pipeline</h3>
            <ul>
                <li><strong>Unit Tests:</strong> Run for every commit, should be fast (seconds to minutes)</li>
                <li><strong>Integration Tests:</strong> Run for PRs/merges, can be slower (minutes)</li>
                <li><strong>Contract Tests:</strong> Run when API contracts change or during PR validation</li>
                <li><strong>Functional Tests:</strong> Run for staging deployments, validate core flows</li>
                <li><strong>Security Tests:</strong> Run regularly, especially for security-sensitive changes</li>
                <li><strong>Performance Tests:</strong> Run on a schedule or before major releases</li>
            </ul>
        </div>
        
        <div class="code-section">
            <h3>GitHub Actions Workflow for API Testing</h3>
            <pre><code># .github/workflows/api-tests.yml
name: API Tests

on:
push:
branches: [ main, develop ]
pull_request:
branches: [ main, develop ]

jobs:
unit-tests:
runs-on: ubuntu-latest

services:
  # Add database service if needed
  postgres:
    image: postgres:13
    env:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: test_db
    ports:
      - 5432:5432
    options: >-
      --health-cmd pg_isready
      --health-interval 10s
      --health-timeout 5s
      --health-retries 5

steps:
- uses: actions/checkout@v2

- name: Set up Python
  uses: actions/setup-python@v2
  with:
    python-version: '3.9'

- name: Install dependencies
  run: |
    python -m pip install --upgrade pip
    pip install pytest pytest-cov flake8
    if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

- name: Lint with flake8
  run: |
    flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
    flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

- name: Run unit tests
  run: |
    pytest tests/unit --cov=app --cov-report=xml

- name: Upload coverage to Codecov
  uses: codecov/codecov-action@v1
  with:
    file: ./coverage.xml
    fail_ci_if_error: false

integration-tests:
needs: unit-tests
runs-on: ubuntu-latest

services:
  postgres:
    image: postgres:13
    env:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: test_db
    ports:
      - 5432:5432
    options: >-
      --health-cmd pg_isready
      --health-interval 10s
      --health-timeout 5s
      --health-retries 5

steps:
- uses: actions/checkout@v2

- name: Set up Python
  uses: actions/setup-python@v2
  with:
    python-version: '3.9'

- name: Install dependencies
  run: |
    python -m pip install --upgrade pip
    if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    pip install pytest pytest-cov

- name: Run integration tests
  run: |
    # Start the API server
    python app.py &
    # Wait for server to start
    sleep 5
    # Run integration tests
    pytest tests/integration

contract-tests:
needs: integration-tests
runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2

- name: Set up Python
  uses: actions/setup-python@v2
  with:
    python-version: '3.9'

- name: Install dependencies
  run: |
    python -m pip install --upgrade pip
    if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    pip install pytest openapi-spec-validator

- name: Check API contract
  run: |
    # Validate OpenAPI spec
    pytest tests/contract/test_openapi_spec.py

- name: Run Pact tests
  if: success()
  run: |
    # Install Pact
    pip install pact-python
    # Run Pact tests
    pytest tests/contract/test_pact.py

security-tests:
runs-on: ubuntu-latest
needs: [unit-tests, integration-tests]

steps:
- uses: actions/checkout@v2

- name: Set up Python
  uses: actions/setup-python@v2
  with:
    python-version: '3.9'

- name: Install dependencies
  run: |
    python -m pip install --upgrade pip
    pip install bandit safety

- name: Static security analysis with Bandit
  run: |
    bandit -r app/ -f json -o bandit-results.json

- name: Check dependencies for vulnerabilities
  run: |
    safety check -r requirements.txt

- name: Upload security reports
  uses: actions/upload-artifact@v2
  with:
    name: security-reports
    path: |
      bandit-results.json

deploy-staging:
if: github.event_name == 'push' && github.ref == 'refs/heads/develop'
needs: [unit-tests, integration-tests, contract-tests, security-tests]
runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2

- name: Deploy to staging
  run: |
    echo "Deploying to staging environment..."
    # Add your deployment steps here

- name: Run functional tests on staging
  run: |
    echo "Running functional tests against staging..."
    # Add your functional test steps here

performance-tests:
if: github.event_name == 'push' && github.ref == 'refs/heads/main'
needs: deploy-staging
runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2

- name: Set up Python
  uses: actions/setup-python@v2
  with:
    python-version: '3.9'

- name: Install dependencies
  run: |
    python -m pip install --upgrade pip
    pip install locust matplotlib pandas

- name: Run performance tests
  run: |
    # Run Locust in headless mode
    locust -f tests/performance/locustfile.py --host=https://staging-api.example.com --headless -u 50 -r 5 --run-time 5m --csv=results

- name: Upload performance test results
  uses: actions/upload-artifact@v2
  with:
    name: performance-results
    path: |
      results*.csv

deploy-production:
if: github.event_name == 'push' && github.ref == 'refs/heads/main'
needs: [performance-tests]
runs-on: ubuntu-latest

steps:
- uses: actions/checkout@v2

- name: Deploy to production
  run: |
    echo "Deploying to production environment..."
    # Add your production deployment steps here</code></pre>

            <h3>GitLab CI/CD Pipeline Configuration</h3>
            <pre><code># .gitlab-ci.yml
stages:
- lint
- test
- security
- deploy-staging
- test-staging
- performance
- deploy-production

variables:
POSTGRES_DB: test_db
POSTGRES_USER: postgres
POSTGRES_PASSWORD: postgres
POSTGRES_HOST: postgres

lint:
stage: lint
image: python:3.9
script:
- pip install flake8
- flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
- flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

unit-tests:
stage: test
image: python:3.9
services:
- postgres:13
script:
- pip install -r requirements.txt
- pip install pytest pytest-cov
- pytest tests/unit --cov=app --cov-report=xml
artifacts:
paths:
  - coverage.xml
reports:
  coverage_report:
    coverage_format: cobertura
    path: coverage.xml

integration-tests:
stage: test
image: python:3.9
services:
- postgres:13
script:
- pip install -r requirements.txt
- pip install pytest
- python app.py &
- sleep 5
- pytest tests/integration
dependencies:
- unit-tests

contract-tests:
stage: test
image: python:3.9
script:
- pip install -r requirements.txt
- pip install pytest openapi-spec-validator pact-python
- pytest tests/contract
dependencies:
- unit-tests

security-tests:
stage: security
image: python:3.9
script:
- pip install bandit safety
- bandit -r app/ -o bandit-results.json
- safety check -r requirements.txt
artifacts:
paths:
  - bandit-results.json
dependencies:
- unit-tests

deploy-staging:
stage: deploy-staging
image: python:3.9
script:
- echo "Deploying to staging environment..."
# Add your deployment steps here
environment:
name: staging
url: https://staging-api.example.com
only:
- develop
dependencies:
- unit-tests
- integration-tests
- contract-tests
- security-tests

functional-tests:
stage: test-staging
image: python:3.9
script:
- pip install -r requirements.txt
- pip install pytest requests
- pytest tests/functional --host=https://staging-api.example.com
environment:
name: staging
url: https://staging-api.example.com
only:
- develop
dependencies:
- deploy-staging

performance-tests:
stage: performance
image: python:3.9
script:
- pip install locust matplotlib pandas
- mkdir -p performance-results
- python tests/performance/run_locust.py --host=https://staging-api.example.com --users=50 --spawn-rate=5 --run-time=5m
artifacts:
paths:
  - performance-results/
only:
- main
dependencies:
- deploy-staging

deploy-production:
stage: deploy-production
image: python:3.9
script:
- echo "Deploying to production environment..."
# Add your production deployment steps here
environment:
name: production
url: https://api.example.com
only:
- main
when: manual
dependencies:
- performance-tests</code></pre>
        </div>
        
        <div class="test-automation-best-practices">
            <h3>Best Practices for Test Automation in CI/CD</h3>
            <ul>
                <li><strong>Fast Feedback Loop:</strong> Run the fastest tests first to provide quick feedback.</li>
                <li><strong>Fail Fast:</strong> Configure the pipeline to stop at the first test failure.</li>
                <li><strong>Parallel Execution:</strong> Run independent tests in parallel to save time.</li>
                <li><strong>Proper Isolation:</strong> Ensure tests don't interfere with each other.</li>
                <li><strong>Environment Consistency:</strong> Use containers/VMs to ensure consistent test environments.</li>
                <li><strong>Artifact Retention:</strong> Save test results, logs, and coverage reports as artifacts.</li>
                <li><strong>Automated Reporting:</strong> Generate and publish test reports automatically.</li>
                <li><strong>Notification System:</strong> Alert the team when tests fail.</li>
                <li><strong>Flaky Test Handling:</strong> Identify and fix or isolate flaky tests.</li>
                <li><strong>Test Data Management:</strong> Ensure tests have the necessary data without conflicts.</li>
            </ul>
        </div>
        
        <div class="test-environments">
            <h3>Managing Test Environments</h3>
            <p>Proper test environment management is crucial for reliable test automation:</p>
            
            <h4>Environment Types</h4>
            <ul>
                <li><strong>Development:</strong> For developers to test their changes locally</li>
                <li><strong>CI:</strong> For running automated tests in the CI pipeline</li>
                <li><strong>Staging:</strong> For testing integrations and running performance tests</li>
                <li><strong>Production:</strong> The live environment</li>
            </ul>
            
            <h4>Environment Management Strategies</h4>
            <ul>
                <li><strong>Containerization:</strong> Use Docker to create consistent environments</li>
                <li><strong>Infrastructure as Code:</strong> Define environments with tools like Terraform or CloudFormation</li>
                <li><strong>Database Management:</strong> Use migrations for schema changes and seed data for tests</li>
                <li><strong>Service Virtualization:</strong> Mock external dependencies for isolated testing</li>
                <li><strong>Environment Variables:</strong> Configure services differently per environment</li>
            </ul>
            
            <h4>Docker Compose for Local Testing</h4>
            <pre><code># docker-compose.yml for local testing
version: '3.8'

services:
api:
build: .
ports:
  - "5000:5000"
environment:
  - FLASK_APP=app.py
  - FLASK_ENV=development
  - DATABASE_URL=postgresql://postgres:postgres@db:5432/api_test
  - SECRET_KEY=test-secret-key
depends_on:
  - db
volumes:
  - .:/app

db:
image: postgres:13
environment:
  - POSTGRES_USER=postgres
  - POSTGRES_PASSWORD=postgres
  - POSTGRES_DB=api_test
ports:
  - "5432:5432"
volumes:
  - postgres_data:/var/lib/postgresql/data

test:
build:
  context: .
  dockerfile: Dockerfile.test
environment:
  - FLASK_ENV=testing
  - DATABASE_URL=postgresql://postgres:postgres@db:5432/api_test
  - API_URL=http://api:5000
  - SECRET_KEY=test-secret-key
depends_on:
  - api
  - db
volumes:
  - .:/app
command: pytest

volumes:
postgres_data:</code></pre>
        </div>
    </section>

    <section class="test_driven_development">
        <h2>Test-Driven Development for APIs</h2>
        
        <p>Test-Driven Development (TDD) is a software development approach where tests are written before the implementation code. When applied to API development, TDD helps ensure that your API meets its requirements and maintains high quality as it evolves.</p>
        
        <p>Think of TDD like creating a detailed blueprint before building a house—you define what you want to achieve and verify that your construction meets those specifications at every step.</p>
        
        <div class="tdd-cycle">
            <h3>The TDD Cycle for API Development</h3>
            <ol>
                <li><strong>Write a Test:</strong> Start by writing a test that defines the expected behavior of your API endpoint or component.</li>
                <li><strong>Run the Test:</strong> Verify that the test fails (since you haven't implemented the functionality yet).</li>
                <li><strong>Write the Implementation:</strong> Write the minimal code needed to make the test pass.</li>
                <li><strong>Run All Tests:</strong> Verify that your implementation passes the new test and doesn't break existing functionality.</li>
                <li><strong>Refactor:</strong> Clean up your code while ensuring all tests still pass.</li>
                <li><strong>Repeat:</strong> Continue the cycle with the next piece of functionality.</li>
            </ol>
        </div>
        
        <div class="code-section">
            <h3>TDD Example for a User API in Flask</h3>
            <p>Let's walk through the TDD process for a user authentication API:</p>
            
            <h4>Step 1: Write the First Test</h4>
            <pre><code># tests/test_user_api.py
import pytest
import json
from app import create_app
from app.models import db as _db
from app.models.user import User

@pytest.fixture
def app():
app = create_app('testing')
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///:memory:'

with app.app_context():
    _db.create_all()

yield app

with app.app_context():
    _db.drop_all()

@pytest.fixture
def client(app):
return app.test_client()

def test_register_user(client):
"""Test user registration endpoint"""
# Define test data
user_data = {
    "username": "testuser",
    "email": "test@example.com",
    "password": "Password123!"
}

# Make request to the endpoint (which doesn't exist yet)
response = client.post(
    '/api/users/register',
    data=json.dumps(user_data),
    content_type='application/json'
)

# Assert expected behavior
assert response.status_code == 201
data = json.loads(response.data)
assert data['username'] == 'testuser'
assert data['email'] == 'test@example.com'
assert 'id' in data

# Verify user was created in database
with app.app_context():
    user = User.query.filter_by(username='testuser').first()
    assert user is not None
    assert user.email == 'test@example.com'</code></pre>
            
            <h4>Step 2: Run the Test (Expect Failure)</h4>
            <pre><code>$ pytest tests/test_user_api.py
FAILED tests/test_user_api.py::test_register_user - ImportError: cannot import name 'create_app'</code></pre>
            
            <h4>Step 3: Implement Minimal Code to Make the Test Pass</h4>
            <pre><code># app/__init__.py
from flask import Flask
from flask_sqlalchemy import SQLAlchemy

db = SQLAlchemy()

def create_app(config_name='default'):
app = Flask(__name__)

if config_name == 'testing':
    app.config['TESTING'] = True
    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db.init_app(app)

from app.api import api_bp
app.register_blueprint(api_bp, url_prefix='/api')

return app

# app/models/user.py
from app import db
from werkzeug.security import generate_password_hash, check_password_hash

class User(db.Model):
id = db.Column(db.Integer, primary_key=True)
username = db.Column(db.String(50), unique=True, nullable=False)
email = db.Column(db.String(100), unique=True, nullable=False)
password_hash = db.Column(db.String(128))

def set_password(self, password):
    self.password_hash = generate_password_hash(password)

def check_password(self, password):
    return check_password_hash(self.password_hash, password)

# app/api/__init__.py
from flask import Blueprint

api_bp = Blueprint('api', __name__)

from app.api import users

# app/api/users.py
from flask import request, jsonify
from app import db
from app.models.user import User
from app.api import api_bp

@api_bp.route('/users/register', methods=['POST'])
def register_user():
data = request.get_json
data = request.get_json()
    
    if not data or 'username' not in data or 'email' not in data or 'password' not in data:
        return jsonify({"error": "Missing required fields"}), 400
    
    # Check if user already exists
    if User.query.filter_by(username=data['username']).first():
        return jsonify({"error": "Username already exists"}), 409
    
    if User.query.filter_by(email=data['email']).first():
        return jsonify({"error": "Email already exists"}), 409
    
    # Create new user
    user = User(
        username=data['username'],
        email=data['email']
    )
    user.set_password(data['password'])
    
    db.session.add(user)
    db.session.commit()
    
    return jsonify({
        "id": user.id,
        "username": user.username,
        "email": user.email
    }), 201</code></pre>
                
                <h4>Step 4: Run the Test Again (Should Pass)</h4>
                <pre><code>$ pytest tests/test_user_api.py
PASSED tests/test_user_api.py::test_register_user</code></pre>
                
                <h4>Step 5: Write the Next Test (for Login)</h4>
                <pre><code>def test_login_user(client, app):
    """Test user login endpoint"""
    # Create a test user
    with app.app_context():
        user = User(username='logintest', email='login@example.com')
        user.set_password('Password123!')
        db.session.add(user)
        db.session.commit()
    
    # Login with correct credentials
    response = client.post(
        '/api/auth/login',
        data=json.dumps({
            "username": "logintest",
            "password": "Password123!"
        }),
        content_type='application/json'
    )
    
    # Assert expected behavior
    assert response.status_code == 200
    data = json.loads(response.data)
    assert 'access_token' in data
    assert 'refresh_token' in data
    
    # Login with incorrect password
    response = client.post(
        '/api/auth/login',
        data=json.dumps({
            "username": "logintest",
            "password": "wrongpassword"
        }),
        content_type='application/json'
    )
    
    # Should fail
    assert response.status_code == 401</code></pre>
                
                <h4>Step 6: Run the Test (Expect Failure)</h4>
                <pre><code>$ pytest tests/test_user_api.py::test_login_user
FAILED tests/test_user_api.py::test_login_user - assert 404 == 200
E   +  where 404 = <Response streamed [404 NOT FOUND]>.status_code</code></pre>
                
                <h4>Step 7: Implement the Login Functionality</h4>
                <pre><code># app/api/auth.py
import datetime
import jwt
from flask import request, jsonify, current_app
from app import db
from app.models.user import User
from app.api import api_bp

@api_bp.route('/auth/login', methods=['POST'])
def login():
    data = request.get_json()
    
    if not data or 'username' not in data or 'password' not in data:
        return jsonify({"error": "Missing username or password"}), 400
    
    user = User.query.filter_by(username=data['username']).first()
    
    if not user or not user.check_password(data['password']):
        return jsonify({"error": "Invalid credentials"}), 401
    
    # Generate access token
    access_token = jwt.encode(
        {
            'user_id': user.id,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=15)
        },
        current_app.config.get('SECRET_KEY', 'dev-key'),
        algorithm='HS256'
    )
    
    # Generate refresh token
    refresh_token = jwt.encode(
        {
            'user_id': user.id,
            'exp': datetime.datetime.utcnow() + datetime.timedelta(days=1)
        },
        current_app.config.get('SECRET_KEY', 'dev-key'),
        algorithm='HS256'
    )
    
    return jsonify({
        "access_token": access_token,
        "refresh_token": refresh_token,
        "user": {
            "id": user.id,
            "username": user.username
        }
    }), 200

# Update app/__init__.py to import the auth module
from app.api import api_bp

def create_app(config_name='default'):
    app = Flask(__name__)
    
    if config_name == 'testing':
        app.config['TESTING'] = True
        app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
        app.config['SECRET_KEY'] = 'test-secret-key'
    
    db.init_app(app)
    
    from app.api import api_bp
    app.register_blueprint(api_bp, url_prefix='/api')
    
    return app

# Update app/api/__init__.py to import the auth module
from flask import Blueprint

api_bp = Blueprint('api', __name__)

from app.api import users, auth</code></pre>
                
                <h4>Step 8: Run the Tests Again (Should Pass)</h4>
                <pre><code>$ pytest tests/test_user_api.py
PASSED tests/test_user_api.py::test_register_user
PASSED tests/test_user_api.py::test_login_user</code></pre>
                
                <h4>Step 9: Write the Next Test (Protected Endpoint)</h4>
                <pre><code>def test_protected_endpoint(client, app):
    """Test protected endpoint requiring authentication"""
    # Create a test user
    with app.app_context():
        user = User(username='protectedtest', email='protected@example.com')
        user.set_password('Password123!')
        db.session.add(user)
        db.session.commit()
    
    # Login to get token
    login_response = client.post(
        '/api/auth/login',
        data=json.dumps({
            "username": "protectedtest",
            "password": "Password123!"
        }),
        content_type='application/json'
    )
    
    data = json.loads(login_response.data)
    token = data['access_token']
    
    # Access protected endpoint with token
    response = client.get(
        '/api/users/profile',
        headers={"Authorization": f"Bearer {token}"}
    )
    
    # Should succeed
    assert response.status_code == 200
    profile_data = json.loads(response.data)
    assert profile_data['username'] == 'protectedtest'
    
    # Access without token
    response = client.get('/api/users/profile')
    
    # Should fail
    assert response.status_code == 401
    
    # Access with invalid token
    response = client.get(
        '/api/users/profile',
        headers={"Authorization": "Bearer invalid_token"}
    )
    
    # Should fail
    assert response.status_code == 401</code></pre>
                
                <h4>Step 10: Continue the TDD Cycle</h4>
                <p>Continue implementing features, writing tests first, then implementing the code to make them pass.</p>
            </div>
            
            <div class="tdd-benefits">
                <h3>Benefits of TDD for API Development</h3>
                <ul>
                    <li><strong>Clear Requirements:</strong> Tests serve as executable specifications, making requirements explicit.</li>
                    <li><strong>Focused Development:</strong> Writing tests first helps focus on one feature at a time.</li>
                    <li><strong>Better Design:</strong> TDD encourages more modular, loosely coupled code.</li>
                    <li><strong>Regression Prevention:</strong> Tests catch regressions early when code is modified.</li>
                    <li><strong>Documentation:</strong> Tests serve as living documentation of how the API should behave.</li>
                    <li><strong>Confidence in Refactoring:</strong> Tests provide a safety net when refactoring or adding features.</li>
                    <li><strong>Higher Quality:</strong> TDD typically results in fewer bugs and higher quality code.</li>
                </ul>
            </div>
            
            <div class="tdd-challenges">
                <h3>Challenges and Tips for API TDD</h3>
                <ul>
                    <li><strong>Learning Curve:</strong> TDD requires practice and discipline to master.</li>
                    <li><strong>External Dependencies:</strong> Testing APIs with external dependencies can be challenging.
                        <ul>
                            <li>Solution: Use mocks, stubs, or test doubles for external services.</li>
                        </ul>
                    </li>
                    <li><strong>Database Interactions:</strong> Database tests can be slow and complex.
                        <ul>
                            <li>Solution: Use in-memory databases for testing or transaction rollbacks.</li>
                        </ul>
                    </li>
                    <li><strong>Authentication/Authorization:</strong> Authentication flows can be complex to test.
                        <ul>
                            <li>Solution: Use test fixtures to set up authenticated contexts.</li>
                        </ul>
                    </li>
                    <li><strong>Test Data Management:</strong> Managing test data across tests can be challenging.
                        <ul>
                            <li>Solution: Use factories or fixtures to generate test data consistently.</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </section>

        <section class="testing_tools">
            <h2>Python API Testing Tools and Frameworks</h2>
            
            <p>Python offers a rich ecosystem of tools and frameworks for API testing, catering to different testing needs and preferences. This section provides an overview of the most popular and useful tools to consider for your API testing strategy.</p>
            
            <div class="tools-comparison">
                <h3>Unit Testing Frameworks</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>pytest</td>
                            <td>Modern Python testing framework</td>
                            <td>
                                - Powerful fixture system<br>
                                - Concise syntax<br>
                                - Extensive plugin ecosystem<br>
                                - Parameterized tests
                            </td>
                            <td>Most Python projects, from small to large scale</td>
                        </tr>
                        <tr>
                            <td>unittest</td>
                            <td>Python's built-in testing framework</td>
                            <td>
                                - Built into Python standard library<br>
                                - xUnit style<br>
                                - Test discovery<br>
                                - Test runners
                            </td>
                            <td>Projects needing standard library compatibility</td>
                        </tr>
                        <tr>
                            <td>nose2</td>
                            <td>Extension of unittest</td>
                            <td>
                                - Plugin architecture<br>
                                - Backward compatibility with nose<br>
                                - Extended test discovery
                            </td>
                            <td>Projects transitioning from nose or unittest</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>API Client Libraries</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>requests</td>
                            <td>HTTP library for Python</td>
                            <td>
                                - Simple, elegant API<br>
                                - Session management<br>
                                - Authentication support<br>
                                - JSON handling
                            </td>
                            <td>General HTTP requests, API client testing</td>
                        </tr>
                        <tr>
                            <td>httpx</td>
                            <td>Next-generation HTTP client</td>
                            <td>
                                - Async/await support<br>
                                - Similar API to requests<br>
                                - HTTP/2 support<br>
                                - Type hints
                            </td>
                            <td>Modern Python projects, async testing</td>
                        </tr>
                        <tr>
                            <td>aiohttp</td>
                            <td>Async HTTP client/server</td>
                            <td>
                                - Fully async<br>
                                - WebSocket support<br>
                                - Server and client functionality
                            </td>
                            <td>Async-focused applications, high-performance testing</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>API Testing Frameworks</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Tavern</td>
                            <td>YAML-based API testing</td>
                            <td>
                                - YAML test definitions<br>
                                - pytest integration<br>
                                - Response validation<br>
                                - Simple, readable syntax
                            </td>
                            <td>Readable, declarative API tests</td>
                        </tr>
                        <tr>
                            <td>Postman/Newman</td>
                            <td>API testing platform</td>
                            <td>
                                - GUI for manual testing<br>
                                - Collection runner<br>
                                - Environment variables<br>
                                - Newman CLI for automation
                            </td>
                            <td>Mixed teams, comprehensive API testing</td>
                        </tr>
                        <tr>
                            <td>pytest-httpx</td>
                            <td>httpx mocking for pytest</td>
                            <td>
                                - Mock HTTP responses<br>
                                - Verify request parameters<br>
                                - Integration with pytest
                            </td>
                            <td>Mocking HTTP requests in pytest</td>
                        </tr>
                        <tr>
                            <td>Schemathesis</td>
                            <td>Property-based API testing</td>
                            <td>
                                - Generate tests from OpenAPI specs<br>
                                - Fuzz testing<br>
                                - Schema validation
                            </td>
                            <td>Automated test generation, contract testing</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>Mock and Stub Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>unittest.mock</td>
                            <td>Python's built-in mocking library</td>
                            <td>
                                - Part of standard library<br>
                                - Mock objects and functions<br>
                                - Patch decorators
                            </td>
                            <td>General mocking needs</td>
                        </tr>
                        <tr>
                            <td>responses</td>
                            <td>HTTP request mocking</td>
                            <td>
                                - Mock HTTP responses<br>
                                - Works with requests library<br>
                                - Exception simulation
                            </td>
                            <td>Mocking HTTP responses in tests</td>
                        </tr>
                        <tr>
                            <td>pytest-mock</td>
                            <td>pytest fixture for unittest.mock</td>
                            <td>
                                - Convenient fixture interface<br>
                                - Automatic teardown<br>
                                - Integration with pytest
                            </td>
                            <td>Simpler mocking in pytest</td>
                        </tr>
                        <tr>
                            <td>VCR.py</td>
                            <td>Record and replay HTTP interactions</td>
                            <td>
                                - Record real HTTP interactions<br>
                                - Replay in future test runs<br>
                                - Customizable cassette storage
                            </td>
                            <td>Integration testing with real services</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>Performance Testing Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Locust</td>
                            <td>Python-based load testing</td>
                            <td>
                                - Python scripts for test scenarios<br>
                                - Distributed load generation<br>
                                - Real-time web UI<br>
                                - Customizable user behavior
                            </td>
                            <td>Developer-friendly load testing</td>
                        </tr>
                        <tr>
                            <td>pytest-benchmark</td>
                            <td>Benchmarking plugin for pytest</td>
                            <td>
                                - Detailed timing statistics<br>
                                - Comparison between runs<br>
                                - Customizable fixtures
                            </td>
                            <td>Code benchmarking in pytest</td>
                        </tr>
                        <tr>
                            <td>Molotov</td>
                            <td>Async load testing</td>
                            <td>
                                - Asyncio-based<br>
                                - Coroutines for scenarios<br>
                                - Integration with pytest
                            </td>
                            <td>Async API load testing</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3>Security Testing Tools</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Tool</th>
                            <th>Description</th>
                            <th>Key Features</th>
                            <th>Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Bandit</td>
                            <td>Security linter for Python code</td>
                            <td>
                                - Static code analysis<br>
                                - Security vulnerability detection<br>
                                - Configurable rules
                            </td>
                            <td>Identifying common security issues in code</td>
                        </tr>
                        <tr>
                            <td>OWASP ZAP</td>
                            <td>Web application security scanner</td>
                            <td>
                                - Active and passive scanning<br>
                                - Automated security testing<br>
                                - Python API for automation
                            </td>
                            <td>Comprehensive security testing</td>
                        </tr>
                        <tr>
                            <td>Safety</td>
                            <td>Dependency vulnerability checker</td>
                            <td>
                                - Check for vulnerable dependencies<br>
                                - Integration with CI/CD<br>
                                - Database of known vulnerabilities
                            </td>
                            <td>Supply chain security</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="tool-selection">
                <h3>How to Choose the Right Tools</h3>
                <p>Consider these factors when selecting testing tools for your API projects:</p>
                
                <ul>
                    <li><strong>Project Requirements:</strong> Match tools to your specific testing needs.</li>
                    <li><strong>Team Expertise:</strong> Consider the learning curve and existing team knowledge.</li>
                    <li><strong>Integration:</strong> Ensure tools integrate well with your development workflow and CI/CD pipeline.</li>
                    <li><strong>Maintainability:</strong> Choose tools with active development and good documentation.</li>
                    <li><strong>Performance:</strong> Consider the speed and resource usage of the testing tools.</li>
                    <li><strong>Community Support:</strong> Larger communities mean better support and more resources.</li>
                </ul>
            </div>
        </section>

        <section class="conclusion">
            <h2>Conclusion and Next Steps</h2>
            
            <p>Testing is an essential aspect of API development, ensuring that your APIs are reliable, secure, performant, and meet their requirements. By implementing a comprehensive testing strategy that includes various types of tests at different levels, you can build high-quality APIs that stand the test of time.</p>
            
            <div class="summary">
                <h3>Key Takeaways</h3>
                <ul>
                    <li><strong>Follow the Testing Pyramid:</strong> Implement more unit tests than integration tests, and more integration tests than end-to-end tests.</li>
                    <li><strong>Use the Right Tools:</strong> Choose appropriate testing tools for different testing needs.</li>
                    <li><strong>Automate Testing:</strong> Integrate tests into your CI/CD pipeline for continuous validation.</li>
                    <li><strong>Test Beyond Functionality:</strong> Include security, performance, and contract testing in your strategy.</li>
                    <li><strong>Consider TDD:</strong> Test-Driven Development can lead to better design and higher quality code.</li>
                    <li><strong>Balance Coverage and Practicality:</strong> Aim for high test coverage while focusing on critical areas.</li>
                    <li><strong>Keep Tests Maintainable:</strong> Well-structured, readable tests are easier to maintain as your API evolves.</li>
                </ul>
            </div>
            
            <div class="next-steps">
                <h3>Next Steps in Your Testing Journey</h3>
                <ul>
                    <li><strong>Assess Your Current Testing:</strong> Evaluate your existing testing practices and identify areas for improvement.</li>
                    <li><strong>Start Small:</strong> Begin by implementing basic unit and integration tests if you don't have them already.</li>
                    <li><strong>Gradually Expand:</strong> Add more sophisticated tests (security, performance, contract) as your testing maturity increases.</li>
                    <li><strong>Automate:</strong> Set up CI/CD pipelines to run tests automatically.</li>
                    <li><strong>Monitor and Improve:</strong> Regularly review test results and refine your testing strategy.</li>
                    <li><strong>Foster a Testing Culture:</strong> Promote the importance of testing within your team or organization.</li>
                </ul>
            </div>
            
            <div class="resources">
                <h3>Further Learning Resources</h3>
                <ul>
                    <li><strong>Books:</strong>
                        <ul>
                            <li>"API Testing and Development with Postman" by Dave Westerveld</li>
                            <li>"Python Testing with pytest" by Brian Okken</li>
                            <li>"Test-Driven Development with Python" by Harry Percival</li>
                            <li>"Building Microservices" by Sam Newman (includes API testing strategies)</li>
                        </ul>
                    </li>
                    <li><strong>Online Resources:</strong>
                        <ul>
                            <li>OWASP API Security Top 10</li>
                            <li>pytest documentation</li>
                            <li>Postman Learning Center</li>
                            <li>Flask and Django REST Framework testing guides</li>
                        </ul>
                    </li>
                    <li><strong>Courses:</strong>
                        <ul>
                            <li>API Testing Foundations</li>
                            <li>Python Testing for Data Science</li>
                            <li>REST API Testing with Python</li>
                            <li>Web API Development and Documentation with Flask</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <p>Remember that testing is not a one-time activity but an ongoing process. As your API evolves, so should your testing strategy. By continuously refining and improving your tests, you can ensure that your API remains reliable, secure, and performant throughout its lifecycle.</p>
        </section>
    </main>

    <footer>
        <p>Python Full Stack Web Course - API Testing Guide</p>
    </footer>
</body>
</html>
